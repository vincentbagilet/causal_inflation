<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Unbiased but Inflated Causal Effects</title>
    <meta charset="utf-8" />
    <meta name="author" content="Vincent Bagilet and Leo Zabrocki" />
    <meta name="date" content="2021-12-08" />
    <link rel="stylesheet" href="mediocre-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Unbiased but Inflated Causal Effects
### Vincent Bagilet and Leo Zabrocki
### 2021-12-08

---




## Motivation

- Our main goal is to identify causal effects: need to avoid potential confounders

- Use causal identifications strategies

- They rely on limiting variation, either in X or in the sample size

- Could lead to decrease statistical power

- In this paper, we highlight a trade-off between statistical power and identification strategy

???

- "We all know" that, our main goal as researchers using observational data is to identify causal effects
- To do so, we need to avoid potential **confounders**
- We therefore use causal identifications strategies such as IV, DID, RDD, etc
- "We all know" that these strategies are often really good at avoiding confounders
- Yet, "we also all know" that these causal identification strategies rely on limiting variation, either in X or in the sample size to identify these causal effects. *To develop, a lot*
- Limiting this variation inherently also could lead to decrease statistical power, *ie* the ability to detect an effect when there is actually one.
- More precisely (*need to put the actual neat def*), statistical power is the probability of rejecting the null when the null is false.
- In this paper, we highlight a trade-off between statistical power and identification strategy and illustrate its consequences

---

## Replication crisis

- There is a replication crisis in economics, mostly in lab experiments

- No confounders: how can there be replicability problems in controled environments?

- It could be that the estimates are inflated due to a lack of power

---

## Illustration of the issue

&lt;img src="presentation_december_files/figure-html/illustration-1.png" width="1200" /&gt;

???

- In red is the estimate from the original study and its 95% confidence interval
- The estimate is significatif and has been published. Yet, it is pretty noisy.
- In blue is the estimate from the replicated study and its 95% confidence interval
- We notice that this second estimate is both more precise and smaller than the initial one. It still remains noisy
- Let's assume that the true effect is actually equal to this second estimate (note that this is unlikely)
- Would the design of the initial study be good enough to detect this true effect? *ie* if we replicated the initial study, could we reject the null of no effect (knowing that the true effect is equal to the replicated estimate)
- In gray is the estimate form the replicated study but with a standard error equal to the initial study's (approximately the standard errors that would have been obtained with the design of the initial study)
- This estimate is non significant. In this instance, we would not have been able to reject the null of no effect
- Now, if we replicate this study 500 times, running 500 lab experiments, in some cases we would get statisitcally significant estimates (the beige dots) and in some others non statistically significant ones (the green dots)
- If we would have been a bit more lucky, we could have gotten a sample of individuals that would have yielded one of the beige estimates
- Now, we notice that, on average, statistically significant estimates overestimate the true effect by a factor 1.7 (average of 0.53 while the true effect is 0.31). Gelman and Carlin call this inflation factor type M error.
- In this case, the power is basically the proportion of statistically significant estimates
- If the study had more power, the sd would be smaller and most estimates would be statistically significative (because there is indeed a non null effect)
- But since the power is low (33%), if by chance the sample of individuals we get yields a statistically significant estimate, this estimate will overestimate the true effect
- One may wonder how power can be that low in experimental setting. To compute power, one needs to make assumption on the true size of the effect. For a fixed design, the larger the true effect, the larger the power. If one is overoptimistic on. Out of luck, they can get effects that are in the same range (especially because the range of possible effects will be very large)
- As seen in this example, a lack of power and statistically significance filter may explain the fact that replicated estimates are often smaller than the initial study (2/3 of the initial estimate sizes in Camerer et al)

---

- Power issue even in controlled settings

- More attention should be devoted to power in observational settings

- In observational settings, also need to take confounders into account

---

## What we do

1. We consider most standard causal inference methods used in the econ literature such as matching, IV, RDD and DiD/event study
2. For each identification strategy:
    - We discuss how key factors (such as the strength of the IV, the bandwidth for the RDD, the number of matched units for matching and ? for DiD) can affect the power of the study and thus create a tension between avoiding confounders and low power
    - Using fake data, we simulate this tension in scenarios based on examples drawn from different econ literatures (Labor econ, education econ, environmental econ and public econ)
3. We discuss potential general avenues to address this problem

---

## Contributions


---
class: inverse, middle, center

# How do we illustrate this trade off?

---
class: middle

- Split the analysis by identification strategy

  - Build fake data simulations

  - Discuss avenues to address this problem for each identification strategy

- Discuss approaches to mitigate this issue

---
class: inverse, middle, center

# RDD

---

- Assume quasi-random allocation at a threshold to get rid of OVB (selection bias)

--

- Example from economics of education

--

&lt;img src="images/DAG_RDD.png" width="400" style="display: block; margin: auto;" /&gt;

- True DGP: `\(\small Final_{i} = \alpha_0 + \beta_0 T_i + \gamma_0 Qual_{i} +  \delta_0 U_{i}^{3} + \epsilon_{i}\)` 

with `\(\small Qual_i = H_i + \delta U_i^{3}\)` where `\(\small H \sim \mathcal{N}(\mu_h, \sigma_h^{2})\)`

- Model: `\(\small Final_{i} = \alpha + \beta T_i + \gamma Qual_{i} + \epsilon_{i}\)`

???
- Natural experiment in which researchers exploit a discontinuity in treatment assignation, based on the value of a forcing variable X.  For instance, below a threshold value for X, individuals are deemed treated while individuals above are untreated.
- For values close the threshold, *ie* in a given bandwidth, researchers can make the assumption that treatment assignment is quasi-random, individuals below and above being comparable on average.

---

## Generated data

- Data drawn from normal distributions with realistic parameters
&lt;img src="images/graph_bandwidth_RDD-1.png" width="800" style="display: block; margin: auto;" /&gt;

---

## Exploration for one simulation
&lt;img src="images/exploration_results_RDD-2.png" width="700" style="display: block; margin: auto;" /&gt;
- Statistically significant results are on average of the mark of the true effect

---

## THE result graph for RDD

&lt;img src="images/graph_results_RDD-1.png" width="800" style="display: block; margin: auto;" /&gt;

---
class: inverse, middle, center

# IV

---

- An unobserved variable affects both dependent and independent variables

--

- Example from environmental econ (?) or trade or ...

--

&lt;img src="images/DAG_IV.png" width="300" style="display: block; margin: auto;" /&gt;

- True DGP: `\(\small X = \alpha_{x0} + \gamma_0 Z + \delta_{x0} U + e_x\)` 

where `\(\small Z \sim \mathcal{N}(0, \sigma_{z}^{2})\)` or `\(\small Z \sim \text{Bernoulli}(p_z)\)`

and `\(\small Y = \alpha_{y0} + \beta X + \delta_{y0} U + e_y\)` 

- Model: `\(\small X = \alpha_x + \gamma_x Z+ e_x\)` and `\(\small Y = \alpha_y + \beta X + e_y\)`

---

## Exploration for one simulation

&lt;img src="images/exploration_results_IV-3.png" width="600" style="display: block; margin: auto;" /&gt;
- Statistically significant results are on average of the mark of the true effect

---

## IV strength and distribution of the estimates

&lt;img src="images/exploration_results_IV-2.png" width="700" style="display: block; margin: auto;" /&gt;

---

## THE result graph for IV

&lt;img src="images/graph_results_IV-1.png" width="700" style="display: block; margin: auto;" /&gt;

---
class: inverse, middle, center

# Conclusion

---

## Summary

.pull-left[

- **Objective**: highlight the existence of a trade off between omitted variable bias and type M error

- **Recommendations**:

  - Be mindful of power even in observational studies
  - Run simple pre analysis simulations
  - Perform post analysis robustness checks

]

.pull-right[
.center[ &lt;img src="images/jason_momoa.jpg" alt="drawing" width="280"/&gt; ]
]


---
class: inverse, middle, center

# Next steps

---

- Implement the simulations (if they seem sensible to you)

- Think about take-away messages for each identifications strategy separately

- Think about general take-away messages

- Develop more complex simulations?

- Suggestions?

---
class: inverse, middle, center

# Thank you
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false,
"highlightLines": true,
"highlightStyle": "github",
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
