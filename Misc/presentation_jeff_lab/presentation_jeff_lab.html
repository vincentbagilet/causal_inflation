<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Unbiased but Inflated Causal Effects</title>
    <meta charset="utf-8" />
    <meta name="author" content="Vincent Bagilet and Leo Zabrocki" />
    <meta name="date" content="2021-12-08" />
    <link rel="stylesheet" href="mediocre-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Unbiased but Inflated Causal Effects
### Vincent Bagilet and Leo Zabrocki
### 2021-12-08

---




## Objectives of this presentation

--

- Is the pitch convincing? How to improve it?

--

- Is the analysis good? How to improve it?

--

- What to develop/simplify?

--

- Structure of the presentation similar to the (forthcoming) paper

???

- The main objective of the paper is to highlight the existence of this trade off between omitted variable bias and type M error
- Very illustrative hence, pitch central
- You know the paper but I will a bit pretend that you don't

- Background: 

  - Submit to a journal like AER: Insights (short format) or to something like Journal of Econometrics
  - 

---
class: inverse, middle, center

# Motivation

---

- Causal identification methods to avoid Omitted Variable Bias (OVB)

--

- Throw away variation `\(\Rightarrow\)` `\(\searrow\)` statistical power

--

- Power **considered** crucial when building RCTs but not in observational studies

--

- With low statistical power, statistically significant estimates overestimate the true effect

--

- Linked to publication bias

--

- Replications of papers produce smaller effects

???

- As economists, obsessed with OVB
- Use neat causal id methods to avoid it
- Successful in getting rid of OVB
- But we know that of ten id strat are associated with decreased statistical power because they throw variation away
- We often do not pay much attention to statistical power
- Statistical power is **basically** the probability of detecting an effect when there is actually one 
- Power **considered** as crucial when building RCTs but much less in later stages and observational studies
- With low statistical power, statistically significant estimates overestimate the true effect
- Linked to publication bias, file drawer problem and statistical significance filter
- This may explain why some replications yield smaller effects (Carmerer et al)

---

## Illustration of the issue

&lt;img src="presentation_jeff_lab_files/figure-html/illustration-1.png" width="1200" /&gt;

???

- Consider the original study (in red): statistically significant, great!
- Now, let's imagine that the replication found the actual true effect of the treatment (dotted line)
- This effect is consistent with what we observed in the initial study (falls within the confidence interval)
- Now, let's assume that we can replicate the initial study 5000 times (knowing that the true effect is equal to the one found in the second study)
- First, we notice that the power is about 35% (the proportion of green points, there is an effect so power is just the proportion of statistically significant points). It is very low
- We see that the the average of all the estimates is equal to the true effect (dotted line) BUT statistically significant estimates are on average larger than the true effect 
- That could not be such a problem but due to publication bias/file drawer problem, studies yielding statistically significant estimates are more likely to be published than those yielding non statistically significant ones, either due to selection/screening by the editor or the authors.
- We can thus consider that, for a given design (question, method, sample size, etc) a study as more chances to be published if the effect found is statistically significant.
- Now if this design has low statistical power, on average a published estimate will overestimate the true effect
- That might be what happened in the case of our replications of RCT: the authors had an imprecise study, they got lucky, they found a significant (and large) effect, they published it in a great journal. 
- Yet, if they would have replicated the study with the same design, they may not have found such a large effect (nor a statistically significant one)

---

## Lessons

- Even RCTs can be wide of the mark of the true effect

- Be mindful of power, in every step of the analysis

- Concerns might be even more prevalent in other identification strategies

## Objective

- Highlight the existence of a trade off between omitted variable bias and type M error


???

- How did we get there? We had an RCT but we fell of the mark of the true effect
- The issue of inflated effect sizes due to low power is not new: several studies underlined this issue in psychology (citation), econ (ioannidis), epidemiology and medecine (citations).
- Power was definitely too small. How can this be? 
- When designing a RCT, we define the sample size to have enough power. But this relies on an hypothesized true effect size. If this hypothetical true effect size is too large, for a given design, we will overestimate the power and can then fall into type M error problems.
- We should therefore also  check the power at the end of the analysis (running robustness tests with true effects as fraction of the estimated effect. But we will develop this later)
- If find that risk of low power, increase the sample size to increase power (but of course it costs money so there is a trade off here)
- That illustrates that even RCTs can yield published estimates that are wide of the mark of the true effect
- 


---
class: inverse, middle, center

# Why could some identification strategies have low power?

---

## Summary of the core issue

.pull-left[

1. We fear omitted variable biases

1. We develop neat identification strategies

1. It sometimes lead to low power

1. It creates type M error

1. The value obtained is wide of the mark of the true effect

]

.pull-right[
.center[ &lt;img src="images/hurdles.gif" alt="drawing" width="450"/&gt; ]
]

---

### RDD
  
- When `\(\searrow\)` bandwidth,
- omitted variable bias `\(\searrow\)`
- **BUT** throw away units outside the bandwidth
- number of observations also `\(\searrow\)` 
- so power `\(\searrow\)` and type M error `\(\nearrow\)`

--

### Matching

- When `\(\searrow\)` caliper (*ie* only match the most similar units),
- omitted variable bias `\(\searrow\)`
- **BUT** throw away non matched units
- number of observations also `\(\searrow\)` 
- so power `\(\searrow\)` and type M error `\(\nearrow\)`

---

### IV
&lt;img src="images/distrib_for_presentation-1.png" width="700" style="display: block; margin: auto;" /&gt;
- Lower power than OLS and thus higher type M error
  
&lt;!-- --- --&gt;

&lt;!-- - **DID**:  --&gt;

---

### DID

- When `\(\searrow\)` number of clusters,
- omitted variable bias `\(\searrow\)` (more agnostic about the correlation structure)
- **BUT** the effective number of observations `\(\searrow\)` 
- so power `\(\searrow\)` 
- type M error `\(\nearrow\)`

--

### DiD (alternative)

- When `\(\searrow\)` number of time periods considered before and after treatment,
- omitted variable bias `\(\searrow\)`
- **BUT** the number of observations `\(\searrow\)` 
- so power `\(\searrow\)` 
- type M error `\(\nearrow\)` 
  
---

### Event study

- Example: health impact of plant shutdown on health (via air pollution)
- Compare the event study to simple regression of health on pollution
 
--
  - When `\(\searrow\)` number of events
  - variation used to estimate the effect `\(\searrow\)` 
  - so power `\(\searrow\)` 
  - type M error `\(\nearrow\)` 
  

---
class: inverse, middle, center

# How do we illustrate this trade off?

---
class: middle

- Split the analysis by identification strategy

  - Build fake data simulations

  - Discuss avenues to address this problem for each identification strategy

- Discuss approaches to mitigate this issue

---
class: inverse, middle, center

# RDD

---

- Assume quasi-random allocation at a threshold to get rid of OVB (selection bias)

--

- Example from economics of education

--

&lt;img src="images/DAG_RDD.png" width="400" style="display: block; margin: auto;" /&gt;

- True DGP: `\(\small Final_{i} = \alpha_0 + \beta_0 T_i + \gamma_0 Qual_{i} +  \delta_0 U_{i}^{3} + \epsilon_{i}\)` 

with `\(\small Qual_i = H_i + \delta U_i^{3}\)` where `\(\small H \sim \mathcal{N}(\mu_h, \sigma_h^{2})\)`

- Model: `\(\small Final_{i} = \alpha + \beta T_i + \gamma Qual_{i} + \epsilon_{i}\)`

???
- Natural experiment in which researchers exploit a discontinuity in treatment assignation, based on the value of a forcing variable X.  For instance, below a threshold value for X, individuals are deemed treated while individuals above are untreated.
- For values close the threshold, *ie* in a given bandwidth, researchers can make the assumption that treatment assignment is quasi-random, individuals below and above being comparable on average.

---

## Generated data

- Data drawn from normal distributions with realistic parameters
&lt;img src="images/graph_bandwidth_RDD-1.png" width="800" style="display: block; margin: auto;" /&gt;

---

## Exploration for one simulation
&lt;img src="images/exploration_results_RDD-2.png" width="700" style="display: block; margin: auto;" /&gt;
- Statistically significant results are on average of the mark of the true effect

---

## THE result graph for RDD

&lt;img src="images/graph_results_RDD-1.png" width="800" style="display: block; margin: auto;" /&gt;

---
class: inverse, middle, center

# IV

---

- An unobserved variable affects both dependent and independent variables

--

- Example from environmental econ (?) or trade or ...

--

&lt;img src="images/DAG_IV.png" width="300" style="display: block; margin: auto;" /&gt;

- True DGP: `\(\small X = \alpha_{x0} + \gamma_0 Z + \delta_{x0} U + e_x\)` 

where `\(\small Z \sim \mathcal{N}(0, \sigma_{z}^{2})\)` or `\(\small Z \sim \text{Bernoulli}(p_z)\)`

and `\(\small Y = \alpha_{y0} + \beta X + \delta_{y0} U + e_y\)` 

- Model: `\(\small X = \alpha_x + \gamma_x Z+ e_x\)` and `\(\small Y = \alpha_y + \beta X + e_y\)`

---

## Exploration for one simulation

&lt;img src="images/exploration_results_IV-3.png" width="600" style="display: block; margin: auto;" /&gt;
- Statistically significant results are on average of the mark of the true effect

---

## IV strength and distribution of the estimates

&lt;img src="images/exploration_results_IV-2.png" width="700" style="display: block; margin: auto;" /&gt;

---

## THE result graph for IV

&lt;img src="images/graph_results_IV-1.png" width="700" style="display: block; margin: auto;" /&gt;

---
class: inverse, middle, center

# Conclusion

---

## Summary

.pull-left[

- **Objective**: highlight the existence of a trade off between omitted variable bias and type M error

- **Recommendations**: 

  - Be mindful of power even in observational studies
  - Run simple pre analysis simulations
  - Perform post analysis robustness checks

]

.pull-right[
.center[ &lt;img src="images/jason_momoa.jpg" alt="drawing" width="280"/&gt; ]
]


---
class: inverse, middle, center

# Next steps

---

- Implement the simulations (if they seem sensible to you)

- Think about take-away messages for each identifications strategy separately

- Think about general take-away messages

- Develop more complex simulations?

- Suggestions?

---
class: inverse, middle, center

# Thank you
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false,
"highlightLines": true,
"highlightStyle": "github",
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
