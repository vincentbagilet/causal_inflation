---
title: "Simulations for panoptic issues"
description: "In this document, we run a simulation exercise to illustrate panoptic type M issues arising in causal and non-causal identification studies."
author:
  - name: Vincent Bagilet 
    url: https://vincentbagilet.github.io/
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: LÃ©o Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup_panoptic, echo=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 200,
               fig.align = "center")  
```  

```{r packages_panoptic, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse) 
library(knitr) 
library(mediocrethemes)
library(broom)
library(tictoc)
library(here)
library(beepr)
library(lmtest)
library(sandwich)

set_mediocre_all(pal = "coty") #, background = TRUE) #for presentations
```

## Summary and intuition

Power and thus type M error can also be affected by aspects that are not proper to one particular identification strategy. In this document we investigate the impact of different factors:

- The proportion of unit treated (at number of observations constant). We all know that power is maximized when the proportion of units treated is 0.5. The further away we are from this value, the smaller the power and the larger the exaggeration ratio. (*I am not certain, I need to check the formula but*) The intuition is that a limited number of treated, or control, limits the variation used to estimate the effect.
- Limiting the variation in y (small number of counts in y)
- The number of clusters. Considering a small number of clusters is to some extent comparable to considering a small number of units and thus partly acts as limiting the number of observations, thus decreasing power and increasing the exaggeration ratio.

## An illustrative example

For readability and to illustrate this loss in power, we consider an example setting. For this illustration we could consider  a large variety of distribution parameter for the variables. We narrow this down to an example setting, considering an analysis of the impact of voter turnout on election results, instrumenting voter turnout with rainfall on the day of the election. Our point should stand in more general settings and the choice of values is mostly for illustration. 


### Modeling choices

For simplicity, we consider several assumptions. These assumptions is not representative of the existing literature but the objective is only to calibrate our simulation with somehow realistic parameter values. Again, this illustration is very simplistic. The high level assumptions are:

- For now, we consider only standard normal distributions

The DGP can be represented using the following Directed Acyclic Graph (DAG): 

```{r DAG_RDD, echo=FALSE, out.width='30%'}
include_graphics(here("images/DAG_panoptic.png"))
```

We thus assume that the outcome $y$ for individual $i$ is defined as follows:

$$y_{i} = \alpha + \beta T_{i} + \delta u_{i} + \epsilon_{i}$$

Where $\alpha$ is a constant, $T_{i}$ a dummy equal to 1 if individual $i$ is treated and if they are not treated, $\beta$ the treatment effect size, $u$ an unobserved variable, $\delta$ the intensity of the OVB and $\epsilon$ an error term.

More precisely, we set:

- $N$ the number of observations
<!-- - $T \sim \text{Bernoulli}(p_T)$ the treatment. $p_T$ represents the proportion of treated units. -->
- $T_i = \mathbb{1}\{i\text{ is treated}\}$ the treatment dummy. $p_T$ represents the proportion of treated units.
- $u \sim \mathcal{N}(0, \sigma_{u}^{2})$ the unobserved variable
- $\epsilon \sim \mathcal{N}(0, \sigma_{\epsilon}^{2})$ the error term


### Data generation

We write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.

Note that, for readability, in this document, we only display the chunks of code that may be important to understand the assumptions behind our simulations and the way we built our simulation. We do not display all the arguably "technical" code, in particular the one used to generate tables and graphs. All this code is however openly available on the GitHub of the project.

```{r DGP_panoptic}
generate_data_panoptic <- function(N,
                                   p_treat,
                                   sigma_u,
                                   sigma_e,
                                   alpha,
                                   treatment_effect, #beta
                                   ovb_intensity #delta
                                   ) {
  tibble(id = 1:N) %>%
    mutate(
      # treated = rbernoulli(N, p_treat),
      treated = (id %in% sample(1:N, p_treat*N)),
      u = rnorm(nrow(.), 0, sigma_u),
      e = rnorm(nrow(.), 0, sigma_e),
      y = alpha + treatment_effect*treated + ovb_intensity*u + e
    )
}
```

<!-- We set baseline values for the parameters to emulate a somehow realistic observational study. We add the parameter value for delta separately as we will vary the value later and will reuse the vector `baseline_param_panoptic`. -->

For now, we consider very simple baseline parameters, mostly standard normal distributions:

```{r simple_param_panoptic, echo=FALSE}
baseline_param_panoptic <- tibble(
  N = 500,
  p_treat = 0.5,
  sigma_u = 1,
  sigma_e = 1,
  alpha = 1,
  treatment_effect = 0.4, 
  ovb_intensity = 1
)

baseline_param_panoptic %>% kable()
```

Here is an example of data created with our data generating process:

```{r example_data_panoptic, echo=FALSE}
baseline_param_panoptic %>%
  mutate(N = 10) %>% 
  pmap_dfr(generate_data_panoptic) %>% #use pmap to pass the set of parameters
  kable()
```

### Estimation

After generating the data, we can run an estimation. We want to be able to run the estimation for different numbers of clusters. We create artificial clusters, based on the individual identification numbers `id`. When we do not wish to cluster the standard errors, we simply set the cluster variable to be equal to the id so that the "clustering" is at the individual level. Note that we use the function `lm_robust` from the `estimatr` package to compute this clustering.

```{r estimate_panoptic}
estimate_panoptic <- function(data, n_clusters = NA) {
  n_clusters_mod <- ifelse(is.na(n_clusters), nrow(data), n_clusters)
  
  data %>%
    mutate(cluster = cut_number(id, n_clusters_mod, labels = FALSE)) %>%
    lm(data = ., formula = y ~ treated) %>%
    coeftest(vcov = vcovCL, cluster = ~cluster) %>% 
    broom::tidy() %>%
    filter(term == "treatedTRUE") %>%
    rename(p_value = p.value, se = std.error) %>%
    select(estimate, p_value, se) %>%
    mutate(n_clusters = n_clusters)
}

# estimate_panoptic_1 <- function(data, n_clusters = NA) {
#   n_clusters_mod <- ifelse(is.na(n_clusters), nrow(data), n_clusters)
#   
#   data %>%
#     mutate(cluster = cut_number(id, n_clusters_mod, labels = FALSE)) %>%
#     estimatr::lm_robust(
#       data = .,
#       formula = y ~ treated,
#       clusters = cluster
#     ) %>%
#     broom::tidy() %>%
#     as_tibble() %>% 
#     filter(term == "treatedTRUE") %>%
#     rename(p_value = p.value, se = std.error) %>%
#     select(estimate, p_value, se) %>%
#     mutate(n_clusters = n_clusters)
# }

# estimate_panoptic_2 <- function(data, n_clusters = NA) {
# 
#   if (is.na(n_clusters)) {
#     reg <- lm(
#       data = data,
#       formula = y ~ treated
#     )
#   } else {
#     reg <- data %>%
#       mutate(cluster = cut_number(id, n_clusters, labels = FALSE)) %>%
#       estimatr::lm_robust(
#         data = .,
#         formula = y ~ treated,
#         clusters = cluster
#       )
#   }
# 
#   reg %>%
#     broom::tidy() %>%
#     as_tibble() %>% 
#     filter(term == "treatedTRUE") %>%
#     rename(p_value = p.value, se = std.error) %>%
#     select(estimate, p_value, se) %>%
#     mutate(n_clusters = n_clusters)
# }
```

### One simulation

We can now run a simulation, combining `generate_data_panoptic` and `estimate_panoptic`. To do so we create the function `compute_sim_panoptic`. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect. Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.

```{r compute_sim_panoptic}
compute_sim_panoptic <- function(N,
                                 p_treat,
                                 sigma_u,
                                 sigma_e,
                                 alpha,
                                 treatment_effect,
                                 ovb_intensity,
                                 n_clusters = NA) {
  generate_data_panoptic(
    N = N,
    p_treat = p_treat,
    sigma_u = sigma_u,
    sigma_e = sigma_e,
    alpha = alpha,
    treatment_effect = treatment_effect,
    ovb_intensity = ovb_intensity
  ) %>%
  estimate_panoptic(n_clusters = n_clusters) %>%
  mutate(
    true_effect = treatment_effect,
    p_treat = p_treat
  ) 
}
```

### All simulations

We will run the simulations for different sets of parameters by mapping our `compute_sim_p_treat` function on each set of parameters. We thus create a table with all the values of the parameters we want to test, `param_p_treat`. First, we only vary the proportion of unit treated. We will vary the number of clusters in a second analysis.

Note that in this table each set of parameters appears `n_iter` times as we want to run the analysis $n_{iter}$ times for each set of parameters.

```{r set_param_p_treat, echo=FALSE}
fixed_param <- baseline_param_panoptic #%>% rbind(...)
vect_p_treat <- seq(0.1, 0.9, 0.1)
n_iter <- 1000

param_p_treat <- fixed_param %>%
  select(-p_treat) %>% 
  crossing(vect_p_treat) %>%
  rename(p_treat = vect_p_treat) %>%
  crossing(rep_id = 1:n_iter) %>%
  select(-rep_id)
```

We then run the simulations by mapping our `compute_sim_p_treat` function on `param_p_treat`.

```{r run_sim_p_treat, eval=FALSE, echo=FALSE}
tic()
sim_p_treat <- pmap_dfr(param_p_treat, compute_sim_panoptic)
beep()
toc()

# saveRDS(sim_p_treat, here("Outputs/sim_p_treat.RDS"))
```

## Analysis of the results (proportion treated units)

### Quick exploration

First, we quickly explore the results.

```{r exploration_results_p_treat, echo=FALSE, fig.asp=0.7}
sim_p_treat <- readRDS(here("Outputs/sim_p_treat.RDS"))

# sim_p_treat %>% 
#   filter(p_treat %in% sample(vect_p_treat, 4)) %>% 
#   ggplot(aes(x = estimate)) +
#   geom_density() +
#   geom_vline(xintercept = unique(sim_p_treat$true_effect)) +
#   facet_wrap(~ p_treat) +
#   labs(
#     title = "Distribution of the estimates of the treatement effect",
#     subtitle = "For different propoportion of units treated",
#     color = "",
#     fill = "",
#     x = "Estimate of the treatement effect",
#     y = "Density",
#     caption = "The vertical line represents the true effect"
#   )

sim_p_treat %>% 
  ggplot() +
  geom_density(aes(x = estimate, color = as.factor(p_treat)), alpha = 0) +
  labs(
    title = "Distribution of the estimates of the treatement effect",
    subtitle = "Comparison across proportions of units treated ",
    color = "Proporion of units treated",
    fill = "Proporion of units treated",
    x = "Estimate of the treatement effect",
    y = "Density"
  )


sim_p_treat %>% 
  filter(p_treat == 0.2) %>%
  mutate(significant = ifelse(p_value < 0.05, "Significant", "Non significant")) %>% 
  ggplot(aes(x = estimate, fill = significant)) +
  geom_histogram() +
  geom_vline(xintercept = unique(sim_p_treat$true_effect)) +
  labs(
    title = "Distribution of the estimates of the treatement effect conditional on significativity",
    subtitle = paste("For a proportion of", 0.2*100, "% units treated"),
    x = "Estimate of the treatement effect",
    y = "Count",
    fill = "",
    caption = "The vertical line represents the true effect"
  )
```

### Computing bias and type M

We want to compare $\mathbb{E}[\beta_0 - \widehat{\beta}]$ and $\mathbb{E}[|\beta_0 - \widehat{\beta}||signif]$. The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.

```{r summarise_p_treat}
summarise_sim_panoptic <- function(data) {
  data %>%
    mutate(significant = (p_value <= 0.05)) %>%
    group_by(p_treat, n_clusters) %>%
    summarise(
      power = mean(significant, na.rm = TRUE)*100,
      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),
      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),
      bias_all = mean(estimate/true_effect, na.rm = TRUE),
      bias_all_median = median(estimate/true_effect, na.rm = TRUE),
      .groups	= "drop"
    ) %>%
    ungroup()
}

summary_sim_p_treat <- summarise_sim_panoptic(sim_p_treat)
```

### THE graph

To analyze our results, we build a unique and simple graph:

```{r graph_results_p_treat, echo=FALSE, fig.asp=0.7}
summary_sim_p_treat %>%
  ggplot(aes(x = p_treat, y = bias_signif)) +
  geom_line(size = 0.8) +
  labs(
    x = "Proportion of units treated",
    y = expression(paste("Average  ", frac("Estimate", "True Effect"))),
    title = "Evolution of bias with the proportion of units treated",
    subtitle = "For statistically significant estimates"
  )
```

## Varying the number of clusters

We then reproduce a similar type of analysis and graph but varying the number of clusters.


```{r set_param_clusters, echo=FALSE}
fixed_param <- baseline_param_panoptic %>% mutate(treatment_effect = 0.05)
vect_n_clusters <- #c(seq(5, 50, 5), seq(50, 500, 50))
n_iter <- 100

param_clusters <- fixed_param %>%
  crossing(vect_n_clusters) %>%
  rename(n_clusters = vect_n_clusters) %>%
  crossing(rep_id = 1:n_iter) %>%
  select(-rep_id)

tic()
sim_clusters <- pmap_dfr(param_clusters, compute_sim_panoptic)
beep()
toc()

# saveRDS(sim_clusters, here("Outputs/sim_clusters.RDS"))
# sim_clusters <- readRDS(here("Outputs/sim_clusters.RDS"))

summary_sim_clusters <- summarise_sim_panoptic(sim_clusters)

summary_sim_clusters %>%
  ggplot(aes(x = n_clusters, y = bias_signif)) +
  geom_line(size = 0.8) +
  labs(
    x = "Number of clusters",
    y = expression(paste("Average  ", frac("Estimate", "True Effect"))),
    title = "Evolution of bias with the number of clusters",
    subtitle = "For statistically significant estimates"
  )
```


