---
title: "Simulations OVB/type M trade-off: RDD"
author:
  - name: Vincent Bagilet 
    url: https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: Léo Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 200,
               fig.align = "center")  
```  

```{r packages, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse) 
library(knitr) 
library(mediocrethemes)
library(fixest)
library(tictoc)
library(here)
library(beepr)

set_mediocre_all()
```

## Purpose of the document

In this document, we run a simulation exercise to illustrate the existence of a trade-off between Omitted Variable Bias (OVB) and type M error in the case of the Regression Discontinuity Design (RDD). This trade-off is mediated by the size of the bandwidth considered in the analysis. The idea behind this is that the smaller the bandwidth, the more comparable units are and therefore the smaller the risk of OVB is. Yet, with a smaller bandwidth, sample size, and thus power, decrease, increasing the risk of type M error. 

## An illustrative example

To illustrate this trade-off between OVB and type M error, we consider a standard application of the RD design in economics of education where an award or grant is assigned based on the score obtained by students on a standardized test. Students with test scores above a given threshold  receive the award while those below do not. The effect of the treatment is measured by comparing the outcomes of students just below and just above this threshold. 

[Thistlewaite and Campbell (1960)](https://psycnet.apa.org/record/1962-00061-001) introduced the concept of RDD using this type of quasi-experiment. In their paper, they take advantage of a sharp discontinuity in the assignment of an award (a Certificate of Merit) based on qualifying scores at a test. This type of analysis is still used today and many papers leveraging similar methodologies have been published since this seminal work. For instance, [Jacob and Lefgren (2004)](https://direct.mit.edu/rest/article/86/1/226/57489/Remedial-Education-and-Student-Achievement-A) exploit this type of discontinuity to study the impact of  summer school and grade retention programs on test scores. Students who score below a given score are required to attend a summer school and to retake the test. Students who do not pass the second have to repeat the grade. 

### Modelisation choices

In the present analysis, we build our simulations to replicate a similar type of quasi-experiment. In our fictional example, all students scoring below a cutoff $C$ in a qualification test are required to take additional lessons. We want to estimate the effect of these additional lessons on scores on a final test taken by all students a year later. 

We assume that the final score of student $i$, $Final_i$, is correlated with their qualification score $Qual_i$, their treatment status $T_i$, *ie* whether student $i$ received additional lessons or not, and their unobserved ability $U_i$. 

The DGP can be represented using the following Directed Acyclic Graph (DAG): 

```{r echo=FALSE, out.width='40%'}
include_graphics(here("Graphs/DAG_RDD.png"))
```

Final test scores are thus defined as follows:

$$Final_{i} = \alpha + \beta T_i + \gamma Qual_{i} +  \delta U_i + \epsilon_{i}$$
Where $\alpha$ is a constant and $e \sim \mathcal{N}(0, \sigma_{e})$ noise. The parameter of interest is $\beta$. Translating this into a potential outcomes framework, we have $Final_i(0) = \alpha + \gamma Qual_{i} + \delta U_i + \epsilon_{i}$ and $Final_i(1) = \alpha + \gamma Qual_{i} + \beta + \delta U_i + \epsilon_{i}$

<!-- on one of the analyses Jacob and Lefgren (2008): estimating the impact of the retention and summer school programs on third graders' maths scores, one year later. More precisely, we consider the same type of treatment allocation mechanism and outcome variable but greatly simplify their setting. The distribution of our variables is chosen to emulate the two first moments of the distributions they observe. For simplicity, we however assume that the variables are normally distributed.  -->

To simplify, we assume a constant treatment effect and a sharp treatment allocation such that $T_i = \mathbb{I}[Qual_{i} < C]$. We also assume that the unobserved ability affect  the qualification and final 

We set: 

- $N$ the number of students
- $U \sim \mathcal{N}(\mu_u, \sigma_u)$ the unobserved ability.
- We assume that the qualification score is composed of two parts: $H \sim \mathcal{N}(\mu_h, \sigma_h)$ and is affected linearly by by the unobserved ability such that $Qual_i = H_i + \delta U_i$. For simplicity we assume that the unobserved availability affects the qualification and final score in a similar way and therefore with the same intensity $\delta$.
- For now and for simplicity, we set a fixed grade threshold $C$ such that $T_i = \mathbb{I}[Qual_{i} < C]$.
- $e \sim \mathcal{N}(0, \sigma_e)$
- $Final_{i} = \alpha  + \beta T_i + \gamma Qual_{i} +  \delta U_i + e_{i}$

### Data generation

We write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis. 

```{r DGP}
generate_data_rdd <- function(delta,
                              N = 1000, 
                              mu_u = 1,
                              sigma_u = 2,
                              mu_h = 74, 
                              sigma_h = 15, 
                              sigma_e = 2, 
                              alpha = 1, 
                              beta = 2,
                              gamma = 0.5
                              ) {
  
  # threshold_pos <- runif(1, 0.2, 0.8) 
  
  data <- 
    tibble(id = 1:N) %>% 
    mutate(
      u = rnorm(nrow(.), mu_u, sigma_u),
      qual = rnorm(nrow(.), mu_h, sigma_h) + delta*u^2,
      # qual = ifelse(qual < 0, 0, qual),
      # qual = ifelse(qual > 100, 100, qual),
      e = rnorm(nrow(.), 0, sigma_e),
      threshold = quantile(qual, 0.5),
      treated = qual < threshold,
      final0 = alpha + gamma*qual + delta*u^2 + e,
      final1 = final0 + beta,
      final = final0 + beta*treated
    )
  
  return(data)
}
```

Before running the estimation, we will need to define the bandwidth and consider observations only in this bandwidth when running our regression. We therefore create a function to define this bandwidth by adding a variable to the data set `treated_bw` that is equal to `NA` if the observations is outside of the bandwidth, `TRUE` if the observation falls in the bandwidth and the student is treated and `FALSE`  if the observation falls in the bandwidth and the student is not treated. We define the band width as a proportion of the standard deviation of the qualification test score.

```{r}
define_bw <- function (data, bw = 0.1) {
  data <- data %>% 
    mutate(
      treated_bw = ifelse(
        # dplyr::between(qual, unique(threshold) - bw/2*sd(qual), unique(threshold) + bw/2*sd(qual)),
        dplyr::between(
          qual, 
          quantile(qual, 0.5 - bw), 
          quantile(qual, 0.5 + bw)
        ), 
        treated, 
        NA
      )
    )
} 
```

### Exploring the generated data

Before going any further, we explore an example of data generated with this DGP to check that the relations between variables are as desired.

```{r}
test_data <- generate_data_rdd(delta = 1, N = 10000) %>% 
  define_bw(0.4)

test_data %>% 
  ggplot(aes(x = u, y = qual)) + 
  geom_point()

test_data %>% 
  ggplot(aes(x = u, y = final)) + 
  geom_point()

generate_data_rdd(delta = 1, N = 10000) %>% 
  define_bw(0.5) %>% 
  ggplot(aes(x = qual, y = final, color = treated_bw)) + 
  geom_point() +
  geom_smooth(method = "lm", fullrange = TRUE)

test_data %>% 
  ggplot(aes(x = qual, y = u, color = treated_bw)) + 
  geom_point()

test_data %>% 
  ggplot(aes(x = final, fill = treated_bw, color = treated_bw)) + 
  geom_density()

generate_data_rdd(delta = 1, N = 100000) %>% 
  define_bw(0.5) %>% 
  ggplot(aes(x = u, fill = treated_bw, color = treated_bw)) + 
  geom_density()


test_data %>% 
  ggplot(aes(x = qual, y = u, color = treated_bw)) + 
  geom_point()
```

### Estimation

After generating the data, we can run an estimation. To do so, we only consider observations within the bandwidth, run the estimation and wrangle the output to return the parameters of interest only. Note that we will need the true effects to run power calculations. Therefore we write a short function to compute it and add this information to the data. 

```{r estimate}
compute_true_effect_rdd <- function(data) {
  treated_data <- data %>% 
    filter(.data$treated) 
  
  return(mean(treated_data$final1 - treated_data$final0))
}  

estimate_rdd <- function(data, bw) {
  data_in_bw <- data %>% 
    define_bw(bw = bw) %>% 
    filter(!is.na(treated_bw))
  
  reg <- lm(
    data = data_in_bw, 
    formula = final ~ treated + qual
  ) %>% 
  broom::tidy() %>%
  filter(term == "treatedTRUE") %>%
  rename(p_value = p.value, se = std.error) %>%
  select(estimate, p_value, se) %>%
  mutate(true_effect = compute_true_effect_rdd(data))
  
  return(reg)
}
```

### Simulations

We first create a function to run one simulation. It creates the data and run the estimations (it is just a simple combination of `generate_data_rdd` and `estimate_rdd`)

```{r}
compute_simulation_rdd <- function(ovb_intensity, bw) {
  generate_data_rdd(delta = ovb_intensity) %>% 
    estimate_rdd(bw = bw) %>% 
    mutate(
      bw = bw,
      ovb_intensity = ovb_intensity
    )
} 
```

We then create a table with all the values of the parameters we want to test. 

```{r set_param}
vect_bw <- seq(0.05, 0.4, 0.05)
vect_ovb_intensity <- c(2)
n_iter <- 1000

param_rdd <- crossing(vect_bw, vect_ovb_intensity) %>% 
  rename(bw = vect_bw, ovb_intensity = vect_ovb_intensity) %>% 
  crossing(rep_id = 1:n_iter) %>% 
  select(-rep_id)
```

We then run the simulations by mapping our `compute_simulation_rdd` function on the set of parameters.

```{r run_sim, eval=FALSE}
tic()
simulations_rdd <- pmap_dfr(param_rdd, compute_simulation_rdd)
beep()
toc()

# saveRDS(simulations_rdd, here("Outputs/simulations_rdd.RDS"))
```

## Analysis of the results

First, we quickly explore the results.

```{r}
simulations_rdd %>% 
  filter(bw == vect_bw[1]) %>% 
  ggplot(aes(x = estimate)) + 
  geom_density() +
  geom_vline(aes(xintercept = mean(estimate))) +
  facet_wrap(~ ovb_intensity)

simulations_rdd %>% 
  filter(bw == vect_bw[1]) %>% 
  mutate(significant = (p_value < 0.05)) %>% 
  ggplot(aes(x = estimate, fill = significant)) + 
  geom_histogram() +
  geom_vline(aes(xintercept = mean(estimate))) +
  facet_wrap(~ ovb_intensity)

simulations_rdd %>% 
  filter(bw == 0.4) %>% 
  mutate(significant = (p_value < 0.05)) %>% 
  group_by(ovb_intensity) %>% 
  mutate(sim_id = row_number()) %>% 
  ungroup() %>% 
  ggplot(aes(x = estimate, color = significant)) + 
  geom_point(aes(x = sim_id, y = estimate)) +
  geom_hline(aes(yintercept = mean(estimate))) +
  facet_wrap(~ ovb_intensity)

simulations_rdd %>% 
  group_by(ovb_intensity, bw) %>% 
  summarise(average_effect = mean(estimate))
```


We want to compute $\mathbb{E}[| \beta_0 - \widehat{\beta_{RDD}}|]$ and $\mathbb{E}[|\beta_0 - \widehat{\beta_{RDD}}||signif]$

```{r summarise}
# simulations_rdd <- readRDS(here("Outputs/simulations_rdd.RDS"))

summarise_simulations <- function(data) {
  data %>%
    mutate(significant = (p_value <= 0.05)) %>% 
    group_by(ovb_intensity, bw) %>%
    summarise(
      power = mean(significant, na.rm = TRUE)*100, 
      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),
      bias_sign = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),
      bias_all = mean(estimate/true_effect, na.rm = TRUE),
      .groups	= "drop"
    ) %>% 
    ungroup()
} 

summary_simulations_rdd <- summarise_simulations(simulations_rdd)

# saveRDS(summary_simulations_rdd, here("Outputs/summary_simulations_rdd.RDS"))
```

### Graph

```{r graph_results}
summary_simulations_rdd %>% 
  pivot_longer(cols = c(bias_sign, bias_all), names_to = "measure")%>% 
  mutate(
    measure = ifelse(measure == "bias_sign", "Significant", "All"),
    ovb_intensity_name = paste("OVB intensity:", ovb_intensity)
  ) %>% 
  ggplot() + 
  # geom_point() + 
  geom_line(aes(x = bw, y = value, color = measure), linetype = "dashed") +
  facet_wrap(~ ovb_intensity_name) +
  labs(
    x = "Bandwidth size", 
    y = "Bias",
    title = "Evolution of bias with bandwith size, conditional on significativity",
    subtitle = "For several values of OVB intensity",
    caption ="Bandwidth as a proportion of the standard deviation of the qualification score"
  )

# summary_simulations_rdd %>% 
#   pivot_longer(cols = c(type_m, bias), names_to = "measure") %>% 
#   mutate(
#     measure = ifelse(measure == "type_m", "Significant", "Non significant"),
#     ovb_intensity_name = paste("OVB intensity:", ovb_intensity)
#   ) %>% 
#   ggplot() + 
#   # geom_point() + 
#   geom_line(aes(x = bw, y = value, color = measure), linetype = "dashed") +
#   facet_wrap(~ ovb_intensity_name) +
#   labs(
#     x = "Bandwidth size", 
#     y = "Type M",
#     title = "Evolution of bias with bandwith size, conditional on significativity",
#     subtitle = "For several values of OVB intensity",
#     caption ="Bandwidth as a proportion of the standard deviation of the qualification score"
#   )
```


