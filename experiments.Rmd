---
title: "Analyzing Experimental Studies for Intuition"
description: "In this document, we analyze replication of experimental studies to illustrate the concept of type M error."
author:
  - name: Vincent Bagilet 
    url: https://vincentbagilet.github.io/
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: LÃ©o Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup_IV, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 200,
               fig.align = "center")  
```  

```{r packages_IV, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse) 
library(knitr) 
library(mediocrethemes)
library(haven)
library(here)
library(readxl)
library(retrodesign)
library(gganimate)

set.seed(3)

set_mediocre_all(pal = "coty")
```

## Laboratory experiments

First, we consider the paper [Camerer et al (2016)](https://www.science.org/doi/10.1126/science.aaf0918). The authors of this paper replicated laboratory experiments in economics. We analyze their results through the prism of statistical power.

They report their replication results, alongside the original results, in table S1 Note that some p-values are strictly smaller than 0.001. As we do not have more information, we set them to 0.001. We also back out the standard errors from the p-values as they will be useful for our analysis. 

We compute the power of the initial analysis if the true effect is in fact equal to the replication's. We store the results in `retro_camerer`.

```{r camerer_retro}
rep_camerer <- read_excel(here("Misc", "rep_camerer.xlsx")) %>% 
  mutate(
    se_original = effect_original/qnorm(1 - pvalue_original), #incorrect
    se_rep = effect_rep/qnorm(1 - pvalue_rep) #incorrect
  ) 

retro_camerer <- rep_camerer %>% 
  mutate(
    se_original = effect_original/qnorm(1 - pvalue_original), #incorrect
    se_rep = effect_rep/qnorm(1 - pvalue_rep) #incorrect
  ) %>% 
  select(A = effect_rep, s = se_original) %>% 
  # select(A, s) %>% 
  pmap_dfr(retrodesign) %>% 
  cbind(rep_camerer) %>% 
  as_tibble()
```

### Analyzing all studies

We quickly plot and analyze the results obtained, *ie* the distribution of the exaggeration ratio and power.

```{r camerer_retro_analysis, echo=FALSE}
retro_camerer %>% 
  ggplot() +
  geom_histogram(aes(x = exaggeration)) +
  labs(
    title = "Distribution of the exaggeration ratio in the original studies",
    subtitle = "If the true effect was equal to the replicated one",
    x = "Exaggeration ratio",
    y = "Number of studies"
  )

retro_camerer %>%
  ggplot() +
  geom_histogram(aes(x = power)) +
  labs(
    title = "Distribution of the power in the original studies",
    subtitle = "If the true effect was equal to the replicated one",
    x = "Power",
    y = "Number of studies"
  )
# 
# retro_camerer %>% 
#   ggplot() +
#   geom_histogram(aes(x = power))
# 
# retro_camerer %>% 
#   count(exaggeration > 1.5)
```

The median power would be `r round(median(retro_camerer$power),2)`%. The median replicated estimates is equal to  `r round(median(1/retro_camerer$exaggeration), 2)` times the original estimate.

We then compute the number and proportion of original studies that were statistically significant. 

```{r echo=FALSE}
rep_camerer %>% 
  mutate(
    original_significant = ifelse(pvalue_original < 0.05, "Yes", "No")
  ) %>% 
  group_by(original_significant) %>% 
  summarise(
    nb = n(),
    prop = n()/nrow(.)
  ) %>% 
  kable(col.names = c(
    "Original estimate statistically significant", 
    "Number",
    "Proportion"),
    digits = 2
  )
```

We then do the same thing for the replication studies.

```{r echo=FALSE}
rep_camerer %>% 
  mutate(
    rep_significant = ifelse(pvalue_rep < 0.05, "Yes", "No")
  ) %>% 
  group_by(rep_significant) %>% 
  summarise(
    nb = n(),
    prop = n()/nrow(.)
  ) %>% 
  kable(col.names = c(
    "Replication estimate statistically significant", 
    "Number",
    "Proportion"),
    digits = 2
  )
```

We then compute the proportion of original studies that would have adequate power as defined by the customary and arbitrary 80% threshold, still assuming that the true effect is equal to the replication one.

```{r echo=FALSE}
retro_camerer %>% 
  mutate(
    adequate_power = ifelse(power >= 0.8, "Yes", "No")
  ) %>% 
  group_by(adequate_power) %>% 
  summarise(
    nb = n(),
    prop = n()/nrow(.)
  ) %>% 
  kable(col.names = c(
    "Adequate power", 
    "Number",
    "Proportion"),
    digits = 2
  )
```

### Focus on one particular study

Here, we focus on one particular study in order to illustrate in more details the problem at play. We want to simulate what could have yielded replication of the initial study if the true effect was equal to the replication estimate.

We select one of the studies and draw the graph of interest.

```{r graph_retrodesign_camerer, echo=FALSE, fig.height=4.3, fig.width=9, out.width=1200, dpi = 700}
random_study <- rep_camerer %>% 
  # slice_sample(n = 1) %>%
  slice(10) 

data_graph_distrib <- rnorm(random_study$effect_rep, random_study$se_original, n = 500) %>% 
  as_tibble() %>% 
  mutate(
    n = row_number(),
    non_significant = dplyr::between(
      value, 
      - 1.96*sd(value), 
      1.96*sd(value)
    ),
    significant = ifelse(non_significant, "Non significant", "Significant") 
  ) 

data_graph_distrib %>% 
  ggplot(aes(x = n, y = value, color = significant)) + 
  geom_point(alpha = 0.8) +
  #original study
  geom_point(aes(x = -30, y = random_study$effect_original), color = "darkred", size = 2) +
  geom_linerange(aes(
    x = -30,
    ymin = random_study$effect_original - 1.96*random_study$se_original,
    ymax = random_study$effect_original + 1.96*random_study$se_original), color = "darkred") +
  #replication 
  geom_point(aes(x = -20, y = random_study$effect_rep), color = "darkblue", size = 2) +
  geom_linerange(aes(
    x = -20,
    ymin = random_study$effect_rep - 1.96*random_study$se_rep,
    ymax = random_study$effect_rep + 1.96*random_study$se_rep), color = "darkblue") +
  #repliccation with design of the original study
  geom_point(aes(x = -10, y = random_study$effect_rep), color = "gray50", size = 2) +
  geom_linerange(aes(
    x = -10,
    ymin = random_study$effect_rep - 1.96*random_study$se_original,
    ymax = random_study$effect_rep + 1.96*random_study$se_original), color = "gray50") +
  # geom_hline(aes(yintercept = mean(value)), size = 0.8) +
  geom_hline(aes(yintercept = 0), size = 0.3, linetype = "solid") +
  labs(
    title = "Illustration of type M errors",
    subtitle = "500 draws of an estimate ~ N(Effect size in replication, std err in original study)",
    x = "Draw",
    y = "Point estimate",
    caption = "The red dot represents the estimate found in the original study"
  ) +
  scale_color_discrete(name = "")
  # transition_layers(layer_length = 2, from_blank = FALSE) #to animate
```

- In red is the estimate from the original study and its 95% confidence interval
- The estimate is significant and has been published. Yet, it is pretty noisy.
- In blue is the estimate from the replicated study and its 95% confidence interval
- We notice that this second estimate is both more precise and smaller than the initial one. It still remains noisy
- Let's assume that the true effect is actually equal to this second estimate (note that this is unlikely)
- Would the design of the initial study be good enough to detect this true effect? *ie* if we replicated the initial study, could we reject the null of no effect (knowing that the true effect is equal to the replicated estimate)
- In gray is the estimate form the replicated study but with a standard error equal to the initial study's (approximately the standard errors that would have been obtained with the design of the initial study)
- This estimate is non significant. In this instance, we would not have been able to reject the null of no effect
- Now, if we replicate this study 500 times, running 500 lab experiments, in some cases we would get statisitcally significant estimates (the beige dots) and in some others non statistically significant ones (the green dots)
- If we would have been a bit more lucky, we could have gotten a sample of individuals that would have yielded one of the beige estimates
- Now, we notice that, on average, statistically significant estimates overestimate the true effect by a factor 1.7 (average of 0.53 while the true effect is 0.31). Gelman and Carlin call this inflation factor type M error.
- In this case, the power is basically the proportion of statistically significant estimates
- If the study had more power, the sd would be smaller and most estimates would be statistically significative (because there is indeed a non null effect)
- But since the power is low (33%), if by chance the sample of individuals we get yields a statistically significant estimate, this estimate will overestimate the true effect

<!-- ```{r} -->
<!-- data_graph_distrib %>%  -->
<!--   group_by(significant) %>%  -->
<!--   summarise( -->
<!--     prop = n()/nrow(.), -->
<!--     mean = mean(value) -->
<!--   ) -->
<!-- ``` -->





## Randomized Control Trials

We want to look at replications of RCTs in Development Economics. To do so, we use [the list of replication papers put together by Sandip Sukhtankar](https://www.aeaweb.org/articles?id=10.1257/aer.p20171120). 

We gather the list of RCTs that have been replicated in Development Economics.

```{r rep_dvpt}
rep_dvpt <- read_dta(here("Misc", "replication_data_final.dta"))

# rep_dvpt %>% 
#   filter((RCT == "Yes") & (Replicated == "Replicated")) %>% 
#   .$ReplicationPaperTitle
```





