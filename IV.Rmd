---
title: "Simulations IV"
description: "In this document, we run a simulation exercise to illustrate the existence of a trade-off between Omitted Variable Bias (OVB) and type M error in the context of an Instrumental Variable (IV) strategy."
author:
  - name: Vincent Bagilet 
    url: https://vincentbagilet.github.io/
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: LÃ©o Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup_IV, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 200,
               fig.align = "center")  
```  

```{r packages_IV, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse) 
library(knitr) 
library(mediocrethemes)
library(AER)
library(tictoc)
library(here)
library(beepr)

set_mediocre_all(pal = "coty") #, background = TRUE) #for presentations
```

## Summary and intuition

In the case of the IV, the OVB/type M trade-off is  mediated by the 'strength' of the instrument considered.

## An illustrative example

For readability and to illustrate this loss in power, we consider an example setting. For this illustration we could use a large variety of Data Genereting Processes (DGP), both in terms of distribution of the variables and of relations between them. We narrow this down to an example setting, considering an analysis of health impacts of air pollution. Our point should stand in more general settings. 

A threat of confounders often arises when analyzing the health effects of air pollution. To estimate such an effect causally, one can consider exogeneous shocks to air pollution. In the present analysis, we consider the example of plant closures as exogeneous shocks.

### Modelisation choices

In the present analysis, we build our simulations to replicate an analysis of the causal health effects of air pollution. In such analyses, in order to avoid potential confounders, researchers often instrument air pollution with exogeneous shocks such as plant closure or thermal inversions for instance. For simplicity we abstract from the panel dimension in this analysis. One can consider that this is an analysis of the impact of life-long exposure to air pollution on health. Again, this illustration is very simplistic and is mostly consider to simplify the understanding of the interactions between the different variables. 

The DGP can be represented using the following Directed Acyclic Graph (DAG): 

```{r DAG_RDD, echo=FALSE, out.width='70%'}
include_graphics(here("images/DAG_IV.png"))
```

The DGP for the health status of person $i$ is defined as follows:

$$Health_{i} = \alpha + \beta Poll_{i} + \delta u_{i} + e^{(h)}_{i}$$

Where $\alpha$ is a constant, $u$ represents an unobserved variable and $e \sim \mathcal{N}(0, \sigma_{e})$ noise. $\beta$ is the parameter of interest.

The DGP for the pollution data is as follows: 

$$Poll_{i} = \gamma + \lambda z_{i} + \eta u_{i} + e^{(p)}_{i}$$
Where $\mu$ is a constant, $z$ is the instrument for air pollution and $\tilde{e} \sim \mathcal{N}(0, \sigma_{\tilde{e}})$ noise. We refer to $\lambda$ as "IV strength".

The effect of air pollution on health is estimated using 2 Stages Least Squares.

<!-- To simplify, we consider the following assumptions: -->

<!-- -  -->

More precisely, we set: 

- $N$ the number of observations
- $z \sim \mathcal{N}(0, \sigma_{z}^{2})$ or $z \sim \text{Bernoulli}(p_z)$ the instrument
- $u \sim \mathcal{N}(0, \sigma_{u}^{2})$ the unobserved variable
- $e^{(h)} \sim \mathcal{N}(0, \sigma_{e_h}^{2})$
- $e^{(p)} \sim \mathcal{N}(0, \sigma_{e_p}^{2})$

### Data generation

We write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis. 

Note that the parameter `type_z` describes whether z is a random sample from a normal or bernoulli distribution. It can take the values `normal` or `bernoulli`. `param_z` represent either $\sigma_z$ if $z$ is normal or $p_z$ if it is Bernoulli.
 
```{r DGP_IV}
generate_data_IV <- function(N,
                             type_z, #"normal" or "bernoulli"
                             param_z,
                             sigma_u,
                             sigma_eh,
                             sigma_ep,
                             alpha_y,
                             alpha_x,
                             treatment_effect,
                             iv_strength,
                             ovb_intensity
                             ) {
  
  if (type_z == "bernoulli") {
    z_gen <- rbernoulli(N, param_z)
  } else if (type_z == "normal") {
    z_gen <- rnorm(N, 0, param_z)
  } else {
    stop("type_z must be either 'bernoulli' or 'normal'")
  } 
  
  data <- tibble(id = 1:N) %>% 
    mutate(
      z = z_gen,
      u = rnorm(nrow(.), 0, sigma_u),
      e_h = rnorm(nrow(.), 0, sigma_eh),
      e_p = rnorm(nrow(.), 0, sigma_ep),
      x = alpha_x + iv_strength*z + ovb_intensity*u + e_p,
      y = alpha_y + treatment_effect*x + ovb_intensity*u + e_h
    )
  
  return(data)
}
```

We set baseline values for the parameters to emulate a somehow realistic observational study in this field. We add the parameter value for delta separately as we will vary the value later and will reuse the vector `baseline_parameters_IV`.

```{r simple_param_IV}
simple_parameters_IV <- tibble(
  N = 500,
  type_z = "normal",
  param_z = 1,
  sigma_u = 1,
  sigma_eh = 1,
  sigma_ep = 1,
  alpha_y = 0,
  alpha_x = 0,
  treatment_effect = 1
)
```

Here is an example of data created with our data generating process:

```{r example_data_IV, echo=FALSE}
simple_parameters_IV %>% 
  mutate(N = 10, iv_strength = 0.2, ovb_intensity = 0.4) %>% 
  pmap_dfr(generate_data_IV) %>% #use pmap to pass the set of parameters
  kable()
```

<!-- ```{r} -->
<!-- ex_data_IV <- simple_parameters_IV %>%  -->
<!--   mutate(N = 1000, iv_strength = 0.2, ovb_intensity = 0.4) %>%  -->
<!--   pmap_dfr(generate_data_IV)  -->

<!-- ex_data_IV %>%  -->
<!--   ggplot(aes(x = z, y = y)) + -->
<!--   geom_point() + -->
<!--   geom_smooth(method = "lm") -->
<!-- ``` -->

### Estimation

After generating the data, we can run an estimation. We want to compare the IV and the OLS for different IV strength values. Hence, we need to estimate both an IV and an OLS and return both set of outcomes of interest.

```{r estimate_IV}
estimate_IV <- function(data) {
  reg_IV <- ivreg(
    data = data, 
    formula = y ~ x | z
    ) 
  
  fstat_IV <- summary(
    reg_IV, 
    diagnostics = TRUE
  )$diagnostics["Weak instruments", "statistic"]
  
  reg_IV <- reg_IV %>% 
    broom::tidy() %>%
    mutate(
      model = "IV",
      fstat = fstat_IV
    )
  
  reg_OLS <- lm(
    data = data, 
    formula = y ~ x
    ) %>% 
    broom::tidy() %>%
    mutate(
      model = "OLS",
      fstat = NA
    )
  
  reg_OLS_unbiased <- lm(
    data = data, 
    formula = y ~ x + u
    ) %>% 
    broom::tidy() %>%
    mutate(
      model = "OLS unbiased",
      fstat = NA
    )
  
  reg <- reg_IV %>% 
    rbind(reg_OLS) %>% 
    rbind(reg_OLS_unbiased) %>% 
    filter(term == "x") %>%
    rename(p_value = p.value, se = std.error) %>%
    select(estimate, p_value, se, fstat, model) %>% 
  
  return(reg)
}
```

### One simulation

We can now run a simulation, combining `generate_data_IV` and `estimate_IV`. To do so we create the function `compute_sim_IV`. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the F-statistic for the IV, the true effect, the IV strength and the intensity of the OVB considered (ovb_intensity). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.

```{r compute_sim_IV}
compute_sim_IV <- function(N,
                           type_z,
                           param_z,
                           sigma_u,
                           sigma_eh,
                           sigma_ep,
                           alpha_y,
                           alpha_x,
                           treatment_effect,
                           iv_strength,
                           ovb_intensity) {
  generate_data_IV(
    N = N,
    type_z = type_z,
    sigma_u = sigma_u,
    param_z = param_z,
    sigma_eh = sigma_eh,
    sigma_ep = sigma_ep,
    alpha_y = alpha_y,
    alpha_x = alpha_x,
    treatment_effect = treatment_effect,
    iv_strength = iv_strength,
    ovb_intensity = ovb_intensity
  ) %>%
    estimate_IV() %>%
    mutate(
      iv_strength = iv_strength,
      ovb_intensity = ovb_intensity,
      true_effect = treatment_effect
    )
} 
```

### All simulations

We will run the simulations for different sets of parameters by mapping our `compute_sim_IV` function on each set of parameters. We thus create a table with all the values of the parameters we want to test `param_IV`. Note that in this table each set of parameters appears `n_iter` times as we want to run the analysis $n_{iter}$ times for each set of parameters.

```{r set_param_IV}
baseline_parameters <- tibble(
  N = 500,
  type_z = "normal",
  param_z = 1,
  sigma_u = 1,
  sigma_eh = 1,
  sigma_ep = 1,
  alpha_y = 0,
  alpha_x = 0,
  treatment_effect = 1
)

fixed_parameters <- baseline_parameters #%>% rbind(...)
vect_iv_strength <- c(seq(0.05, 0.4, 0.05), seq(0.4, 0.6, 0.1))
# vect_iv_strength <- c(0.1)
vect_ovb_intensity <- c(0.4)
n_iter <- 1

param_IV <- fixed_parameters %>% 
  crossing(vect_iv_strength, vect_ovb_intensity) %>% 
  rename(iv_strength = vect_iv_strength, ovb_intensity = vect_ovb_intensity) %>% 
  crossing(rep_id = 1:n_iter) %>% 
  select(-rep_id)
```

We then run the simulations by mapping our `compute_sim_IV` function on `param_IV`.

```{r run_sim_IV, eval=FALSE}
tic()
simulations_IV <- pmap_dfr(param_IV, compute_sim_IV)
beep()
toc()

# saveRDS(simulations_IV, here("Outputs/simulations_IV.RDS"))
```

## Analysis of the results

### Quick exploration

First, we quickly explore the results.

```{r exploration_results_IV, echo=FALSE, fig.asp=0.7}
simulations_IV <- readRDS(here("Outputs/simulations_IV.RDS"))

simulations_IV %>% 
  filter(between(estimate, 0, 2)) %>%
  filter(ovb_intensity == sample(vect_ovb_intensity, 1)) %>% 
  filter(iv_strength %in% c(0.1, 0.2, 0.4, 0.6)) %>% 
  mutate(iv_strength = str_c("IV strength: ", iv_strength)) %>% 
  ggplot(aes(x = estimate, fill = model, color = model)) +
  geom_vline(xintercept = 1) +
  geom_density() +
  facet_wrap(~ iv_strength) +
  labs(
    title = "Distribution of the estimates of the treatement effect",
    subtitle = "For different IV strengths and models",
    color = "",
    fill = "",
    x = "Estimate of the treatement effect",
    y = "Density",
    caption = "The vertical line represents the true effect"
  )

simulations_IV %>%
  filter(between(estimate, -0.5, 2.5)) %>%
  filter(model == "IV") %>%
  filter(ovb_intensity == sample(vect_ovb_intensity, 1)) %>%
  ggplot() +
  # geom_density_ridges(aes(
  geom_density(aes(
    x = estimate, 
    # y = iv_strength,
    color = as.factor(iv_strength), 
    fill = as.factor(iv_strength)),
    alpha = 0.05
  ) +
  labs(
    title = "Distribution of the estimates of the treatement effect",
    subtitle = "Comparison across IV strengths",
    color = "IV strength",
    fill = "IV strength",
    x = "Estimate of the treatement effect",
    y = "Density",
    caption = "For readibility, extreme estimates are filtered out"
  )

data_one_sim_IV <- simulations_IV %>% 
  filter(between(estimate, -1, 2)) %>% 
  filter(iv_strength == 0.1) %>% 
  filter(ovb_intensity == sample(vect_ovb_intensity, 1)) %>% 
  mutate(significant = ifelse(p_value < 0.05, "Significant", "Non significant")) 

data_one_sim_IV %>% 
  ggplot(aes(x = estimate, fill = significant)) +
  geom_vline(xintercept = 1) +
  geom_histogram() +
  facet_wrap(~ model) +
  labs(
    title = "Distribution of the estimates of the treatement effect conditional on significativity",
    subtitle = paste(
      "For different models (IV strength =", 
      unique(data_one_sim_IV$iv_strength), 
      "and OVB intensity = ", 
      unique(data_one_sim_IV$ovb_intensity), ")"
    ),
    x = "Estimate of the treatement effect",
    y = "Count",
    fill = "",
    caption = "The sample is restricted to estimates relatively close to the true value"
  )
```

We notice that the OLS is always biased and that the IV is never biased. However, for limited IV strengths, the distribution of the estimates flattens. The smaller the IV strength, the most like it is to get an estimate away from the true value, even though the expected value remains equal to the true effect size. 
<!-- We can notice that statistically significant estimates are on average located further away from zero that  -->

```{r distrib_for_presentation, eval=TRUE, include=FALSE}
#Graph only for presentations, not compiled in the main .Rmd
simulations_IV %>% 
  filter(between(estimate, 0.5, 1.7)) %>%
  filter(ovb_intensity == sample(vect_ovb_intensity, 1)) %>% 
  filter(iv_strength == 0.4) %>% 
  mutate(iv_strength = str_c("IV strength: ", iv_strength)) %>% 
  ggplot(aes(x = estimate, fill = model, color = model)) +
  geom_vline(xintercept = 1) +
  geom_density() +
  # facet_wrap(~ iv_strength) +
  labs(
    title = "Distribution of the estimates of the treatement effect",
    subtitle = "Comparison across models different models",
    color = "",
    fill = "",
    x = "Estimate of the treatement effect",
    y = "Density",
    caption = "The vertical line represents the true effect"
  ) +
  theme_mediocre(pal = "coty", background = TRUE)
```


### Computing bias and type M

We want to compare $\mathbb{E}[\beta_0 - \widehat{\beta_{i}}]$ and $\mathbb{E}[|\beta_0 - \widehat{\beta_{IV}}||signif]$. The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance. 

```{r summarise_IV}
summarise_simulations <- function(data) {
  data %>%
    mutate(significant = (p_value <= 0.05)) %>% 
    group_by(ovb_intensity, iv_strength, model) %>%
    summarise(
      power = mean(significant, na.rm = TRUE)*100, 
      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),
      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),
      bias_all = mean(estimate/true_effect, na.rm = TRUE),
      bias_all_median = median(estimate/true_effect, na.rm = TRUE),
      median_fstat = mean(fstat, na.rm = TRUE),
      .groups	= "drop"
    ) %>% 
    ungroup()
} 

summary_simulations_IV <- summarise_simulations(simulations_IV)
# saveRDS(summary_simulations_IV, here("Outputs/summary_simulations_IV.RDS"))
```

### Graph

To analyze our results, we build a unique and simple graph:

```{r graph_results_IV, echo=FALSE, fig.asp=0.7}
summary_simulations_IV %>% 
  filter(model %in% c("OLS", "IV")) %>% 
  mutate(
    ovb_intensity_name = paste("OVB intensity:", ovb_intensity)
  ) %>%
  ggplot(aes(x = iv_strength, y = bias_signif, color = model)) + 
  # geom_point() +
  geom_line(size = 0.8) +
  facet_wrap(~Â ovb_intensity_name) +
  labs(
    x = "IV strength", 
    y = expression(paste("Average  ", frac("Estimate", "True Effect"))),
    color = "Model",
    title = "Evolution of bias with intensity of the IV",
    subtitle = "For statistically significant estimates"
  ) 

# summary_simulations_IV %>% 
#   pivot_longer(c(bias_signif, bias_all), names_to = "bias_type", values_to = "bias") %>% 
#   ggplot(aes(x = iv_strength, y = bias, color = bias_type)) + 
#   geom_line(size = 0.8) 
```

Of course, if one considers all estimates, as the IV is unbiased, this issue does not arise. For now, we consider the median because for very low IV strength, we get very extreme values. We need to investigate this further.

```{r graph_results_all_IV, echo=FALSE, fig.asp=0.7}
summary_simulations_IV %>% 
  mutate(
    ovb_intensity_name = paste("OVB intensity:", ovb_intensity)
  ) %>%
  ggplot(aes(x = iv_strength, y = bias_all_median, color = model)) + 
  # geom_point() +
  geom_line(size = 0.8) +
  facet_wrap(~ ovb_intensity_name) +
  labs(
    x = "IV strength", 
    y = expression(paste("Median  ", frac("Estimate", "True Effect"))),
    color = "Model",
    title = "Evolution of bias with intensity of the IV",
    subtitle = "For statistically significant estimates"
  ) 
```


### F-statistic analysis


```{r fstat_graph, echo=FALSE, fig.asp=0.7}
simulations_IV %>% 
  mutate(significant = (p_value <= 0.05)) %>% 
  filter(iv_strength > 0.06) %>% 
  mutate(bias = estimate - true_effect) %>% 
  # filter(abs(bias) < 10) %>% 
  ggplot(aes(x = fstat, y = bias, color = significant)) +
  geom_point()
```

```{r}
simulations_IV %>% 
  mutate(significant = (p_value <= 0.05)) %>% 
  ggplot(aes(x = iv_strength, y = fstat, color = significant)) +
  geom_point() +
  geom_jitter() +
  ylim(c(0, 40))

lm(data = simulations_IV, fstat ~ iv_strength) %>% 
  summary() %>% 
  .$adj.r.squared

library(ggridges)

simulations_IV %>% 
  mutate(significant = (p_value <= 0.05)) %>% 
  filter(model == "IV") %>% 
  ggplot() +
  geom_density_ridges(aes(x = fstat, y = factor(iv_strength), fill = significant, color = significant), alpha = 0.6)+
  coord_flip()+
  xlim(c(0, 50))
```




```{r}
simulations_IV %>% 
  filter(model == "IV") %>%
  mutate(
    significant = (p_value <= 0.05),
    bin_fstat = cut_number(fstat, n = 10) %>% 
      paste() %>% 
      str_extract("(?<=,)(\\d|\\.)+") %>% 
      as.numeric()
  ) %>% 
  group_by(ovb_intensity, bin_fstat) %>%
  summarise(
    power = mean(significant, na.rm = TRUE)*100, 
    type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),
    bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),
    bias_all = mean(estimate/true_effect, na.rm = TRUE),
    bias_all_median = median(estimate/true_effect, na.rm = TRUE),
    median_fstat = mean(fstat, na.rm = TRUE),
    .groups	= "drop"
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x = bin_fstat, y = bias_signif)) +
  geom_line() +
  geom_vline(xintercept = 10) +
  xlim(c(0, 100))
  
  
  
```










