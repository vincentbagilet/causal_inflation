---
title: "Simulations DID / Event study"
description: "In this document, we run a simulation exercise to illustrate the loss in power and resulting type M error when the number of events in a Difference In Differences design decreases."
author:
  - name: Vincent Bagilet 
    url: https://vincentbagilet.github.io/
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: LÃ©o Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup_RDD, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 200,
               fig.align = "center",
               dev.args = list(bg="transparent"))  
```  

```{r packages_RDD, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse) 
library(knitr) 
library(fixest)
library(mediocrethemes)
library(broom)
library(tictoc)
library(here)
library(beepr)

set_mediocre_all(pal = "coty", gradient = "left")
```

## Summary and intuition

In the case of Event studies and DiD, the OVB/type M trade-off is  mediated by the number of events.

## An illustrative example

<!-- For readability and to illustrate this loss in power, we consider an example setting. For this illustration we could use a large variety of Data Genereting Processes (DGP), both in terms of distribution of the variables and of relations between them. We narrow this down to an example setting, considering an analysis of health impacts of air pollution. Our point should stand in more general settings.  -->

<!-- A threat of confounders often arises when analyzing the health effects of air pollution. To estimate such an effect causally, one can consider exogeneous shocks to air pollution. In the present analysis, we consider the example of plant closures as exogeneous shocks. -->

### Modeling choices

To simplify, I consider the assumptions described below. Of course these assumptions are purely arbitrary and I invite you to play with them. Note that, fixed effects and the covariate are not necessary to the analysis. I only add them to make the analysis more realistic if necessary but I set their baseline values to 0.

- Each individual has fixed characteristics drawn from a normal distribution,
- Each time period presents fixed characteristics also drawn from a normal distribution,
- A unique covariate $x$ drawn from a normal distribution,
- A proportion $p_{treat}$ of individuals are ever treated over the period. Hence, a proportion of $1-p_{treat}$ individuals are never treated over the period. I draw these individual at random. Note that the value of the individual identifiers do not matter here. So I could assume that the non-treated individuals are those with the larger individual ids for instance,
- The implementation of the treatment can be staggered or not. If it is not staggered, the treatment date is set to be in the middle of the period
- The treatment can vary along two dimensions, time and individual. Details are given below.

More precisely, I set: 

- $N_i$ the number of individual
- $N_t$ the number of periods
- $\lambda_i \sim \mathcal{N}(\mu_{IFE}, \sigma_{IFE}^{2})$ the fixed effect for individual $i$
- $\eta_t \sim \mathcal{N}(\mu_{TFE}, \sigma_{TFE}^{2})$ the fixed effect for time period $t$
- $x_{it} \sim \mathcal{N}(\mu_{x}, \sigma_{x}^{2})$
- $e_{it} \sim \mathcal{N}(0, \sigma_{e}^{2})$ some noise
- $T_{it}$ represent the treatment allocation, it is equal to one if individual $i$ is treated at time $t$ and 0 otherwise,
- $y_{it} = \alpha + \beta_{it} T_{it} + \gamma x_{it} + \lambda_i + \eta_t + e_{it}$ where $\alpha$ and $\gamma$ are some constants.
- $\beta_{it}$ is represents the magnitude of the treatment effect and is linked to the input parameter `beta`. 

  - Across individuals, the treatment can either be:
  
    - homogeneous: `het_indiv == homogeneous`, for each individual, the treatment is equal to `beta`, 
    - random: `het_indiv == random`, for each individual, the treatment is drawn from  $\mathcal{U}(0.5\beta, 1.5\beta)$,
    - larger for those that are treated first: `het_indiv == large_first`, for each individual, the treatment is equal to $N_t - \beta$.
  
  - Across time, the effect of the treatment can either be 
    
    - constant: `het_time == constant`,
    - increasing linearly in time: `het_time == linear`.

I also create a bunch of variables that can be useful:

- $InTreatment_i$ equal to 1 if individual $i$ ever gets treated,
- $t^{event}_i$ equal to the date at which individual $i$ gets treated,
- $t^{centered}_i$ representing the distance in terms of period to the beginning of the treatment for individual $i$,
- $Post_{it}$ equal to 1 if the period $t$ is after the treatment has begun for individual $i$. This variable is only useful for non-staggered treatment allocation,

### Data generation

I write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis. 

```{r DGP}
generate_data_DID <- function(N_i,
                              N_t,
                              sigma_e,
                              p_treat,
                              staggered,
                              het_indiv,
                              het_time,
                              alpha,
                              beta,
                              mu_indiv_fe = 0, 
                              sigma_indiv_fe = 0,
                              mu_time_fe = 0, 
                              sigma_time_fe = 0,
                              mu_x = 0, 
                              sigma_x = 0,
                              gamma = 0
                             ) {

  if (!is.logical(staggered)) {stop("staggered must be logical")} 
  if (!(het_indiv %in% c("large_first", "random", "homogeneous"))) {
    stop('het_indiv must be either "large_first", "random" or "homogeneous"')
  } 
  if (!(het_time %in% c("constant", "linear"))) {
    stop('het_time must be either "constant" or "linear"')
  } 
  
  data <- tibble(indiv = 1:N_i) %>%
    mutate(in_treatment = (indiv %in% sample(1:N_i, floor(N_i*p_treat)))) %>% 
    crossing(t = 1:N_t) %>%
    group_by(indiv) %>%
    mutate(
      indiv_fe = rnorm(1, mu_indiv_fe, sigma_indiv_fe),
      t_event = ifelse(staggered, sample(2:(N_t - 1), 1), floor(N_t/2)), 
        #I use 2:(N_t-1) to have a pre and post period
      t_event = ifelse(in_treatment, t_event, NA),
      beta_i = case_when(
        het_indiv == "large_first" ~ N_t-t_event,
        het_indiv == "random" ~ runif(1, beta*0.5, beta*1.5), 
        het_indiv == "homogeneous" ~ beta
      ),
      beta_i = ifelse(is.na(t_event), 0, beta_i)
    ) %>%
    ungroup() %>%
    group_by(t) %>%
    mutate(time_fe = rnorm(1, mu_time_fe, sigma_time_fe)) %>%
    ungroup() %>%
    mutate(
      post = (t > t_event),
      treated = in_treatment & post, 
      beta_i = ifelse(
        het_time == "linear" & post & !is.na(t_event),
        beta_i*(t - t_event), 
        beta_i
      ),
      t_centered = t - t_event,
      x = rnorm(nrow(.), mu_x, sigma_x),
      e = rnorm(nrow(.), 0, sigma_e),
      y0 = alpha + gamma * x + indiv_fe + time_fe + e,
      y1 = y0 + beta_i,
      y = treated*y1 + (1 - treated)*y0
    )
  
  return(data)
}
```

I set baseline values for the parameters as very standard. These values are arbitrary.

```{r baseline_param}
baseline_param_DID <- tibble(
  N_i = 20,
  N_t = 50,
  sigma_e = 1,
  p_treat = 0.5,
  staggered = FALSE,
  het_indiv = "homogeneous",
  het_time = "constant",
  alpha = 1,
  beta = 1
)
```

Here is an example of data created with the data generating process and baseline parameter values, for 2 individuals and 8 time periods:

```{r example_data, echo=FALSE}
baseline_param_DID %>% 
  mutate(N_i = 2, N_t = 8) %>%
  pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters
  # select(indiv, t, y, in_treatment, post, treated, t_centered, e) %>% 
  kable()
```

Let's now have a look at different types of treatment and treatment allocations. First, let's look at treatment allocation mechanisms. The allocation can either be staggered or not, the treatment homogeneous across individual or not and cconstant or not in time.

```{r treatment_allocation_DID, echo=FALSE}
labs_graph_staggered <- labs(
    title = "Treatment assignment across time and individuals",
    x = "Time index", 
    y = "Individual id", 
    fill = "Treated"
  )

baseline_param_DID %>% 
  mutate(staggered = FALSE) %>%
  pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters
  ggplot(aes(x = t, y = factor(indiv), fill = factor(treated))) + 
  geom_tile(color = "white", lwd = 0.3, linetype = 1) +
  coord_fixed() +
  labs_graph_staggered + 
  labs(subtitle = "Non staggered")

# baseline_param_DID %>% 
#   mutate(staggered = TRUE) %>%
#   pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters
#   ggplot(aes(x = t, y = factor(indiv), fill = factor(treated))) + 
#   geom_tile(color = "white", lwd = 0.3, linetype = 1) +
#   coord_fixed() +
#   labs_graph_staggered + 
#   labs(subtitle = "Staggered")
```

<!-- Now, let's vary treatment effect size across individuals, considering a staggered adoption. -->

<!-- ```{r treatment_effect_DID, echo=FALSE} -->
<!-- labs_graph_size <- labs( -->
<!--     title = "Treatment effect size across time and individuals", -->
<!--     x = "Time index",  -->
<!--     y = "Individual id",  -->
<!--     fill = "Treatment effect size" -->
<!--   ) -->

<!-- baseline_param_DID %>%  -->
<!--   mutate(het_indiv = "homogeneous", het_time = "constant") %>% -->
<!--   pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters -->
<!--   ggplot(aes(x = t, y = factor(indiv), fill = round(treated*beta_i, 2))) +  -->
<!--   geom_tile(color = "white", lwd = 0.3, linetype = 1) + -->
<!--   coord_fixed() + -->
<!--   labs_graph_size +  -->
<!--   labs(subtitle = "Homogeneous treatment effect across individuals, constant in time") -->

<!-- baseline_param_DID %>%  -->
<!--   mutate(het_indiv = "random", het_time = "constant") %>% -->
<!--   pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters -->
<!--   ggplot(aes(x = t, y = factor(indiv), fill = round(treated*beta_i, 2))) +  -->
<!--   geom_tile(color = "white", lwd = 0.3, linetype = 1) + -->
<!--   coord_fixed() + -->
<!--   labs_graph_size +  -->
<!--   labs(subtitle = "Random treatment effect size across individuals, constant in time") -->

<!-- baseline_param_DID %>%  -->
<!--   mutate(het_indiv = "large_first", het_time = "constant") %>% -->
<!--   pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters -->
<!--   ggplot(aes(x = t, y = factor(indiv), fill = round(treated*beta_i, 2))) +  -->
<!--   geom_tile(color = "white", lwd = 0.3, linetype = 1) + -->
<!--   coord_fixed() + -->
<!--   labs_graph_size +  -->
<!--   labs(subtitle = "First treated have larger treatment effect, constant in time") -->
<!-- ``` -->

<!-- A last thing we can vary is that we can make individual effects increase linearly in time. -->

<!-- ```{r treatment_time_DID, echo=FALSE} -->
<!-- baseline_param_DID %>%  -->
<!--   mutate(het_indiv = "homogeneous", het_time = "linear") %>% -->
<!--   pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters -->
<!--   ggplot(aes(x = t, y = factor(indiv), fill = round(treated*beta_i, 2))) + #treated*beta_i -->
<!--   geom_tile(color = "white", lwd = 0.3, linetype = 1) + -->
<!--   coord_fixed() + -->
<!--   labs_graph_size + -->
<!--   labs(subtitle = "Treatment effect increasing linearly in time") -->
<!-- ``` -->

<!-- One can now play with this function to generate their own data and run their own analyses. In the following sections, I try to run my own analyses. -->

### Estimation

After generating the data, we can run an estimation.

```{r estimate_DID}
estimate_DID <- function(data) {
  reg <- data %>% 
    mutate(
      indiv = as.factor(indiv),
      t = as.factor(t),
      treated = as.numeric(treated),
      in_treatment = as.numeric(in_treatment),
      t_centered = as.factor(t_centered)
    ) %>% 
    feols(
      data = ., 
      fml = y ~ treated | indiv + t
    ) %>% 
    broom::tidy() %>% 
    rename(p_value = p.value, se = std.error) %>% 
    select(-statistic) 
    # suppressMessages() #Warning saying that NA values dropped and 
    # #that one or two factors are removed due to colinearity
  
  return(reg)
}
```

```{r one_estimation_DID}
baseline_param_DID %>% 
  pmap_dfr(generate_data_DID) %>%
  estimate_DID() %>% 
  slice(1:15) %>% 
  kable()
```

### One simulation

We can now run a simulation, combining `generate_data_DID` and `estimate_DID`. To do so we create the function `compute_sim_DID`. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect, the IV strength and the intensity of the OVB considered (ovb_intensity). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.

```{r compute_true_effect_DID}
compute_true_effect_DID <- function(data) {
  data %>% 
    filter(treated) %>% 
    summarise(true_effect = mean(y1 - y0)) %>% 
    .$true_effect
}  
```

```{r compute_sim_DID}
compute_sim_DID <- function(N_i,
                                    N_t,
                                    sigma_e,
                                    p_treat,
                                    staggered,
                                    het_indiv,
                                    het_time,
                                    alpha,
                                    beta,
                                    mu_indiv_fe = 0,
                                    sigma_indiv_fe = 0,
                                    mu_time_fe = 0,
                                    sigma_time_fe = 0,
                                    mu_x = 0,
                                    sigma_x = 0,
                                    gamma = 0) {
  data <- generate_data_DID(
    N_i = N_i,
    N_t = N_t,
    sigma_e = sigma_e,
    p_treat = p_treat,
    staggered = staggered,
    het_indiv = het_indiv,
    het_time = het_time,
    alpha = alpha,
    beta = beta,
    mu_indiv_fe = mu_indiv_fe,
    sigma_indiv_fe = sigma_indiv_fe,
    mu_time_fe = mu_time_fe,
    sigma_time_fe = sigma_time_fe,
    mu_x = mu_x,
    sigma_x = sigma_x,
    gamma = gamma
  ) 
  
  data %>%
    estimate_DID() %>% 
    mutate(
      N_i = N_i,
      N_t = N_t,
      p_treat = p_treat,
      true_effect = compute_true_effect_DID(data)
    )
} 
```

Here is an example of an output of this function.

```{r example_output_sim_RDD, echo=FALSE}
baseline_param_DID %>%
  pmap_dfr(compute_sim_DID) %>% 
  kable()
```

### All simulations

We will run the simulations for different sets of parameters by mapping our `compute_sim_DID` function on each set of parameters. We thus create a table with all the values of the parameters we want to test, `param_DID`. Note that in this table each set of parameters appears `n_iter` times as we want to run the analysis $n_{iter}$ times for each set of parameters.

```{r set_param_DID, echo=FALSE}
fixed_param <- baseline_param_DID #%>% rbind(...)
vect_N_i <- c(4, 25, 50)
vect_N_t <- c(4, 25, 50)
# vect_N_i <- seq(2, 50, 8)
# vect_N_t <- seq(2, 50, 8)
vect_p_treat <- c(0.5)
n_iter <- 1000

param_DID <- fixed_param %>% 
  select(-N_i, -N_t, -p_treat) %>% #just quick fix, need to change this
  crossing(vect_N_i, vect_N_t, vect_p_treat) %>% 
  rename(N_i = vect_N_i, N_t = vect_N_t, p_treat = vect_p_treat) %>% 
  crossing(rep_id = 1:n_iter) %>% 
  select(-rep_id)
```

We then run the simulations by mapping our `compute_sim_IV` function on `param_IV`.

```{r run_sim_DID, eval=FALSE, echo=FALSE}
tic()
sim_DID <- pmap_dfr(param_DID, compute_sim_DID)
beep()
toc()

# saveRDS(sim_DID, here("Outputs/sim_DID.RDS"))
```

## Analysis of the results

### Quick exploration

First, we quickly explore the results.

```{r exploration_results_DID, echo=FALSE, fig.asp=0.7}
sim_DID <- readRDS(here("Outputs/sim_DID.RDS"))

sim_DID %>% 
  mutate(
    N_i = paste("Number individuals: ", str_pad(N_i, 2)),
    N_t = paste("Number periods: ", str_pad(N_t, 2))
  ) %>% 
  ggplot(aes(x = estimate)) +
  geom_vline(xintercept = 1) +
  geom_density() +
  facet_grid(fct_rev(N_t) ~ N_i) +
  labs(
    title = "Distribution of the estimates of the treatment effect",
    subtitle = "For different number of individuals and time period",
    x = "Estimate of the treatment effect",
    y = "Density",
    caption = "The vertical line represents the true effect"
  )
```

### Computing bias and type M

We want to compare $\mathbb{E}[\beta_0 - \widehat{\beta_{i}}]$ and $\mathbb{E}[|\beta_0 - \widehat{\beta_{IV}}||signif]$. The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance. 

```{r summarise_IV}
summarise_sim_DID <- function(data) {
  data %>%
    mutate(significant = (p_value <= 0.05)) %>% 
    group_by(p_treat, N_i, N_t) %>%
    summarise(
      power = mean(significant, na.rm = TRUE)*100, 
      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),
      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),
      bias_all = mean(estimate/true_effect, na.rm = TRUE),
      bias_all_median = median(estimate/true_effect, na.rm = TRUE),
      .groups	= "drop"
    ) %>% 
    ungroup()
} 

summary_sim_DID <- summarise_sim_DID(sim_DID)
# saveRDS(summary_sim_DID, here("Outputs/summary_sim_DID.RDS"))
```

### Graph

To analyze our results, we build a unique and simple graph:

```{r graph_results_DID, echo=FALSE, fig.asp=0.7}
summary_sim_DID %>% 
  mutate(
    N_i = as.factor(N_i),
    N_t = as.factor(N_t)
  ) %>% 
  ggplot(aes(x = N_i, y = N_t, fill = type_m)) +
  geom_tile(color = "white", lwd = 1, linetype = 1) +
  coord_fixed() +
  labs(
    # title = "Type M error for different number of periods and observation",
    title = "The 'balls of yarn' graph",
    x = "Number of individuals",
    y = "Number of time periods",
    fill = "Type M error"
  ) 
```

## Varying the proportion of treated



```{r sim_p_treat, echo=FALSE, message=FALSE}
fixed_param <- baseline_param_DID #%>% rbind(...)
vect_N_i <- c(100)
vect_N_t <- c(10)
vect_p_treat <- seq(0.1, 0.5, 0.1)# seq(0.1, 0.9, 0.1)
n_iter <- 200

param_DID_p_treat <- fixed_param %>% 
  mutate(beta = 0.4) %>% 
  select(-N_i, -N_t, -p_treat) %>% #just quick fix, need to change this
  crossing(vect_N_i, vect_N_t, vect_p_treat) %>% 
  rename(N_i = vect_N_i, N_t = vect_N_t, p_treat = vect_p_treat) %>% 
  crossing(rep_id = 1:n_iter) %>% 
  select(-rep_id)

tic()
sim_DID_p_treat <- pmap_dfr(param_DID_p_treat, compute_sim_DID)
beep()
toc()

# saveRDS(sim_DID_p_treat, here("Outputs/sim_DID_p_treat.RDS"))
# sim_DID_p_treat <- readRDS(here("Outputs/sim_DID_p_treat.RDS"))

sim_DID_p_treat %>% 
  filter(p_treat %in% sample(vect_p_treat, 4)) %>% 
  ggplot(aes(x = estimate)) +
  geom_density() +
  facet_wrap(~ p_treat)

sim_DID_p_treat %>% 
  summarise_sim_DID() %>% 
  ggplot(aes(x = p_treat, y = bias_signif)) + 
  # geom_point() +
  geom_line(size = 0.8) +
  labs(
    x = "Proportion of units treated", 
    y = expression(paste("Average  ", frac("Estimate", "True Effect"))),
    title = "Evolution of bias with the proportion of units treated",
    subtitle = "For statistically significant estimates, 100 individuals on 10 periods"
  ) 
```

