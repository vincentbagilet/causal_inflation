---
title: "Simulations DID / Event study"
description: "In this document, we run a simulation exercise to illustrate how using a Difference-in-Differences (DiD) or event study approach to avoid confounders may lead to a loss in power and inflated effect sizes."
author:
  - name: Vincent Bagilet 
    url: https://vincentbagilet.github.io/
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: LÃ©o Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup_RDD, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 300,
               fig.align = "center",
               dev.args = list(bg="transparent"))  
```  

```{r packages_RDD, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse) 
library(knitr) 
library(fixest)
library(mediocrethemes)
library(broom)
library(tictoc)
library(here)
library(beepr)

set_mediocre_all(pal = "coty", gradient = "left")
```

## Summary and intuition

In the case of event studies, DiD or basically any study using discrete axogeneous shocks, the unconfoundedness / exaggeration trade-off is  mediated by the number of observations treated. In some classic DiD settings, while the number of observations is large, the number of events might be limited. Pinpointing exogenous shocks is never easy and we are sometimes only able to find a limited number of occurrences of this shock or shocks affecting a small portion of the population. More precisely, the number of observations with a non null treatment status might be small. In this instance, the variation available to identify the treatment is limited, possibly leading power to be low and exaggeration issues to arise. 

## An illustrative example

For readability and to illustrate this loss in power, we consider an example setting. For this illustration we could use a large variety of Data Genereting Processes (DGP), both in terms of distribution of the variables and of relations between them. We narrow this down to an example setting, considering an analysis of health impacts of air pollution. More precisely, we simulate and analyze the impacts of air pollution reduction on birthweights. Air pollution might vary with other variables that may also affect birthweight (for instance economic activity). Some of these variables might be unobserved and bias estimates of a simple regression of birthweight on pollution levels. A strategy to avoid such issues is to consider exogenous shocks to air pollution such as plant closures, plant openings, creation of a low emission zone or an urban toll, strikes, etc.

Even if we consider an example setting for clarity, the unconfoundedness / exaggeration trade-off mediated by the number of observations treated should also arise in other settings.

### Modeling choices

To simplify, we consider the assumptions described below. Of course these assumptions are arbitrary and we invite you to play with them. 
<!-- Note that, fixed effects and the covariate are not necessary to the analysis. I only add them to make the analysis more realistic if necessary but I set their baseline values to 0. -->

As many studies in the literature, for instance [Currie et al. (2015)](https://www.aeaweb.org/articles?id=10.1257/aer.20121656) and [Lavaine and Neidell (2017)](https://www.journals.uchicago.edu/doi/full/10.1086/691554), to build a panel, we aggregate observations. We look at the evolution of birthweights in zip codes in time, for instance months. 

For clarity in the explanations, let's assume that the exogenous shock considered is a permanent plant closure and that this reduces air pollution level. If the reader prefers, they can think of it as any permanent exogenous change in air pollution levels. For now, we will only estimate a reduced form. We are therefore not interested in modeling the effect of the plant closure on pollution levels. We consider that the plant closure leads to some air pollution reduction and want to estimate the effect of this closure on birthweight. We plan to add pollution in the future in order to compare the performance of the reduced form and a straight regression of birthweight on pollution. Yet, generating pollution is rather complex and we therefore only consider the reduced form for now.

For each time period, a zip code is either treated (plant closed) or not treated. Over the whole period, some zip codes experience a plant closure, others do not either because they do not have a plant that affect their air pollution levels or because their plant did not close. 

We consider that the average birthweight in zip code $z$ at time period $t$, $w_{zt}$, depends on a zip code fixed effect $\zeta_z$, a time fixed effect $\tau_t$ and the treatment status $T_i$, *ie* whether a plant closed in this period or not. For now, we do not simulate omitted variable biases as we consider that the shocks are truly exogenous. Thus, birthweight is defined as follows:

$$w_{z,t} = \alpha + \beta T_{z, t} + \zeta_z + \tau_t + \epsilon_{z,t}$$
<!-- 
is correlated with the average birthweight in this zip code in other periods and in nearby zip codes and also depends the treatment status $T_i$, *ie* whether a plant closed in this period or not. For now, we do not simulate omitted variable biases as we consider that the shocks are truly exogenous.

We consider an AR(1) process along each dimension. Thus, birthweight is defined as follows: -->

<!-- $$Birthweight_{z,t} = \alpha + \beta T_{z, t} + \tau Birthweight_{z, (t-1)} + \zeta Birthweight_{(z-1), t} + \epsilon_{z,t}$$ -->

<!-- The DGP can be represented using the following Directed Acyclic Graph (DAG):  -->

<!-- ```{r DAG_RDD, echo=FALSE, out.width='60%'} -->
<!-- include_graphics(here("images/DAGs/DAGs.004.png")) -->
<!-- ``` -->


To simplify, we consider the following additional assumptions:

- The treatment is constant in time and homogeneous across zip codes
- A proportion $p_{treat}$ of zip codes are ever treated over the period. Hence, a proportion of $1-p_{treat}$ zip codes are never treated over the period. we draw these zip codes at random. Note that the value of the zip code identifiers do not matter here. So we could assume that the non-treated zip codes are those with the larger zip codes identifiers for instance,
- The implementation of the treatment can be staggered or not. If it is not staggered, the treatment date is set to be in the middle of the period. For now we only analyse the non staggered version.

More precisely, we set: 

- $N_z$ the number of zip codes,
- $N_t$ the number of periods (months),
- $\zeta_z \sim \mathcal{N}(\mu_{\zeta}, \sigma_{\zeta}^{2})$ the fixed effect for zip code $z$,
- $\tau_t \sim \mathcal{N}(\mu_{\tau}, \sigma_{\tau}^{2})$ the fixed effect for time period $t$,
- $\epsilon_{zt} \sim \mathcal{N}(0, \sigma_{\epsilon}^{2})$ some noise,
- $T_{zt}$ represent the treatment allocation, it is equal to 1 if zip code $z$ is treated at time $t$ and 0 otherwise,
- $w_{z,t} = \alpha + \beta T_{z, t} + \zeta_z + \tau_t + \epsilon_{z,t}$ where $\alpha$ is a constant,
- $\beta$ is represents the magnitude of the treatment,
- We define `staggered` as a logical variable.

We also create a bunch of variables that can be useful:

- $InTreatment_z$ equal to 1 if zip code $z$ ever gets treated,
- $t^{event}_z$ equal to the date at which zip code $z$ gets treated, especially usefull in the staggered setting,
- $t^{centered}_z$ representing the distance in terms of periods to the beginning of the treatment for zip code $z$,
- $Post_{zt}$ equal to 1 if the period $t$ is after the treatment has begun for zip code $z$. This variable is only useful for non-staggered treatment allocation cas.

### Data generation

We write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis. 

```{r DGP_DID}
generate_data_DID <- function(N_z,
                              N_t,
                              sigma_e,
                              p_treat,
                              staggered,
                              alpha,
                              beta,
                              mu_zip_fe = 0, 
                              sigma_zip_fe = 0,
                              mu_time_fe = 0, 
                              sigma_time_fe = 0
                             ) {
  
  if (!is.logical(staggered)) {stop("staggered must be logical")} 
  
  data <- tibble(zip = 1:N_z) %>%
    mutate(in_treatment = (zip %in% sample(1:N_z, floor(N_z*p_treat)))) %>% 
    crossing(t = 1:N_t) %>%
    group_by(zip) %>%
    mutate(
      zip_fe = rnorm(1, mu_zip_fe, sigma_zip_fe),
      t_event = ifelse(staggered, sample(2:(N_t - 1), 1), floor(N_t/2)), 
        #We use 2:(N_t-1) to aalways have a pre and post period
      t_event = ifelse(in_treatment, t_event, NA),
      beta = ifelse(is.na(t_event), 0, beta)
    ) %>%
    ungroup() %>%
    group_by(t) %>%
    mutate(time_fe = rnorm(1, mu_time_fe, sigma_time_fe)) %>%
    ungroup() %>%
    mutate(
      post = (t > t_event),
      treated = in_treatment & post, 
      t_centered = t - t_event,
      e = rnorm(nrow(.), 0, sigma_e),
      birthweight0 = alpha + zip_fe + time_fe + e,
      birthweight1 = birthweight0 + beta,
      birthweight = treated*birthweight1 + (1 - treated)*birthweight0
    )
  
  return(data)
}
```

We set baseline values for the parameters as very standard. These values are arbitrary.

```{r baseline_param_DID, echo=FALSE}
baseline_param_DID <- tibble(
  N_z = 20,
  N_t = 50,
  sigma_e = 1,
  p_treat = 0.5,
  staggered = FALSE,
  alpha = 1,
  beta = 1,
  mu_zip_fe = 1, 
  sigma_zip_fe = 1,
  mu_time_fe = 4, 
  sigma_time_fe = 4
)

baseline_param_RDD %>% kable()
```

Here is an example of data created with the data generating process and baseline parameter values, for 2 zip codes and 8 time periods:

```{r example_data_DID, echo=FALSE}
baseline_param_DID %>% 
  mutate(N_z = 2, N_t = 8) %>%
  pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters
  kable()
```

Treatment allocations, in the staggered and non staggered case are as follows:

```{r treatment_allocation_DID, echo=FALSE}
labs_graph_staggered <- labs(
    title = "Treatment assignment across time and zip codes",
    x = "Time index", 
    y = "Zip code id", 
    fill = "Treated"
  )

baseline_param_DID %>% 
  pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters
  ggplot(aes(x = t, y = factor(zip), fill = factor(treated))) + 
  geom_tile(color = "white", lwd = 0.3, linetype = 1) +
  coord_fixed() +
  labs_graph_staggered + 
  labs(subtitle = "Non staggered")

baseline_param_DID %>%
  mutate(staggered = TRUE) %>%
  pmap_dfr(generate_data_DID) %>% #use pmap to pass the set of parameters
  ggplot(aes(x = t, y = factor(zip), fill = factor(treated))) +
  geom_tile(color = "white", lwd = 0.3, linetype = 1) +
  coord_fixed() +
  labs_graph_staggered +
  labs(subtitle = "Staggered")
```

### Estimation

After generating the data, we can run an estimation.

```{r estimate_DID}
estimate_DID <- function(data) {
  reg <- data %>% 
    mutate(
      zip = as.factor(zip),
      t = as.factor(t),
      treated = as.numeric(treated),
      in_treatment = as.numeric(in_treatment),
      t_centered = as.factor(t_centered)
    ) %>% 
    feols(
      data = ., 
      fml = birthweight ~ treated | zip + t
    ) %>% 
    broom::tidy() %>% 
    rename(p_value = p.value, se = std.error) %>% 
    select(-statistic) 
    # suppressMessages() #Warning saying that NA values dropped and 
    # #that one or two factors are removed due to colinearity
  
  return(reg)
}
```

```{r one_estimation_DID}
baseline_param_DID %>% 
  pmap_dfr(generate_data_DID) %>%
  estimate_DID() %>% 
  slice(1:15) %>% 
  kable()
```

### One simulation

Note that to run power calculations, we need to have access to the true effects. Therefore, before running the estimation, we write a short function to compute the average treatment effect on the treated (ATET). We will add this information to the estimation results. 

```{r compute_true_effect_DID}
compute_true_effect_DID <- function(data) {
  data %>% 
    filter(treated) %>% 
    summarise(true_effect = mean(birthweight1 - birthweight0)) %>% 
    .$true_effect
}  
```

We can now run a simulation, combining `generate_data_DID` and `estimate_DID`. To do so we create the function `compute_sim_DID`. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect, the proportion of treated units and whether the treatment was staggered or not. Note that for now, we do not store the values of the other parameters for simplicity because we consider them fixed over the study.

```{r compute_sim_DID}
compute_sim_DID <- function(N_z,
                                    N_t,
                                    sigma_e,
                                    p_treat,
                                    staggered,
                                    alpha,
                                    beta,
                                    mu_zip_fe = 0,
                                    sigma_zip_fe = 0,
                                    mu_time_fe = 0,
                                    sigma_time_fe = 0) {
  data <- generate_data_DID(
    N_z = N_z,
    N_t = N_t,
    sigma_e = sigma_e,
    p_treat = p_treat,
    staggered = staggered,
    alpha = alpha,
    beta = beta,
    mu_zip_fe = mu_zip_fe,
    sigma_zip_fe = sigma_zip_fe,
    mu_time_fe = mu_time_fe,
    sigma_time_fe = sigma_time_fe
  ) 
  
  data %>%
    estimate_DID() %>% 
    mutate(
      N_z = N_z,
      N_t = N_t,
      p_treat = p_treat,
      true_effect = compute_true_effect_DID(data)
    )
} 
```

Here is an example of an output of this function.

```{r example_output_sim_RDD, echo=FALSE}
baseline_param_DID %>%
  pmap_dfr(compute_sim_DID) %>% 
  kable()
```

### All simulations

We will run the simulations for different sets of parameters by mapping our `compute_sim_DID` function on each set of parameters. We thus create a table with all the values of the parameters we want to test, `param_DID`. Note that in this table each set of parameters appears `N_iter` times as we want to run the analysis $n_{iter}$ times for each set of parameters.

```{r set_param_DID, echo=FALSE}
vect_N_z <- c(4, 25, 50)
vect_N_t <- c(4, 25, 50)
vect_p_treat <- c(0.5)
N_iter <- 200

param_DID <- baseline_param_DID %>% 
  select(-N_z, -N_t, -p_treat) %>% 
  crossing(vect_N_z, vect_N_t, vect_p_treat) %>% 
  rename(N_z = vect_N_z, N_t = vect_N_t, p_treat = vect_p_treat) %>% 
  crossing(rep_id = 1:N_iter) %>% 
  select(-rep_id)
```

We then run the simulations by mapping our `compute_sim_IV` function on `param_IV`.

```{r run_sim_DID, eval=FALSE, echo=FALSE}
tic()
sim_DID <- pmap_dfr(param_DID, compute_sim_DID)
beep()
toc()

# saveRDS(sim_DID, here("Outputs/sim_DID .RDS"))
```

## Analysis of the results

### Quick exploration

First, we quickly explore the results.

```{r exploration_results_DID, echo=FALSE, fig.asp=0.7}
sim_DID <- readRDS(here("Outputs/sim_DID.RDS"))

sim_DID %>% 
  mutate(
    N_z = paste("Number zip codes: ", str_pad(N_z, 2)),
    N_t = paste("Number periods: ", str_pad(N_t, 2))
  ) %>% 
  ggplot(aes(x = estimate)) +
  geom_vline(xintercept = 1) +
  geom_density() +
  facet_grid(fct_rev(N_t) ~ N_z) +
  labs(
    title = "Distribution of the estimates of the treatment effect",
    subtitle = "For different number of zip codes and time period",
    x = "Estimate of the treatment effect",
    y = "Density",
    caption = "The vertical line represents the true effect"
  )
```

### Computing bias and exaaggeration ratio

We want to compare $\mathbb{E}[\beta_0/\widehat{\beta}]$ and $\mathbb{E}[\beta_0/ \widehat{\beta}|signif]$. The first term represents the bias and the second term represents the 
exaggeration ratio. These terms depend on the true effect size.

```{r summarise_IV}
summarise_sim_DID <- function(data) {
  data %>%
    mutate(significant = (p_value <= 0.05)) %>% 
    group_by(p_treat, N_z, N_t) %>%
    summarise(
      power = mean(significant, na.rm = TRUE)*100, 
      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),
      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),
      bias_all = mean(estimate/true_effect, na.rm = TRUE),
      bias_all_median = median(estimate/true_effect, na.rm = TRUE),
      .groups	= "drop"
    ) %>% 
    ungroup()
} 

summary_sim_DID <- summarise_sim_DID(sim_DID)
# saveRDS(summary_sim_DID, here("Outputs/summary_sim_DID.RDS"))
```

### Graph

To analyze our results, we build a unique and simple graph:

```{r graph_results_DID, echo=FALSE, fig.asp=0.7}
summary_sim_DID %>% 
  mutate(
    N_z = as.factor(N_z),
    N_t = as.factor(N_t)
  ) %>% 
  ggplot(aes(x = N_z, y = N_t, fill = type_m)) +
  geom_tile(color = "white", lwd = 1, linetype = 1) +
  coord_fixed() +
  labs(
    # title = "Type M error for different number of periods and observation",
    title = "The 'balls of yarn' graph",
    x = "Number of zip codes",
    y = "Number of time periods",
    fill = "Exaggeration ratio"
  ) 
```

## Varying the proportion of treated



```{r sim_p_treat, echo=FALSE, message=FALSE}
vect_N_z <- c(100)
vect_N_t <- c(10)
vect_p_treat <- seq(0.1, 0.5, 0.1)# seq(0.1, 0.9, 0.1)
N_iter <- 200

param_DID_p_treat <- baseline_param_DID %>% 
  mutate(beta = 0.4) %>% 
  select(-N_z, -N_t, -p_treat) %>% #just quick fix, need to change this
  crossing(vect_N_z, vect_N_t, vect_p_treat) %>% 
  rename(N_z = vect_N_z, N_t = vect_N_t, p_treat = vect_p_treat) %>% 
  crossing(rep_id = 1:N_iter) %>% 
  select(-rep_id)

# tic()
# sim_DID_p_treat <- pmap_dfr(param_DID_p_treat, compute_sim_DID)
# beep()
# toc()

# saveRDS(sim_DID_p_treat, here("Outputs/sim_DID_p_treat.RDS"))
sim_DID_p_treat <- readRDS(here("Outputs/sim_DID_p_treat.RDS"))

sim_DID_p_treat %>% 
  filter(p_treat %in% sample(vect_p_treat, 4)) %>% 
  ggplot(aes(x = estimate)) +
  geom_density() +
  facet_wrap(~ p_treat)

sim_DID_p_treat %>% 
  summarise_sim_DID() %>% 
  mutate(n_treated = p_treat*N_z*N_t) %>% 
  ggplot(aes(x = n_treated, y = bias_signif)) + 
  # geom_point() +
  geom_line(size = 0.8) +
  labs(
    x = "Number of treated observations", 
    y = expression(paste("Average  ", frac("Estimate", "True Effect"))),
    title = "Evolution of bias with the number of treated observations",
    subtitle = "For statistically significant estimates, 100 zip codes on 10 periods"
  ) 
```

