---
title: "Simulations RDD"
description: "In this document, we run a simulation exercise to illustrate how using a Regression Discontinuity Design (RDD) to avoid confounders may lead to a loss in power and inflated effect sizes."
author:
  - name: Vincent Bagilet 
    url: https://vincentbagilet.github.io/
    affiliation: Columbia University
    affiliation_url: https://www.columbia.edu/
  - name: LÃ©o Zabrocki 
    url: https://www.parisschoolofeconomics.eu/en/
    affiliation: Paris School of Economics
    affiliation_url: https://www.parisschoolofeconomics.eu/en/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup_RDD, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 300,
               fig.align = "center",
               dev.args = list(bg="transparent"))  
```  

```{r packages_RDD, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse) 
library(knitr) 
library(mediocrethemes)
library(broom)
library(tictoc)
library(here)
library(beepr)

set_mediocre_all(pal = "coty")
```

## Summary and intuition

In the case of the RDD, the trade-offbetween avoiding confounding and inflated effect sizes due to low power issues is mediated by the size of the bandwidth considered in the analysis. The underlying idea is that the smaller the bandwidth, the more comparable units are and therefore the smaller the risk of confounding is. Yet, with a smaller bandwidth, sample size and thus power decrease, increasing the exaggeration ratio.

## An illustrative example

To illustrate this trade-off, we consider a standard application of the RD design in economics of education where a grant or additional lessons are assigned based on the score obtained by students on a standardized test. Students with test scores below a given threshold receive the treatment while those above do not. Yet, students far above and far below the threshold may differ along unobserved characteristics such as ability. To limit this bias, the effect of the treatment is estimated by comparing the outcomes of students just below and just above this threshold. This enable to limit disparities in terms of unobserved characteristics.

[Thistlewaite and Campbell (1960)](https://psycnet.apa.org/record/1962-00061-001) introduced the concept of RDD using this type of quasi-experiment. In their paper, they take advantage of a sharp discontinuity in the assignment of an award (a Certificate of Merit) based on qualifying scores at a test. This type of analysis is still used today and many papers leveraging similar methodologies have been published since this seminal work. For instance, [Jacob and Lefgren (2004)](https://direct.mit.edu/rest/article/86/1/226/57489/Remedial-Education-and-Student-Achievement-A) exploit this type of discontinuity to study the impact of  summer school and grade retention programs on test scores. Students who score below a given score are required to attend a summer school and to retake the test. Students who do not pass the second have to repeat the grade. 

### Modeling choices

In the present analysis, we build our simulations to replicate a similar type of quasi-experiment. In our fictional example, all students scoring below a cutoff $C$ in a qualification test are required to take additional lessons. We want to estimate the effect of these additional lessons on scores on a final test taken by all students a year later. 

We assume that the final score of student $i$, $Final_i$, is correlated with their qualification score $Qual_i$ and their treatment status $T_i$, *ie* whether student $i$ received additional lessons or not. We further assume that both qualification and final test scores are affected by students' unobserved ability $U_i$ in a non linear way. 

The DGP can be represented using the following Directed Acyclic Graph (DAG): 

```{r DAG_RDD, echo=FALSE, fig.asp=0.35, out.width='70%'}
library(ggdag)

dagify(Final ~ U + T + Qual,
       T ~ Qual,
       Qual ~ U, 
       outcome = "Final",
       latent = "U",
       coords = list(x = c(Final = 3, T = 2, U = 2, Qual = 1),
                     y = c(Final = 0, T = 0, U = 1, Qual = 1))) %>% 
  # ggdag() +
  ggplot(ggplot2::aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(alpha = 0, stroke = TRUE) +
  geom_dag_edges(edge_colour = mediocrethemes::colors_table$base[6]) +
  geom_dag_text(
    
    colour = mediocrethemes::colors_table$base[6], 
    size = 6,
    family = "Lora"
  ) +
  theme_void()
```

Final test scores are thus defined as follows:

$$Final_{i} = \alpha + \beta T_i + \gamma Qual_{i} +  \delta f(U_i) + \epsilon_{i}$$
Where $\alpha$ is a constant, $f$ a non linear function and $e \sim \mathcal{N}(0, \sigma_{e})$ noise. The parameter of interest is $\beta$. Translating this into a potential outcomes framework, we have $Final_i(0) = \alpha + \gamma Qual_{i} + \delta f(U_i) + \epsilon_{i}$ and $Final_i(1) = \alpha + \gamma Qual_{i} + \beta + \delta f(U_i) + \epsilon_{i}$

<!-- on one of the analyses Jacob and Lefgren (2008): estimating the impact of the retention and summer school programs on third graders' maths scores, one year later. More precisely, we consider the same type of treatment allocation mechanism and outcome variable but greatly simplify their setting. The distribution of our variables is chosen to emulate the two first moments of the distributions they observe. For simplicity, we however assume that the variables are normally distributed.  -->

To simplify, we consider the following assumptions:

- Full compliance and a sharp treatment allocation such that $T_i = \mathbb{I}[Qual_{i} < C]$. All students with a qualification score below the threshold are treated and receive additional lessons. None of the students with a qualification score above the threshold are treated. 
- The unobserved ability affects qualification and final test scores in a cubic way. A large ability has a strong positive impact on test scores. Similarly a particularly low ability strongly impacts test scores negatively. An average ability does not have much impact on test scores. Such a functional form seems realistic. Note that ability creates an OVB only if it has a non linear impact on test scores.
- We assume constant treatment effects. This assumption is not necessary and our results hold if we consider non-constant treatment effects. We thus may drop this assumption in the future.
- We assume that the unobserved availability affects the qualification and final score in a similar way and therefore with the same intensity $\delta$.

More precisely, we set: 

- $N$ the number of students
- $U \sim \mathcal{N}(0, \sigma_u^{2})$ the unobserved ability.
- $Qual_i = H_i + \delta U_i^{2}$ where $H \sim \mathcal{N}(\mu_h, \sigma_h^{2})$. We center the qualification scores such that treated units are below 0 and non treated ones above.
- $T_i = \mathbb{I}[Qual_{i} < q_c]$ where for now and for simplicity, $q_c$ is a fixed grade threshold given as the quantile in the qualification score distribution.
- $e \sim \mathcal{N}(0, \sigma_e^2)$
- $Final_{i} = \alpha  + \beta T_i + \gamma Qual_{i} +  \delta U_i^{2} + e_{i}$

### Data generation

We write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis. 

Once the fake data is generated, to make things more realistic we consider our data as if it was actual data. We do not take advantage of our knowledge of the data generating process in the estimation procedure. However, we observe both potential outcomes and the unobserved ability. Note that, in a real world setting, one would generally know the value of the threshold (and thus of $q_c$). Based on that and to simplify the computation of the bandwidth, we store $q_c$.

```{r DGP_RDD}
generate_data_rdd <- function(N, 
                              sigma_u,
                              mu_h, 
                              sigma_h, 
                              sigma_e, 
                              alpha, 
                              beta,
                              gamma,
                              delta,
                              q_c) {
  
  data <- tibble(id = 1:N) %>% 
    mutate(
      # qual = rnorm(nrow(.), mu_h, sigma_h),
      # u = rnorm(nrow(.), 0.5, sigma_u) + qual + 0.3*qual^3,
      u = rnorm(nrow(.), 0, sigma_u),
      qual = rnorm(nrow(.), mu_h, sigma_h) + delta*u^2,
      e = rnorm(nrow(.), 0, sigma_e),
      # qual_c = qual - quantile(qual, q_c),
      # treated = qual_c < 0,
      # threshold = quantile(qual, q_c),
      treated = qual < quantile(qual, q_c),
      final0 = alpha + gamma*qual + delta*u^2 + e,
      final1 = final0 + beta,
      final = final0 + beta*treated,
      q_c = q_c
    )
  
  return(data)
}
```

We set baseline values for the parameters to emulate a somehow realistic observational study in this field. The set of parameters may produce test score outside of the range 0-100 in some iterations but that does not affect the analysis. We add the parameter value for delta separately as we will vary the value later and will reuse the vector `baseline_param_RDD`.

```{r baseline_param_RDD, echo=FALSE}
baseline_param_RDD <- tibble(
  N = 1000,
  sigma_u = 0.5,
  mu_h = 75,
  sigma_h = 7,
  sigma_e = 4,
  alpha = 20,
  beta = 1,
  gamma = 0,
  q_c = 0.5,
  delta = 20
)

baseline_param_RDD %>% kable()
```

Here is an example of data created with our data generating process:

```{r example_data_RDD, echo=FALSE}
baseline_param_RDD %>% 
  mutate(N = 10) %>% 
  pmap_dfr(generate_data_rdd) %>% #use pmap to pass the set of parameters
  kable()
```

### Defining the bandwidth

In a RDD, the model is estimated only for observations close enough to the threshold, *ie* in a given bandwidth. We therefore create a function to define this bandwidth by adding a variable to the data set `treated_bw` that is equal to `NA` if the observations is outside of the bandwidth, `TRUE` if the observation falls in the bandwidth and the student is treated and `FALSE`  if the observation falls in the bandwidth and the student is not treated. The bandwidth parameter `bw` represents the proportion of units that are in the bandwidth. If `bw = 0.1`, 10% of the students are in the bandwidth for instance.

```{r define_bw_RDD}
define_bw <- function (data, bw) {
  data <- data %>% 
    mutate(
      treated_bw = ifelse(
        dplyr::between(
          qual, 
          quantile(qual, unique(q_c) - bw/2), 
          quantile(qual, unique(q_c) + bw/2)
        ), 
        treated, 
        NA
      )
    )
} 
```

The following graph illustrates this process by plotting final test scores against qualification ones depending on the value of `treated_bw`.

```{r graph_bandwidth_RDD, echo=FALSE}
baseline_param_RDD %>% 
  pmap_dfr(generate_data_rdd) %>% #use pmap to pass the set of parameters
  define_bw(0.5) %>%
  # mutate(
  #   treated_bw_phrase = case_when(
  #     treated_bw ~ "Treated and inside bandwidth",
  #     !treated_bw ~ "Non treated and inside bandwidth",
  #     treated_bw ~ "Treated and inside bandwidth",
  #   )
  # )
  ggplot(aes(x = qual, y = final, color = treated_bw)) +
  geom_point() +
  labs(
    title = "Final scores against qualification scores",
    subtitle = "Illustration of the definition of the bandwidth and the treatment",
    x = "Qualification score",
    y = "Final sccore",
    color = ""
  ) +
  scale_color_discrete(
    labels = c(
      "Non treated and inside bandwidth",
      "Treated and inside bandwidth",
      "Outside bandwidth")
  ) +
  coord_fixed()
```

### Estimation

After generating the data, we can run an estimation. 

Note that to run power calculations, we need to have access to the true effects. Therefore, before running the estimation, we write a short function to compute the average treatment effect on the treated (ATET). We will add this information to the estimation results. 

```{r compute_true_effect_RDD}
compute_true_effect_rdd <- function(data) {
  treated_data <- data %>% 
    filter(treated) 
  return(mean(treated_data$final1 - treated_data$final0))
}  
```

We then run the estimation. To do so, we only consider observations within the bandwidth and regress the final test scores on the treatment, the qualification score and their interaction. Note that we include this interaction term to allow more flexibility and to mimic an realistic estimation. Yet, we know that this interaction term does not appear in the DGP. Including it or not do not change the results. Also note that, of course, we do not include the unobserved ability in this model to create an OVB. 

```{r estimate_RDD}
estimate_rdd <- function(data, bw) {
  data_in_bw <- data %>% 
    define_bw(bw = bw) %>% 
    filter(!is.na(treated_bw))
  
  reg <- lm(
    data = data_in_bw, 
    formula = final ~ treated + qual
  ) %>% 
    broom::tidy() %>%
    filter(term == "treatedTRUE") %>%
    rename(p_value = p.value, se = std.error) %>%
    select(estimate, p_value, se) %>%
    mutate(
      true_effect = compute_true_effect_rdd(data),
      bw = bw
    )
  
  return(reg)
}
```

### One simulation

We can now run a simulation, combining `generate_data_rdd` and `estimate_rdd`. To do so we create the function `compute_sim_RDD`. This simple function takes as input the various parameters along with a vector of bandwidth sizes, `vect_bw`. If we want to run several simulations with different bandwidths, we can reuse the same data, hence why we allow to passing a vector of bandwidths and not only one bandwidth. The function returns a table with the estimate of the treatment, its p-value and standard error, the true effect and the bandwidth and intensity of the OVB considered (delta). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.

```{r compute_sim_RDD}
compute_sim_RDD <- function(N,
                            sigma_u,
                            mu_h,
                            sigma_h,
                            sigma_e,
                            alpha,
                            beta,
                            gamma,
                            delta,
                            q_c,
                            vect_bw) {
  
  data <- generate_data_rdd(
    N = N,
    sigma_u = sigma_u,
    mu_h = mu_h,
    sigma_h = sigma_h,
    sigma_e = sigma_e,
    alpha = alpha,
    beta = beta,
    gamma = gamma,
    delta = delta,
    q_c = q_c
  ) 
  
  map_dfr(vect_bw, estimate_rdd, data = data) %>%
    mutate(delta = delta)
} 
```

Here is an example of an output of this function.

```{r example_output_sim_RDD, echo=FALSE}
baseline_param_RDD %>% 
  pmap_dfr(compute_sim_RDD, vect_bw = c(0.1, 0.2)) %>% 
  kable()
```

### All simulations

We will run the simulations for different sets of parameters by mapping our `compute_sim_RDD` function on each set of parameters. We thus create a table with all the values of the parameters we want to test `param_rdd`. Note that in this table each set of parameters appears `n_iter` times as we want to run the analysis $n_{iter}$ times for each set of parameters.

```{r set_param_RDD}
simple_param_RDD <- tibble(
  N = 1000,
  sigma_u = 1,
  mu_h = 0,
  sigma_h = 1,
  sigma_e = 0.5,
  alpha = 1,
  beta = 0.7,
  gamma = 0.7,
  q_c = 0.5,
  delta = 3,
  vect_bw = 0.1
)

fixed_param_RDD <- baseline_param_RDD %>% #%>% rbind(...)
  mutate(vect_bw = 0.1)
# vect_bw <- seq(0.05, 0.4, 0.05)
vect_bw <- c(seq(0.05, 0.4, 0.05), seq(0.4, 1, 0.1))
vect_delta <- c(3)
n_iter <- 100

param_rdd <- fixed_param_RDD %>% 
  select(-delta, -vect_bw) %>% #parameters we modify
  crossing(delta = vect_delta) %>% 
  mutate(vect_bw = list(vect_bw)) %>% 
  crossing(rep_id = 1:n_iter) %>% 
  select(-rep_id)
```

We then run the simulations by mapping our `compute_sim_RDD` function on `param_rdd`.

```{r run_sim_RDD, eval=FALSE}
tic()
sim_rdd <- pmap_dfr(param_rdd, compute_sim_RDD)
beep()
toc()

# saveRDS(sim_rdd, here("Outputs/sim_rdd.RDS"))
```

## Analysis of the results

### Quick exploration

First, we quickly explore the results. In the following figure, we can see that for small bandwidth estimates are unbiased but imprecise while for large bandwidths estimates are precise but biased.

```{r result_density_RDD, echo=FALSE, fig.asp=0.8}
sim_rdd <- readRDS(here("Outputs/sim_rdd.RDS"))

sim_rdd %>%
  # filter(bw == vect_bw[5]) %>%
  mutate(bw_name = str_c("Bandwidth: ", bw)) %>% 
  ggplot(aes(x = estimate)) +
  geom_density() +
  # geom_vline(aes(xintercept = mean(estimate))) +
  geom_vline(aes(xintercept = true_effect), linetype = "dashed") +
  facet_wrap(~ bw_name) +
  labs(
    title = "Distribution of the estimates of the treatment effect",
    subtitle = "For different bandwidth sizes, as proportion of observations in the bandwidth",
    x = "Estimate of the treatment effect",
    y = "Density",
  )
```

When the bandwidth is relatively small, estimates are spread out and the mean of statistically significant estimates is larger than the true effect. Note that the average of all estimates, significant and non-significant, is close to the true effect. Applying a statistical significance filter leads to overestimate the true effect in this case.

```{r result_histogram_RDD, echo=FALSE}
sim_rdd %>% 
  filter(bw == vect_bw[2]) %>%
  mutate(significant = ifelse(p_value < 0.05, "Significant", "Non significant")) %>% 
  ggplot(aes(x = estimate, fill = significant)) + 
  geom_histogram(bins = 25) +
  geom_vline(
    aes(xintercept = mean(abs(estimate[significant == "Significant"]))),
    linetype = "solid"
  ) +
  geom_vline(aes(xintercept = 1)) +
  # facet_wrap(~ bw) +
  labs(
    title = "Distribution of the estimates of the treatment effect",
    subtitle = str_c("For a bandwidth containing ", vect_bw[2]*100, "% of the observations"),
    x = "Estimate of the treatment effect",
    y = "Count",
    fill = "",
    caption = "The solid line represents the average mean effect for significant 
    estimates (in absolute value), the dashed line represents the true effect"
  ) 

# sim_rdd %>% 
#   filter(bw == 0.2) %>% 
#   mutate(significant = (p_value < 0.05)) %>% 
#   group_by(delta) %>% 
#   mutate(sim_id = row_number()) %>% 
#   ungroup() %>% 
#   ggplot(aes(x = estimate, color = significant)) + 
#   geom_point(aes(x = sim_id, y = estimate)) +
#   geom_hline(aes(yintercept = mean(estimate))) +
#   facet_wrap(~ delta)
# 
# sim_rdd %>% 
#   group_by(delta, bw) %>% 
#   summarise(average_effect = mean(estimate))
```


### Computing bias and exaggeration ratio

We want to compare $\mathbb{E}\left[\left|\frac{\widehat{\beta_{RDD}}}{\beta_0}\right|\right]$ and $\mathbb{E}\left[\left|\frac{\widehat{\beta_{RDD}}}{\beta_0}\right| | signif \right]$. The first term represents the bias and the second term represents the exaggeration ratio. This terms depend on the true effect size. To enable comparison across simulations and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance. 

```{r summarise_RDD}
summarise_simulations <- function(data) {
  data %>%
    mutate(significant = (p_value <= 0.05)) %>% 
    group_by(delta, bw) %>%
    summarise(
      power = mean(significant, na.rm = TRUE)*100, 
      type_m = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),
      bias_all = mean(estimate/true_effect, na.rm = TRUE),
      .groups	= "drop"
    ) %>% 
    ungroup()
} 

summary_sim_rdd <- summarise_simulations(sim_rdd)
# saveRDS(summary_sim_rdd, here("Outputs/summary_sim_rdd.RDS"))
```

### Graph

To analyze our results, we build a unique and simple graph:

```{r main_graph_RDD, echo=FALSE}
summary_sim_rdd %>% 
  pivot_longer(cols = c(type_m, bias_all), names_to = "measure") %>% 
  mutate(
    measure = ifelse(measure == "type_m", "Significants", "All"),
    delta_name = paste("OVB intensity:", delta)
  ) %>% 
  ggplot(aes(x = bw, y = value, color = measure)) + 
  geom_line(size = 0.8) +
  labs(
    x = "Bandwidth size", 
    y = expression(paste("Average  ", frac("Estimate", "True Effect"))),
    color = "Estimates",
    title = "Evolution of bias with bandwidth size, conditional on significativity",
    # subtitle = "For several values of OVB intensity",
    caption = "Bandwidth size as a proportion of the total number of observations"
  ) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 5))
```

```{r main_graph_RDD_paper, dpi=600, echo=FALSE, eval=FALSE, include=FALSE}
#graph to be displayed in the paper
summary_sim_rdd %>% 
  pivot_longer(cols = c(type_m, bias_all), names_to = "measure") %>% 
  mutate(
    measure = ifelse(measure == "type_m", "Significants", "All"),
    delta_name = paste("OVB intensity:", delta)
  ) %>% 
  ggplot(aes(x = bw, y = value, color = measure)) + #, linetype = fct_rev(measure))) + 
  geom_line(size = 0.8) +
  labs(
    x = "Bandwidth size", 
    y = expression(paste("Average  ", frac("Estimate", "True Effect"))),
    linetype = "Estimates",
    color = "Estimates",
    # subtitle = "For several values of OVB intensity",
    caption = "Bandwidth size as a proportion of the total number of observations"
  ) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +
  theme_mediocre(base_family = "Lora") 
  # theme_mediocre(pal = "blackandwhite", base_family = "Lora") +
  # scale_mediocre_d(pal = "blackandwhite")
```

We notice that, the smaller the bandwidth size, the closer the average of all estimates is to the true effect. Yet, when the bandwidth gets small significant estimates overestimate the true effect. This arises because of a loss of power, as shown in the graph below.

```{r graph_results_RDD_power}
summary_sim_rdd %>% 
  ggplot(aes(x = bw, y = power)) + 
  geom_line(size = 0.8) +
  labs(
    x = "Bandwidth size",
    y = "Power",
    title = "Evolution of power with bandwith size",
    caption = "Bandwidth size as a proportion of the total number of observations"
  ) +
  xlim(c(0,100))
```

