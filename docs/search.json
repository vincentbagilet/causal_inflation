{
  "articles": [
    {
      "path": "index.html",
      "title": "Overcoming the Omitted Variable Bias at the Cost of Type M Error",
      "author": [],
      "contents": "\n\n          \n      \n      An OVB-type M trade off\n      \n      \n      Home\n      \n      \n      Simulations\n       \n      ▾\n      \n      \n      RDD\n      IV\n      \n      \n      \n      \n      \n      ☰\n      \n      \n      \n        \n          \n            \n              \n            \n              Overcoming the Omitted Variable Bias at the Cost of Type M Error\n            \n            \n              \n                \n                    \n                      \n                         GitHub\n                      \n                    \n                  \n                                  \n            \n          \n        \n        \n        \n          \n            Hi and welcome!\n            This website gathers code and additional material for the paper Overcoming the Omitted Variable Bias at the Cost of Type M Error by Vincent Bagilet and Léo Zabrocki.\n            The website is under construction and the analysis is still in a preliminary stage.\n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Overcoming the Omitted Variable Bias at the Cost of Type M Error\n            \n            \n              \n                \n                                    \n                    \n                       GitHub\n                    \n                  \n                                  \n              \n            \n            \n              Hi and welcome!\n              This website gathers code and additional material for the paper Overcoming the Omitted Variable Bias at the Cost of Type M Error by Vincent Bagilet and Léo Zabrocki.\n              The website is under construction and the analysis is still in a preliminary stage.\n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2021-10-27T16:27:46-04:00"
    },
    {
      "path": "IV.html",
      "title": "Simulations OVB/type M trade-off: IV",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nPurpose of the document\nAn illustrative exampleModelisation choices\nData generation\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\n\n\n\nbody {\ntext-align: justify}\nPurpose of the document\nIn this document, we run a simulation exercise to illustrate the existence of a trade-off between Omitted Variable Bias (OVB) and type M error in the context of an Instrumental Variable (IV) strategy. This trade-off is mediated by\nAn illustrative example\nTo illustrate this trade-off between OVB and type M error, we consider a standard application of the IV design in environmental economics.\nModelisation choices\nIn the present analysis, we build our simulations to replicate a similar type of quasi-experiment.\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nTo simplify, we consider the following assumptions:\nMore precisely, we set:\n\\(N\\) the number of observations\n\\(U \\sim \\mathcal{N}(0, \\sigma_u^{2})\\) the unobserved\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\n\n\ngenerate_data_IV <- function(N = 1000,\n                             sigma_u = 1,\n                             sigma_z = 1,\n                             sigma_ex = 1,\n                             sigma_ey = 1,\n                             alpha_y = 0,\n                             alpha_x = 0,\n                             beta = 1, #treatment effect\n                             gamma = 0.3, #IV strength\n                             delta = 2 #OVB intensity\n                             ) {\n  \n  data <- tibble(id = 1:N) %>% \n    mutate(\n      z = rnorm(nrow(.), 0, sigma_z),\n      u = rnorm(nrow(.), 0, sigma_u),\n      e_x = rnorm(nrow(.), 0, sigma_ex),\n      e_y = rnorm(nrow(.), 0, sigma_ey),\n      x = alpha_x + gamma*z + delta*u + e_x,\n      y = alpha_y + beta*x + delta*u + e_y\n    )\n  \n  return(data)\n}\n\n\n\nThe following graph illustrates this process by plotting final test scores against qualification ones depending on the value of treated_bw.\n\n\n\nEstimation\nAfter generating the data, we can run an estimation. We want to compare the IV and the OLS for different IV strength values. Hence, we need to estimate both an IV and an OLS and return both set of outcomes of interest.\n\n\nestimate_IV <- function(data) {\n  reg_IV <- ivreg(\n    data = data, \n    formula = y ~ x | z\n    ) %>% \n    broom::tidy() %>%\n    filter(term == \"x\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se) %>% \n    mutate(model = \"IV\")\n  \n  reg_OLS <- ivreg(\n    data = data, \n    formula = y ~ x\n    ) %>% \n    broom::tidy() %>%\n    filter(term == \"x\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se) %>% \n    mutate(model = \"OLS\")\n  \n  reg <- reg_IV %>% \n    rbind(reg_OLS)\n  \n  return(reg)\n}\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_IV and estimate_IV. To do so we create the function compute_simulation_IV. This simple function takes as input the various parameters along with the bandwidth size, bw. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect and the bandwidth and intensity of the OVB considered (delta). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_simulation_IV <- function(N = 500, \n                                  sigma_u = 1,\n                                  sigma_z = 1,\n                                  sigma_ex = 1,\n                                  sigma_ey = 1,\n                                  alpha_y = 0,\n                                  alpha_x = 0,\n                                  beta = 1, \n                                  gamma = 0.3, \n                                  delta = 2) {\n  generate_data_IV(\n    N = N, \n    sigma_u = sigma_u,\n    sigma_z = sigma_z, \n    sigma_ex = sigma_ex, \n    sigma_ey = sigma_ey,\n    alpha_y = alpha_y, \n    alpha_x = alpha_x,\n    beta = beta,\n    gamma = gamma,\n    delta = delta\n  ) %>% \n  estimate_IV() %>% \n  mutate(\n    gamma = gamma,\n    delta = delta\n  )\n} \n\n\n\nHere is an example of an output of this function.\n\nestimate\np_value\nse\nmodel\ngamma\ndelta\n1.262697\n1.5e-06\n0.2590898\nIV\n0.3\n2\n1.783966\n0.0e+00\n0.0266822\nOLS\n0.3\n2\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_simulation_IV function on each set of parameters. We thus create a table with all the values of the parameters we want to test param_IV. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\nvect_gamma <- c(seq(0.05, 0.4, 0.05), seq(0.4, 1, 0.1))\nvect_delta <- c(0.4)\nn_iter <- 1000\n\nparam_IV <- crossing(vect_gamma, vect_delta) %>% \n  rename(gamma = vect_gamma, delta = vect_delta) %>% \n  crossing(rep_id = 1:n_iter) %>% \n  select(-rep_id)\n\n\n\nWe then run the simulations by mapping our compute_simulation_IV function on param_IV.\n\n\ntic()\nsimulations_IV <- pmap_dfr(param_IV, compute_simulation_IV)\nbeep()\ntoc()\n\n# saveRDS(simulations_IV, here(\"Outputs/simulations_IV.RDS\"))\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta_{RDD}}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta_{RDD}}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_simulations <- function(data, true_effect = 1) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(delta, gamma, model) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),\n      bias_sign = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_simulations_IV <- summarise_simulations(simulations_IV)\n# saveRDS(summary_simulations_IV, here(\"Outputs/summary_simulations_IV.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\n\n\n\n",
      "last_modified": "2021-10-27T16:28:00-04:00"
    },
    {
      "path": "RDD.html",
      "title": "Simulations OVB/type M trade-off: RDD",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://www.sipa.columbia.edu/experience-sipa/sipa-profiles/vincent-bagilet"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nPurpose of the document\nAn illustrative exampleModelisation choices\nData generation\nDefining the bandwidth\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\n\n\n\nbody {\ntext-align: justify}\nPurpose of the document\nIn this document, we run a simulation exercise to illustrate the existence of a trade-off between Omitted Variable Bias (OVB) and type M error in the context of a Regression Discontinuity Design (RDD). This trade-off is mediated by the size of the bandwidth considered in the analysis. The underlying idea is that the smaller the bandwidth, the more comparable units are and therefore the smaller the risk of OVB is. Yet, with a smaller bandwidth, sample size and thus power decrease, increasing type M error.\nAn illustrative example\nTo illustrate this trade-off between OVB and type M error, we consider a standard application of the RD design in economics of education where an grant or additional lessons are assigned based on the score obtained by students on a standardized test. Students with test scores below a given threshold receive the treatment while those above do not. Yet, students far above and far below the threshold may differ along unobserved characteristics such as ability. To limit this bias, the effect of the treatment is estimated by comparing the outcomes of students just below and just above this threshold. This enable to limit disparities in terms of unobserved characteristics.\nThistlewaite and Campbell (1960) introduced the concept of RDD using this type of quasi-experiment. In their paper, they take advantage of a sharp discontinuity in the assignment of an award (a Certificate of Merit) based on qualifying scores at a test. This type of analysis is still used today and many papers leveraging similar methodologies have been published since this seminal work. For instance, Jacob and Lefgren (2004) exploit this type of discontinuity to study the impact of summer school and grade retention programs on test scores. Students who score below a given score are required to attend a summer school and to retake the test. Students who do not pass the second have to repeat the grade.\nModelisation choices\nIn the present analysis, we build our simulations to replicate a similar type of quasi-experiment. In our fictional example, all students scoring below a cutoff \\(C\\) in a qualification test are required to take additional lessons. We want to estimate the effect of these additional lessons on scores on a final test taken by all students a year later.\nWe assume that the final score of student \\(i\\), \\(Final_i\\), is correlated with their qualification score \\(Qual_i\\) and their treatment status \\(T_i\\), ie whether student \\(i\\) received additional lessons or not. We further assume that both qualification and final test scores are affected by students’ unobserved ability \\(U_i\\) in a non linear way.\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nFinal test scores are thus defined as follows:\n\\[Final_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} +  \\delta f(U_i) + \\epsilon_{i}\\] Where \\(\\alpha\\) is a constant, \\(f\\) a non linear function and \\(e \\sim \\mathcal{N}(0, \\sigma_{e})\\) noise. The parameter of interest is \\(\\beta\\). Translating this into a potential outcomes framework, we have \\(Final_i(0) = \\alpha + \\gamma Qual_{i} + \\delta f(U_i) + \\epsilon_{i}\\) and \\(Final_i(1) = \\alpha + \\gamma Qual_{i} + \\beta + \\delta f(U_i) + \\epsilon_{i}\\)\n\nTo simplify, we consider the following assumptions:\nFull compliance and a sharp treatment allocation such that \\(T_i = \\mathbb{I}[Qual_{i} < C]\\). All students with a qualification score below the threshold are treated and receive additional lessons. None of the students with a qualification score above the threshold are treated.\nThe unobserved ability affects qualification and final test scores in a cubic way. A large ability has a strong positive impact on test scores. Similarly a particularly low ability strongly impacts test scores negatively. An average ability does not have much impact on test scores. Such a functional form seems realistic. Note that ability creates an OVB only if it has a non linear impact on test scores.\nWe assume constant treatment effects. This assumption is not necessary and our results hold if we consider non-constant treatment effects. We thus may drop this assumption in the future.\nWe assume that the unobserved availability affects the qualification and final score in a similar way and therefore with the same intensity \\(\\delta\\).\nMore precisely, we set:\n\\(N\\) the number of students\n\\(U \\sim \\mathcal{N}(0, \\sigma_u^{2})\\) the unobserved ability.\n\\(Qual_i = H_i + \\delta U_i^{3}\\) where \\(H \\sim \\mathcal{N}(\\mu_h, \\sigma_h^{2})\\). We center the qualification scores such that treated units are below 0 and non treated ones above.\n\\(T_i = \\mathbb{I}[Qual_{i} < q_c]\\) where for now and for simplicity, \\(q_c\\) is a fixed grade threshold given as the quantile in the qualification score distribution.\n\\(e \\sim \\mathcal{N}(0, \\sigma_e^2)\\)\n\\(Final_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} + \\delta U_i^{3} + e_{i}\\)\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis. We set baseline values for the parameters to emulate a somehow realistic observational study in this field. The set of parameters may produce test score outside of the range 0-100 in some iterations but that does not affect the analysis.\nOnce the fake data is generated, to make things more realistic we consider our data as if it was actual data. We do not take advantage of our knowledge of the data generating process in the estimation procedure. However, we observe both potential outcomes and the unobserved ability. Note that, in a real world setting, one would generally know the value of the threshold (and thus of \\(q_c\\)). Based on that and to simplify the computation of the bandwidth, we store \\(q_c\\).\n\n\ngenerate_data_rdd <- function(N = 1000, \n                              sigma_u = 1,\n                              mu_h = 0, \n                              sigma_h = 1, \n                              sigma_e = 1, \n                              alpha = 1, \n                              beta = 1,\n                              gamma = 0.3,\n                              delta = 2,\n                              q_c = 0.5\n                              ) {\n  \n  data <- tibble(id = 1:N) %>% \n    mutate(\n      # qual = rnorm(nrow(.), mu_h, sigma_h),\n      # u = rnorm(nrow(.), 0.5, sigma_u) + qual + 0.3*qual^3,\n      u = rnorm(nrow(.), 0, sigma_u),\n      qual = rnorm(nrow(.), mu_h, sigma_h) + delta*u^2,\n      e = rnorm(nrow(.), 0, sigma_e),\n      # qual_c = qual - quantile(qual, q_c),\n      # treated = qual_c < 0,\n      # threshold = quantile(qual, q_c),\n      treated = qual < quantile(qual, q_c),\n      final0 = alpha + gamma*qual + delta*u^2 + e,\n      final1 = final0 + beta,\n      final = final0 + beta*treated,\n      q_c = q_c\n    )\n  \n  return(data)\n}\n\n\n\nHere is an example of data created through this process:\n\nid\nu\nqual\ne\ntreated\nfinal0\nfinal1\nfinal\nq_c\n1\n2.2483385\n11.0747447\n1.3981792\nFALSE\n15.8306542\n16.8306542\n15.8306542\n0.5\n2\n-0.1885190\n-0.6029233\n-1.2173272\nTRUE\n-0.3271253\n0.6728747\n0.6728747\n0.5\n3\n0.6774519\n0.0451063\n-1.2068965\nTRUE\n0.7245175\n1.7245175\n1.7245175\n0.5\n4\n-0.2381472\n0.5318733\n-0.0274091\nTRUE\n1.2455811\n2.2455811\n2.2455811\n0.5\n5\n-0.7870785\n1.1422335\n2.0961495\nFALSE\n4.6778046\n5.6778046\n4.6778046\n0.5\n6\n1.9030090\n7.9555108\n-0.4787354\nFALSE\n10.1508042\n11.1508042\n10.1508042\n0.5\n7\n-0.4453993\n-0.4565606\n-1.6621091\nTRUE\n-0.4023162\n0.5976838\n0.5976838\n0.5\n8\n0.9361622\n3.2279327\n1.7381450\nFALSE\n5.4593243\n6.4593243\n5.4593243\n0.5\n9\n-0.7283127\n0.5806054\n-1.3040565\nTRUE\n0.9310039\n1.9310039\n1.9310039\n0.5\n10\n0.3298802\n1.7532184\n-0.2527274\nFALSE\n1.4908800\n2.4908800\n1.4908800\n0.5\n\nDefining the bandwidth\nIn a RDD, the model is estimated only for observations close enough to the threshold, ie in a given bandwidth. We therefore create a function to define this bandwidth by adding a variable to the data set treated_bw that is equal to NA if the observations is outside of the bandwidth, TRUE if the observation falls in the bandwidth and the student is treated and FALSE if the observation falls in the bandwidth and the student is not treated. The bandwidth parameter bw represents the proportion of units that are in the bandwidth. If bw = 0.1, 10% of the students are in the bandwidth for instance.\n\n\ndefine_bw <- function (data, bw = 0.1) {\n  data <- data %>% \n    mutate(\n      treated_bw = ifelse(\n        dplyr::between(\n          qual, \n          quantile(qual, unique(q_c) - bw/2), \n          quantile(qual, unique(q_c) + bw/2)\n        ), \n        treated, \n        NA\n      )\n    )\n} \n\n\n\nThe following graph illustrates this process by plotting final test scores against qualification ones depending on the value of treated_bw.\n\n\n\nEstimation\nAfter generating the data, we can run an estimation.\nNote that to run power calculations, we need to have access to the true effects. Therefore, before running the estimation, we write a short function to compute the average treatment effect on the treated (ATET). We will add this information to the estimation results.\n\n\ncompute_true_effect_rdd <- function(data) {\n  treated_data <- data %>% \n    filter(treated) \n  return(mean(treated_data$final1 - treated_data$final0))\n}  \n\n\n\nWe then run the estimation. To do so, we only consider observations within the bandwidth and regress the final test scores on the treatment, the qualification score and their interaction. Note that we include this interaction term to allow more flexibility and to mimic an realistic estimation. Yet, we know that this interaction term does not appear in the DGP. Including it or not do not change the results. Also note that, of course, we do not include the unobserved ability in this model to create an OVB.\n\n\nestimate_rdd <- function(data, bw) {\n  data_in_bw <- data %>% \n    define_bw(bw = bw) %>% \n    filter(!is.na(treated_bw))\n  \n  reg <- lm(\n    data = data_in_bw, \n    formula = final ~ treated + qual\n  ) %>% \n  broom::tidy() %>%\n  filter(term == \"treatedTRUE\") %>%\n  rename(p_value = p.value, se = std.error) %>%\n  select(estimate, p_value, se) %>%\n  mutate(true_effect = compute_true_effect_rdd(data))\n  \n  return(reg)\n}\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_rdd and estimate_rdd. To do so we create the function compute_simulation_rdd. This simple function takes as input the various parameters along with the bandwidth size, bw. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect and the bandwidth and intensity of the OVB considered (delta). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_simulation_rdd <- function(N = 500, \n                              sigma_u = 1,\n                              mu_h = 0, \n                              sigma_h = 1, \n                              sigma_e = 0.5, \n                              alpha = 1, \n                              beta = 1,\n                              gamma = 0.7,\n                              delta = 1,\n                              q_c = 0.5,\n                              bw = 0.1) {\n  generate_data_rdd(\n    N = N, \n    sigma_u = sigma_u,\n    mu_h = mu_h, \n    sigma_h = sigma_h, \n    sigma_e = sigma_e,\n    alpha = alpha, \n    beta = beta,\n    gamma = gamma,\n    delta = delta,\n    q_c = q_c\n  ) %>% \n  estimate_rdd(bw = bw) %>% \n  mutate(\n    bw = bw,\n    delta = delta\n  )\n} \n\n\n\nHere is an example of an output of this function.\n\nestimate\np_value\nse\ntrue_effect\nbw\ndelta\n1.134022\n0.0048151\n0.3831856\n1\n0.1\n1\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_simulation_rdd function on each set of parameters. We thus create a table with all the values of the parameters we want to test param_rdd. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\n# vect_bw <- seq(0.05, 0.4, 0.05)\nvect_bw <- c(seq(0.05, 0.4, 0.05), seq(0.4, 1, 0.1))\nvect_delta <- c(3)\nn_iter <- 1000\n\nparam_rdd <- crossing(vect_bw, vect_delta) %>% \n  rename(bw = vect_bw, delta = vect_delta) %>% \n  crossing(rep_id = 1:n_iter) %>% \n  select(-rep_id)\n\n\n\nWe then run the simulations by mapping our compute_simulation_rdd function on param_rdd.\n\n\ntic()\nsimulations_rdd <- pmap_dfr(param_rdd, compute_simulation_rdd)\nbeep()\ntoc()\n\n# saveRDS(simulations_rdd, here(\"Outputs/simulations_rdd.RDS\"))\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta_{RDD}}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta_{RDD}}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_simulations <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(delta, bw) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),\n      bias_sign = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_simulations_rdd <- summarise_simulations(simulations_rdd)\n# saveRDS(summary_simulations_rdd, here(\"Outputs/summary_simulations_rdd.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\n\n\n\n",
      "last_modified": "2021-10-27T16:28:08-04:00"
    }
  ],
  "collections": []
}
