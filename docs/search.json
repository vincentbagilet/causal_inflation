{
  "articles": [
    {
      "path": "DID.html",
      "title": "Simulations DID / Event study",
      "description": "In this document, we run a simulation exercise to illustrate the loss in power and resulting type M error when the number of events in a Difference In Differences design decreases.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModeling choices\nData generation\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\n\nVarying the proportion of treated\n\n\nbody {\ntext-align: justify}\nSummary and intuition\nIn the case of Event studies and DiD, the OVB/type M trade-off is mediated by the number of events.\nAn illustrative example\n\n\nModeling choices\nTo simplify, I consider the assumptions described below. Of course these assumptions are purely arbitrary and I invite you to play with them. Note that, fixed effects and the covariate are not necessary to the analysis. I only add them to make the analysis more realistic if necessary but I set their baseline values to 0.\nEach individual has fixed characteristics drawn from a normal distribution,\nEach time period presents fixed characteristics also drawn from a normal distribution,\nA unique covariate \\(x\\) drawn from a normal distribution,\nA proportion \\(p_{treat}\\) of individuals are ever treated over the period. Hence, a proportion of \\(1-p_{treat}\\) individuals are never treated over the period. I draw these individual at random. Note that the value of the individual identifiers do not matter here. So I could assume that the non-treated individuals are those with the larger individual ids for instance,\nThe implementation of the treatment can be staggered or not. If it is not staggered, the treatment date is set to be in the middle of the period\nThe treatment can vary along two dimensions, time and individual. Details are given below.\nMore precisely, I set:\n\\(N_i\\) the number of individual\n\\(N_t\\) the number of periods\n\\(\\lambda_i \\sim \\mathcal{N}(\\mu_{IFE}, \\sigma_{IFE}^{2})\\) the fixed effect for individual \\(i\\)\n\\(\\eta_t \\sim \\mathcal{N}(\\mu_{TFE}, \\sigma_{TFE}^{2})\\) the fixed effect for time period \\(t\\)\n\\(x_{it} \\sim \\mathcal{N}(\\mu_{x}, \\sigma_{x}^{2})\\)\n\\(e_{it} \\sim \\mathcal{N}(0, \\sigma_{e}^{2})\\) some noise\n\\(T_{it}\\) represent the treatment allocation, it is equal to one if individual \\(i\\) is treated at time \\(t\\) and 0 otherwise,\n\\(y_{it} = \\alpha + \\beta_{it} T_{it} + \\gamma x_{it} + \\lambda_i + \\eta_t + e_{it}\\) where \\(\\alpha\\) and \\(\\gamma\\) are some constants.\n\\(\\beta_{it}\\) is represents the magnitude of the treatment effect and is linked to the input parameter beta.\nAcross individuals, the treatment can either be:\nhomogeneous: het_indiv == homogeneous, for each individual, the treatment is equal to beta,\nrandom: het_indiv == random, for each individual, the treatment is drawn from \\(\\mathcal{U}(0.5\\beta, 1.5\\beta)\\),\nlarger for those that are treated first: het_indiv == large_first, for each individual, the treatment is equal to \\(N_t - \\beta\\).\n\nAcross time, the effect of the treatment can either be\nconstant: het_time == constant,\nincreasing linearly in time: het_time == linear.\n\n\nI also create a bunch of variables that can be useful:\n\\(InTreatment_i\\) equal to 1 if individual \\(i\\) ever gets treated,\n\\(t^{event}_i\\) equal to the date at which individual \\(i\\) gets treated,\n\\(t^{centered}_i\\) representing the distance in terms of period to the beginning of the treatment for individual \\(i\\),\n\\(Post_{it}\\) equal to 1 if the period \\(t\\) is after the treatment has begun for individual \\(i\\). This variable is only useful for non-staggered treatment allocation,\nData generation\nI write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\n\n\ngenerate_data_DID <- function(N_i,\n                              N_t,\n                              sigma_e,\n                              p_treat,\n                              staggered,\n                              het_indiv,\n                              het_time,\n                              alpha,\n                              beta,\n                              mu_indiv_fe = 0, \n                              sigma_indiv_fe = 0,\n                              mu_time_fe = 0, \n                              sigma_time_fe = 0,\n                              mu_x = 0, \n                              sigma_x = 0,\n                              gamma = 0\n                             ) {\n\n  if (!is.logical(staggered)) {stop(\"staggered must be logical\")} \n  if (!(het_indiv %in% c(\"large_first\", \"random\", \"homogeneous\"))) {\n    stop('het_indiv must be either \"large_first\", \"random\" or \"homogeneous\"')\n  } \n  if (!(het_time %in% c(\"constant\", \"linear\"))) {\n    stop('het_time must be either \"constant\" or \"linear\"')\n  } \n  \n  data <- tibble(indiv = 1:N_i) %>%\n    mutate(in_treatment = (indiv %in% sample(1:N_i, floor(N_i*p_treat)))) %>% \n    crossing(t = 1:N_t) %>%\n    group_by(indiv) %>%\n    mutate(\n      indiv_fe = rnorm(1, mu_indiv_fe, sigma_indiv_fe),\n      t_event = ifelse(staggered, sample(2:(N_t - 1), 1), floor(N_t/2)), \n        #I use 2:(N_t-1) to have a pre and post period\n      t_event = ifelse(in_treatment, t_event, NA),\n      beta_i = case_when(\n        het_indiv == \"large_first\" ~ N_t-t_event,\n        het_indiv == \"random\" ~ runif(1, beta*0.5, beta*1.5), \n        het_indiv == \"homogeneous\" ~ beta\n      ),\n      beta_i = ifelse(is.na(t_event), 0, beta_i)\n    ) %>%\n    ungroup() %>%\n    group_by(t) %>%\n    mutate(time_fe = rnorm(1, mu_time_fe, sigma_time_fe)) %>%\n    ungroup() %>%\n    mutate(\n      post = (t > t_event),\n      treated = in_treatment & post, \n      beta_i = ifelse(\n        het_time == \"linear\" & post & !is.na(t_event),\n        beta_i*(t - t_event), \n        beta_i\n      ),\n      t_centered = t - t_event,\n      x = rnorm(nrow(.), mu_x, sigma_x),\n      e = rnorm(nrow(.), 0, sigma_e),\n      y0 = alpha + gamma * x + indiv_fe + time_fe + e,\n      y1 = y0 + beta_i,\n      y = treated*y1 + (1 - treated)*y0\n    )\n  \n  return(data)\n}\n\n\n\nI set baseline values for the parameters as very standard. These values are arbitrary.\n\n\nbaseline_param_DID <- tibble(\n  N_i = 20,\n  N_t = 50,\n  sigma_e = 1,\n  p_treat = 0.5,\n  staggered = FALSE,\n  het_indiv = \"homogeneous\",\n  het_time = \"constant\",\n  alpha = 1,\n  beta = 1\n)\n\n\n\nHere is an example of data created with the data generating process and baseline parameter values, for 2 individuals and 8 time periods:\n\nindiv\nin_treatment\nt\nindiv_fe\nt_event\nbeta_i\ntime_fe\npost\ntreated\nt_centered\nx\ne\ny0\ny1\ny\n1\nFALSE\n1\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-2.1674942\n-1.1674942\n-1.1674942\n-1.1674942\n1\nFALSE\n2\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-0.7731616\n0.2268384\n0.2268384\n0.2268384\n1\nFALSE\n3\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-0.2794447\n0.7205553\n0.7205553\n0.7205553\n1\nFALSE\n4\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-0.7092973\n0.2907027\n0.2907027\n0.2907027\n1\nFALSE\n5\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n0.3442857\n1.3442857\n1.3442857\n1.3442857\n1\nFALSE\n6\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-1.5789894\n-0.5789894\n-0.5789894\n-0.5789894\n1\nFALSE\n7\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-2.1478112\n-1.1478112\n-1.1478112\n-1.1478112\n1\nFALSE\n8\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n1.0627709\n2.0627709\n2.0627709\n2.0627709\n2\nTRUE\n1\n0\n4\n1\n0\nFALSE\nFALSE\n-3\n0\n1.6618032\n2.6618032\n3.6618032\n2.6618032\n2\nTRUE\n2\n0\n4\n1\n0\nFALSE\nFALSE\n-2\n0\n0.3135374\n1.3135374\n2.3135374\n1.3135374\n2\nTRUE\n3\n0\n4\n1\n0\nFALSE\nFALSE\n-1\n0\n-0.6204604\n0.3795396\n1.3795396\n0.3795396\n2\nTRUE\n4\n0\n4\n1\n0\nFALSE\nFALSE\n0\n0\n-0.3895688\n0.6104312\n1.6104312\n0.6104312\n2\nTRUE\n5\n0\n4\n1\n0\nTRUE\nTRUE\n1\n0\n-0.0253525\n0.9746475\n1.9746475\n1.9746475\n2\nTRUE\n6\n0\n4\n1\n0\nTRUE\nTRUE\n2\n0\n0.1725214\n1.1725214\n2.1725214\n2.1725214\n2\nTRUE\n7\n0\n4\n1\n0\nTRUE\nTRUE\n3\n0\n-1.0051718\n-0.0051718\n0.9948282\n0.9948282\n2\nTRUE\n8\n0\n4\n1\n0\nTRUE\nTRUE\n4\n0\n0.8420314\n1.8420314\n2.8420314\n2.8420314\n\nLet’s now have a look at different types of treatment and treatment allocations. First, let’s look at treatment allocation mechanisms. The allocation can either be staggered or not, the treatment homogeneous across individual or not and cconstant or not in time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimation\nAfter generating the data, we can run an estimation.\n\n\nestimate_DID <- function(data) {\n  reg <- data %>% \n    mutate(\n      indiv = as.factor(indiv),\n      t = as.factor(t),\n      treated = as.numeric(treated),\n      in_treatment = as.numeric(in_treatment),\n      t_centered = as.factor(t_centered)\n    ) %>% \n    feols(\n      data = ., \n      fml = y ~ treated | indiv + t\n    ) %>% \n    broom::tidy() %>% \n    rename(p_value = p.value, se = std.error) %>% \n    select(-statistic) \n    # suppressMessages() #Warning saying that NA values dropped and \n    # #that one or two factors are removed due to colinearity\n  \n  return(reg)\n}\n\n\n\n\n\nbaseline_param_DID %>% \n  pmap_dfr(generate_data_DID) %>%\n  estimate_DID() %>% \n  slice(1:15) %>% \n  kable()\n\n\nterm\nestimate\nse\np_value\ntreated\n0.955986\n0.1258343\n4e-07\n\nOne simulation\nWe can now run a simulation, combining generate_data_DID and estimate_DID. To do so we create the function compute_sim_DID. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect, the IV strength and the intensity of the OVB considered (ovb_intensity). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_true_effect_DID <- function(data) {\n  data %>% \n    filter(treated) %>% \n    summarise(true_effect = mean(y1 - y0)) %>% \n    .$true_effect\n}  \n\n\n\n\n\ncompute_sim_DID <- function(N_i,\n                                    N_t,\n                                    sigma_e,\n                                    p_treat,\n                                    staggered,\n                                    het_indiv,\n                                    het_time,\n                                    alpha,\n                                    beta,\n                                    mu_indiv_fe = 0,\n                                    sigma_indiv_fe = 0,\n                                    mu_time_fe = 0,\n                                    sigma_time_fe = 0,\n                                    mu_x = 0,\n                                    sigma_x = 0,\n                                    gamma = 0) {\n  data <- generate_data_DID(\n    N_i = N_i,\n    N_t = N_t,\n    sigma_e = sigma_e,\n    p_treat = p_treat,\n    staggered = staggered,\n    het_indiv = het_indiv,\n    het_time = het_time,\n    alpha = alpha,\n    beta = beta,\n    mu_indiv_fe = mu_indiv_fe,\n    sigma_indiv_fe = sigma_indiv_fe,\n    mu_time_fe = mu_time_fe,\n    sigma_time_fe = sigma_time_fe,\n    mu_x = mu_x,\n    sigma_x = sigma_x,\n    gamma = gamma\n  ) \n  \n  data %>%\n    estimate_DID() %>% \n    mutate(\n      N_i = N_i,\n      N_t = N_t,\n      p_treat = p_treat,\n      true_effect = compute_true_effect_DID(data)\n    )\n} \n\n\n\nHere is an example of an output of this function.\n\nterm\nestimate\nse\np_value\nN_i\nN_t\np_treat\ntrue_effect\ntreated\n1.162665\n0.1356029\n1e-07\n20\n50\n0.5\n1\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_DID function on each set of parameters. We thus create a table with all the values of the parameters we want to test, param_DID. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\n\nWe then run the simulations by mapping our compute_sim_IV function on param_IV.\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta_{i}}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta_{IV}}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_sim_DID <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(p_treat, N_i, N_t) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),\n      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      bias_all_median = median(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_sim_DID <- summarise_sim_DID(sim_DID)\n# saveRDS(summary_sim_DID, here(\"Outputs/summary_sim_DID.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\nVarying the proportion of treated\n\n69.357 sec elapsed\n\n\n\n\n\n",
      "last_modified": "2022-01-10T11:18:56+01:00"
    },
    {
      "path": "experiments.html",
      "title": "Analyzing Experimental Studies for Intuition",
      "description": "In this document, we analyze replication of experimental studies to illustrate the concept of type M error.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nLaboratory experimentsAnalyzing all studies\nFocus on one particular study\n\nRandomized Control Trials\n\n\nbody {\ntext-align: justify}\nLaboratory experiments\nFirst, we consider the paper Camerer et al (2016). The authors of this paper replicated laboratory experiments in economics. We analyze their results through the prism of statistical power.\nThey report their replication results, alongside the original results, in table S1 Note that some p-values are strictly smaller than 0.001. As we do not have more information, we set them to 0.001. We also back out the standard errors from the p-values as they will be useful for our analysis.\nWe compute the power of the initial analysis if the true effect is in fact equal to the replication’s. We store the results in retro_camerer.\n\n\nrep_camerer <- read_excel(here(\"Misc\", \"rep_camerer.xlsx\")) %>% \n  mutate(\n    se_original = effect_original/qnorm(1 - pvalue_original), #incorrect\n    se_rep = effect_rep/qnorm(1 - pvalue_rep) #incorrect\n  ) \n\nretro_camerer <- rep_camerer %>% \n  mutate(\n    se_original = effect_original/qnorm(1 - pvalue_original), #incorrect\n    se_rep = effect_rep/qnorm(1 - pvalue_rep) #incorrect\n  ) %>% \n  select(A = effect_rep, s = se_original) %>% \n  # select(A, s) %>% \n  pmap_dfr(retrodesign) %>% \n  cbind(rep_camerer) %>% \n  as_tibble()\n\n\n\nAnalyzing all studies\nWe quickly plot and analyze the results obtained, ie the distribution of the exaggeration ratio and power.\n\n\n\nThe median power would be 0.38%. The median replicated estimates is equal to 0.62 times the original estimate.\nWe then compute the number and proportion of original studies that were statistically significant.\n\nOriginal estimate statistically significant\nNumber\nProportion\nNo\n2\n0.11\nYes\n16\n0.89\n\nWe then do the same thing for the replication studies.\n\nReplication estimate statistically significant\nNumber\nProportion\nNo\n7\n0.39\nYes\n11\n0.61\n\nWe then compute the proportion of original studies that would have adequate power as defined by the customary and arbitrary 80% threshold, still assuming that the true effect is equal to the replication one.\n\nAdequate power\nNumber\nProportion\nNo\n14\n0.78\nYes\n4\n0.22\n\nFocus on one particular study\nHere, we focus on one particular study in order to illustrate in more details the problem at play. We want to simulate what could have yielded replication of the initial study if the true effect was equal to the replication estimate.\nWe select one of the studies and draw the graph of interest.\n\n\n\nIn red is the estimate from the original study and its 95% confidence interval\nThe estimate is significant and has been published. Yet, it is pretty noisy.\nIn blue is the estimate from the replicated study and its 95% confidence interval\nWe notice that this second estimate is both more precise and smaller than the initial one. It still remains noisy\nLet’s assume that the true effect is actually equal to this second estimate (note that this is unlikely)\nWould the design of the initial study be good enough to detect this true effect? ie if we replicated the initial study, could we reject the null of no effect (knowing that the true effect is equal to the replicated estimate)\nIn gray is the estimate form the replicated study but with a standard error equal to the initial study’s (approximately the standard errors that would have been obtained with the design of the initial study)\nThis estimate is non significant. In this instance, we would not have been able to reject the null of no effect\nNow, if we replicate this study 500 times, running 500 lab experiments, in some cases we would get statisitcally significant estimates (the beige dots) and in some others non statistically significant ones (the green dots)\nIf we would have been a bit more lucky, we could have gotten a sample of individuals that would have yielded one of the beige estimates\nNow, we notice that, on average, statistically significant estimates overestimate the true effect by a factor 1.7 (average of 0.53 while the true effect is 0.31). Gelman and Carlin call this inflation factor type M error.\nIn this case, the power is basically the proportion of statistically significant estimates\nIf the study had more power, the sd would be smaller and most estimates would be statistically significative (because there is indeed a non null effect)\nBut since the power is low (33%), if by chance the sample of individuals we get yields a statistically significant estimate, this estimate will overestimate the true effect\nRandomized Control Trials\nWe want to look at replications of RCTs in Development Economics. To do so, we use the list of replication papers put together by Sandip Sukhtankar.\nWe gather the list of RCTs that have been replicated in Development Economics.\n\n\nrep_dvpt <- read_dta(here(\"Misc\", \"replication_data_final.dta\"))\n\n# rep_dvpt %>% \n#   filter((RCT == \"Yes\") & (Replicated == \"Replicated\")) %>% \n#   .$ReplicationPaperTitle\n\n\n\n\n\n\n",
      "last_modified": "2022-01-10T11:19:08+01:00"
    },
    {
      "path": "index.html",
      "title": "Unbiased but Inflated Causal Effects",
      "author": [],
      "contents": "\n\n          \n      \n      Unbiased but Inflated Causal Effects\n      \n      \n      Home\n      Intuition\n      \n      \n      Simulations\n       \n      ▾\n      \n      \n      RDD\n      IV\n      Matching\n      DiD / Event study\n      Panoptic issues\n      \n      \n      \n      \n      \n      ☰\n      \n      \n      \n        \n          \n            \n              \n            \n              Unbiased but Inflated Causal Effects\n            \n            \n              \n                \n                    \n                      \n                         GitHub\n                      \n                    \n                  \n                                  \n            \n          \n        \n        \n        \n          \n            Hi and welcome!\n            This website gathers code and additional material for the paper “Unbiased but Inflated Causal Effects” by Vincent Bagilet and Léo Zabrocki.\n            The website is under construction and the analysis is still at a preliminary stage.\n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Unbiased but Inflated Causal Effects\n            \n            \n              \n                \n                                    \n                    \n                       GitHub\n                    \n                  \n                                  \n              \n            \n            \n              Hi and welcome!\n              This website gathers code and additional material for the paper “Unbiased but Inflated Causal Effects” by Vincent Bagilet and Léo Zabrocki.\n              The website is under construction and the analysis is still at a preliminary stage.\n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2022-01-10T11:19:09+01:00"
    },
    {
      "path": "IV.html",
      "title": "Simulations IV",
      "description": "In this document, we run a simulation exercise to illustrate the existence of a trade-off between Omitted Variable Bias (OVB) and type M error in the context of an Instrumental Variable (IV) strategy.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModeling choices\nData generation\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\nF-statistic analysis\n\nA basic example\n\n\nbody {\ntext-align: justify}\n\n\n\nSummary and intuition\nIn the case of the IV, the OVB/type M trade-off is mediated by the ‘strength’ of the instrument considered. When the instrument does not explain a lot of the variation in the explanatory variable, the IV can still be successful in avoiding OVB but the power can low, potentially leading to high rates of type M error.\nAn illustrative example\nFor readability and to illustrate this loss in power, we consider an example setting. For this illustration we could consider a large variety of distribution parameter for the variables. We narrow this down to an example setting, considering an analysis of the impact of voter turnout on election results, instrumenting voter turnout with rainfall on the day of the election. Our point should stand in more general settings and the choice of values is mostly for illustration.\nA threat of confounders often arises when analyzing the link between voter turnout and election results. To estimate such an effect causally, one can consider exogeneous shocks to voter turnout such as rainfall. Some potential exclusion restriction problems have been highlighted in this instance but we abstract from them and simulate no exclusion restriction violations here.\nModeling choices\nFor simplicity, we consider several assumptions. These assumptions is not representative of the existing literature but the objective is only to calibrate our simulation with somehow realistic parameter values. Again, this illustration is very simplistic. The high level assumptions are:\nWe abstract from the panel dimension in this analysis and consider only one time period. This is could be considered as looking at the outcomes of a unique election.\nWe only consider the impact of rain on the day of the election.\nWe assume no correlation in rainfall between locations. This could be equivalent to considering only a set of remote locations.\nWe assume simplify the data generating process and thus do not add any exclusion restriction violations.\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nThe DGP for the vote share of let’s say the republican party in location \\(i\\), \\(Share_i\\), is defined as follows:\n\\[Share_{i} = \\alpha + \\beta Turnout_{i} + \\delta u_{i} + e^{(S)}_{i}\\]\nWhere \\(\\alpha\\) is a constant, \\(u\\) represents an unobserved variable and \\(e^{(S)} \\sim \\mathcal{N}(0, \\sigma_{e_S})\\) noise. \\(\\beta\\) is the parameter of interest. We call it ‘treatment effect’.\nThe DGP for the turnout data is as follows:\n\\[Turnout_{i} = \\gamma + \\lambda Rain_{i} + \\eta u_{i} + e^{(T)}_{i}\\]\nWhere \\(\\mu\\) is a constant, \\(Rain\\) is either a continuous variable (amount of rain in location \\(i\\) on the day of the election) or a dummy variable (whether it rained or not) and \\(e^{(T)} \\sim \\mathcal{N}(0, \\sigma_{e_T})\\) noise. We refer to \\(\\lambda\\) as “IV strength”.\nThe impact on voter turnout on election outcome (share of the republican party) is estimated using 2 Stages Least Squares.\nMore precisely, we set:\n\\(N\\) the number of observations\n\\(Rain \\sim \\mathcal{N}(0, \\sigma_{R}^{2})\\) or \\(Rain \\sim \\text{Bernoulli}(p_R)\\) the instrument\n\\(u \\sim \\mathcal{N}(0, \\sigma_{u}^{2})\\) the unobserved variable\n\\(e^{(S)} \\sim \\mathcal{N}(0, \\sigma_{e_S}^{2})\\)\n\\(e^{(T)} \\sim \\mathcal{N}(0, \\sigma_{e_T}^{2})\\)\nWe assume that \\(\\delta = -\\eta\\) for simplicity. There is no actual basis for that and we may change that in the future. The opposite sign is just to get an upward bias, which makes the comparison between OLS and IV easier since the bias and type M go in the same direction.\nIf one abstract from the name of the variable, they can notice that this setting is actually very general.\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\nNote that the parameter type_rain describes whether \\(Rain\\) is a random sample from a normal or Bernoulli distribution. The distributions of rainfall heights can be approximated with a gamma distribution. The Bernoulli distribution is used if one only consider the impact of rain or no rain on voter turnout. A normal distribution does not represent actual rainfall distributions but is added to run these simulations in other contexts than linking rainfall, voter turnout and election outcomes.\ntype_rain can take the values gamma, bernoulli or normal. param_rain represents either \\(\\sigma_R\\) if \\(Rain\\) is normal, \\(p_R\\) if it is Bernoulli or a vector of shape and scale parameters for the gamma distribution.\nNote that, for readability, in this document, we only display the chunks of code that may be important to understand the assumptions behind our simulations and the way we built our simulation. We do not display all the arguably “technical” code, in particular the one used to generate tables and graphs. All this code is however openly available on the GitHub of the project.\n\n\ngenerate_data_IV <- function(N,\n                             type_rain, #\"gamma\", \"normal\" or \"bernoulli\"\n                             param_rain,\n                             sigma_u,\n                             sigma_es,\n                             sigma_et,\n                             alpha,\n                             gamma,\n                             treatment_effect,\n                             iv_strength,\n                             ovb_intensity\n                             ) {\n  \n  if (type_rain == \"bernoulli\") {\n    rain_gen <- rbernoulli(N, param_rain[1])\n  } else if (type_rain == \"normal\") {\n    rain_gen <- rnorm(N, 0, param_rain[1])\n  } else if (type_rain == \"gamma\") {\n    rain_gen <- rgamma(N, shape = param_rain[1], scale = param_rain[2])\n  } else {\n    stop(\"type_rain must be either 'bernoulli', 'gamma' or 'normal'\")\n  }\n  \n  data <- tibble(id = 1:N) %>%\n    mutate(\n      rain = rain_gen,\n      u = rnorm(nrow(.), 0, sigma_u),\n      e_s = rnorm(nrow(.), 0, sigma_es),\n      e_t = rnorm(nrow(.), 0, sigma_et),\n      turnout = gamma + iv_strength*rain - ovb_intensity*u + e_t,\n      share = alpha + treatment_effect*turnout + ovb_intensity*u + e_s\n    )\n\n  return(data)\n}\n\n\n\nWe set baseline values for the parameters to emulate a somehow realistic observational study. We add the parameter value for delta separately as we will vary the value later and will reuse the vector baseline_param_IV.\nWe get “inspiration” for the values of parameters from Fujiwara et al. and Cooperman who replicates a work by Gomez et al..\nWe consider that:\nturnout and vote share are expressed in percent\nFujiwara et al. find that “The trends specifications suggest that 1 millimeter of rainfall decreases turnout by 0.05–0.07 percentage points” and Gomez et al. (and thus Cooperman) find “a county that receives one inch of rainfall on election day is likely to have approximately 1 percentage point lower voter turnout” which is equivalent to a 1mm increase in rainfall is associated with about a 0.04 percentage points decrease in voter turnout.\nFor simplicity in interpretation, when rainfall is not a dummy, it is expressed in centimeters. So, we will consider iv_strength in the range -0.1 and -1\nWe set the standard deviation of the omitted variable bias to be of the order of magnitude of the treatment effect\nWe calibrate the distribution parameters to fit a mix if information from table 1 from both Fujiwara et al. and Cooperman (converting the rainfall into centimeters):\nA gamma distribution represents well the distribution of rainfall. Gamma distribution can have two parameters a shape and a scale. The mean is \\(shape \\times scale\\) and the variance \\(shape \\times scale^{2}\\). The parameters of the distribution of rainfall are comparable in both papers (mean 2.4 and standard deviation 6.6). We solve the system of mean and variance for shape and scale and get 0.13 and 18.\nWe set the intercepts and standard deviations of the errors to produce turnouts and vote shares consistent with the papers. Voter turnout parameters are roughly similar in both papers (mean 58 sd 14) and mean and standard deviation of Republican vote share are given in Fujiwara et al. (mean 55.3 and sd 14.2). We may actually take values from a recent election (eg the last presidential election)\n\nWe thus consider the following parameters:\n\nN\ntype_rain\nparam_rain\nsigma_u\nsigma_es\nsigma_et\nalpha\ngamma\ntreatment_effect\n500\ngamma\n0.13, 18.00\n2\n9\n10\n112\n58\n-1\n\nHere is an example of data created with our data generating process:\n\nid\nrain\nu\ne_s\ne_t\nturnout\nshare\n1\n0.0000356\n0.1778264\n2.468880\n9.484628\n67.66244\n46.62862\n2\n0.0020568\n5.4261525\n2.633470\n-1.502734\n61.92239\n47.28493\n3\n0.0111119\n-4.5789390\n3.041930\n-3.183787\n50.23172\n69.38915\n4\n11.3806220\n2.1706406\n3.338365\n3.755300\n58.23563\n54.93210\n5\n0.0033441\n0.2817406\n-2.092464\n9.164472\n67.44454\n42.18125\n6\n0.0000001\n1.0723378\n-9.724307\n7.522362\n66.59470\n34.60866\n7\n5.6924024\n-0.1687575\n1.717059\n15.021074\n70.00611\n43.87970\n8\n0.6968785\n2.3940394\n1.094538\n15.637214\n75.68281\n35.01768\n9\n0.0023225\n-0.6255069\n6.719902\n-1.578709\n55.79462\n63.55079\n10\n0.0072424\n-1.1992162\n-2.275293\n-2.560434\n54.23673\n56.68719\n\nExploring the distribution of the data\nWe just quickly explore the distribution of the data for a baseline set of parameters. For this, we consider a mid range value for IV strength (-0.5).\n\n\n\nWe also check the standard deviation and means of the parameters:\n\nStatistic\nshare\nturnout\nrain\nMean\n55.08780\n56.71861\n2.342058\nStandard Deviation\n14.35317\n10.69820\n6.483874\n\nEstimation\nAfter generating the data, we can run an estimation. We want to compare the IV and the OLS for different IV strength values. Hence, we need to estimate both an IV and an OLS and return both set of outcomes of interest.\n\n\nestimate_IV <- function(data) {\n  reg_IV <- AER::ivreg(\n    data = data, \n    formula = share ~ turnout | rain\n    ) \n  \n  fstat_IV <- summary(\n    reg_IV, \n    diagnostics = TRUE\n  )$diagnostics[\"Weak instruments\", \"statistic\"]\n  \n  reg_IV <- reg_IV %>% \n    broom::tidy() %>%\n    mutate(\n      model = \"IV\",\n      fstat = fstat_IV\n    )\n  \n  reg_OLS <- lm(\n    data = data, \n    formula = share ~ turnout\n    ) %>% \n    broom::tidy() %>%\n    mutate(\n      model = \"OLS\",\n      fstat = NA\n    )\n  \n  reg_OLS_unbiased <- lm(\n    data = data, \n    formula = share ~ turnout + u\n    ) %>% \n    broom::tidy() %>%\n    mutate(\n      model = \"OLS unbiased\",\n      fstat = NA\n    )\n  \n  reg <- reg_IV %>% \n    rbind(reg_OLS) %>% \n    rbind(reg_OLS_unbiased) %>% \n    filter(term == \"turnout\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se, fstat, model) %>% \n  \n  return(reg)\n}\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_IV and estimate_IV. To do so we create the function compute_sim_IV. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the F-statistic for the IV, the true effect, the IV strength and the intensity of the OVB considered (ovb_intensity). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_sim_IV <- function(N,\n                           type_rain,\n                           param_rain,\n                           sigma_u,\n                           sigma_es,\n                           sigma_et,\n                           alpha,\n                           gamma,\n                           treatment_effect,\n                           iv_strength,\n                           ovb_intensity) {\n  generate_data_IV(\n    N = N,\n    type_rain = type_rain,\n    sigma_u = sigma_u,\n    param_rain = param_rain,\n    sigma_es = sigma_es,\n    sigma_et = sigma_et,\n    alpha = alpha,\n    gamma = gamma,\n    treatment_effect = treatment_effect,\n    iv_strength = iv_strength,\n    ovb_intensity = ovb_intensity\n  ) %>%\n    estimate_IV() %>%\n    mutate(\n      iv_strength = iv_strength,\n      ovb_intensity = ovb_intensity,\n      true_effect = treatment_effect\n    )\n} \n\n\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_IV function on each set of parameters. We thus create a table with all the values of the parameters we want to test, param_IV. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\n\nWe then run the simulations by mapping our compute_sim_IV function on param_IV.\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nWe notice that the OLS is always biased and that the IV is never biased. However, for limited IV strengths, the distribution of the estimates flattens. The smaller the IV strength, the most like it is to get an estimate away from the true value, even though the expected value remains equal to the true effect size. \nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta_{i}}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta_{IV}}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_simulations <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(ovb_intensity, iv_strength, model) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),\n      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      bias_all_median = median(estimate/true_effect, na.rm = TRUE),\n      median_fstat = mean(fstat, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_sim_IV <- summarise_simulations(sim_IV)\n# saveRDS(summary_sim_IV, here(\"Outputs/summary_sim_IV.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\nOf course, if one considers all estimates, as the IV is unbiased, this issue does not arise. For now, we consider the median because for very low IV strength, we get very extreme values. We need to investigate this further.\n\n\n\nF-statistic analysis\nWe then run some exploratory analysis to see the link between type M and F-stat (under construction).\n\n\n\nWe can notice that there is a correlation between what we call IV strength and the F statistic. When looking in more details at the distribution of F-stats, we can notice that, even for some IV strength, there are a a good chunk of significant estimates with F-stats greater thant the standard but arbitrary threshold of 10.\n\n\n\nWe cannot compute directly the bias of interest against the F-statistic because the F-statistic is not a parameter of the simulations and we do not control them, only the IV strength. To overcome this, we compute the median power by binned F-statistic. However, this is not correct as we end up comparing and pulling together simulations with different parameter values. We still display the graph:\n\n\n\nA basic example\nFor simplicity and for communication purposes, we considered an applied example. However, our results can also hold in general settings. We illustrate this, considering that most of the variables are distributed according to standard normal distributions. We keep the same DAG as it is the classic IV DAG. We also keep the same variable names but one should abstract from their meaning as they do not represent the same measure any longer.\nWe define the following parameters:\n\nN\ntype_rain\nparam_rain\nsigma_u\nsigma_es\nsigma_et\nalpha\ngamma\ntreatment_effect\n500\nnormal\n1\n1\n1\n1\n0\n0\n-1\n\nWe then run the whole analysis as before, with an OVB intensity of 0.4 so that it is not extremely large.\n\n\n\nThe results and the overall illustration are comparable with the political economy example.\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2022-01-10T11:19:32+01:00"
    },
    {
      "path": "Matching.html",
      "title": "Matching Simulations",
      "description": "\"In this document, we run a simulation exercise to illustrate the existence of a trade-off between Omitted Variable Bias (OVB) and type M error in the context of a matching procedure.\"\n",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nPackages Loading\nData Generating ProcedureGeneral Approach\nFunction to Generate the Data\nEDA for One Dataset\n\nOutcome Regression Analysis\nMatching ProcedureProposensity Score Function\nSimulations\n\n\n\nbody {\ntext-align: justify}\nIn this document, we show through simulations the Type M error - omitted variable bias trade-off for observational studies relying on matching methods. We create fake-data similar to those used for analyzing non-randomized labor training program. Should you have any questions or find coding errors, please do not hesitate to reach us at vincent.bagilet@columbia.edu and leo.zabrocki@psemail.eu.\nPackages Loading\nWe first load the required packages to set-up the simulations:\n\n\n# load required packages\nlibrary(knitr) # for creating the R Markdown document\nlibrary(tidyverse) # for data manipulation and visualization\nlibrary(MatchIt) # for matching analysis\nlibrary(lmtest) # for modifying regression standard errors\nlibrary(sandwich) # for robust and cluster robust standard errors\nlibrary(DT) # for displaying the data as tables\nlibrary(mediocrethemes) # vincent's custom ggplot2 theme\nlibrary(tictoc) # for measuring running time\nlibrary(beepr) # for making a sound when the code is done\nlibrary(here)\n\n# set ggplot theme\nset_mediocre_all(pal = \"coty\") #, background = TRUE) #for presentations\n\n\n\nData Generating Procedure\nGeneral Approach\nTo illustrate the the Type M error - OVB trade-off, we simulate fake-data from a non-randomized labor training program targeting young individuals:\nWe first create the units identifiers (id).\nWe then simulate 4 correlated binary covariates:\nThe true propensity score variable true_ps is drawn from \\(N(0.3, 0.1)\\) for control units and from \\(N(0.5, 0.12)\\) for treated units.\nOnce the the true propensity scores are created, we define the potential outcomes of each individual. Here, potential outcomes represent the income (in euros) of the individuals if they undertake the training program or not. The potential outcome without treatment adoption, Y(0), is simulated using the following equation:\ny_0 = 2000 * true_ps + rnorm(n(), mean = 300, sd = 200)\nWe finally simulate the potential outcomes when individuals benefit from the training program. The average treatment effect on the treated (ATT) was set to 100. The average treatment effect on the control (ATC) was set to 50.\nFunction to Generate the Data\nWe display below the code for the function generate_data_matching() which creates the dataset. Its single argument takes the desired sample size.\n\n\ngenerate_data_matching  <- function(sample_size) {\n  data <- tibble(id = 1:sample_size) %>%\n    mutate(\n      treatment = rbinom(n = sample_size, size = 1, prob = 0.25),\n      true_ps = ifelse(\n        treatment == 0,\n        rnorm(n(), mean = 0.3, sd = 0.1),\n        rnorm(n(), mean = 0.5, sd = 0.12)\n      ),\n      true_ps = case_when(true_ps > 1 ~ 1,\n                          true_ps < 0 ~ 0,\n                          true_ps >= 0 & true_ps <= 1 ~ true_ps),\n      # generate the potential outcomes\n      y_0 = 2000 * true_ps + rnorm(n(), mean = 300, sd = 200),\n      y_0 = y_0 %>% round(., 0),\n      y_1 = ifelse(treatment == 1,\n                   y_0 + 100,\n                   y_0 + 50),\n      # generate observed outcomes\n      y_obs = ifelse(treatment == 1, y_1, y_0) %>% round(., 0)\n    )\n  return(data)\n}\n\n\n\nEDA for One Dataset\nWe run one iteration of the function generate_data_matching() to explore the resulting data with 500 units:\n\n\n# run the function for a sample of 500 units\ndata <- generate_data_matching(500)\n\n# display the table\ndatatable(data)\n\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\",\"273\",\"274\",\"275\",\"276\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"286\",\"287\",\"288\",\"289\",\"290\",\"291\",\"292\",\"293\",\"294\",\"295\",\"296\",\"297\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"313\",\"314\",\"315\",\"316\",\"317\",\"318\",\"319\",\"320\",\"321\",\"322\",\"323\",\"324\",\"325\",\"326\",\"327\",\"328\",\"329\",\"330\",\"331\",\"332\",\"333\",\"334\",\"335\",\"336\",\"337\",\"338\",\"339\",\"340\",\"341\",\"342\",\"343\",\"344\",\"345\",\"346\",\"347\",\"348\",\"349\",\"350\",\"351\",\"352\",\"353\",\"354\",\"355\",\"356\",\"357\",\"358\",\"359\",\"360\",\"361\",\"362\",\"363\",\"364\",\"365\",\"366\",\"367\",\"368\",\"369\",\"370\",\"371\",\"372\",\"373\",\"374\",\"375\",\"376\",\"377\",\"378\",\"379\",\"380\",\"381\",\"382\",\"383\",\"384\",\"385\",\"386\",\"387\",\"388\",\"389\",\"390\",\"391\",\"392\",\"393\",\"394\",\"395\",\"396\",\"397\",\"398\",\"399\",\"400\",\"401\",\"402\",\"403\",\"404\",\"405\",\"406\",\"407\",\"408\",\"409\",\"410\",\"411\",\"412\",\"413\",\"414\",\"415\",\"416\",\"417\",\"418\",\"419\",\"420\",\"421\",\"422\",\"423\",\"424\",\"425\",\"426\",\"427\",\"428\",\"429\",\"430\",\"431\",\"432\",\"433\",\"434\",\"435\",\"436\",\"437\",\"438\",\"439\",\"440\",\"441\",\"442\",\"443\",\"444\",\"445\",\"446\",\"447\",\"448\",\"449\",\"450\",\"451\",\"452\",\"453\",\"454\",\"455\",\"456\",\"457\",\"458\",\"459\",\"460\",\"461\",\"462\",\"463\",\"464\",\"465\",\"466\",\"467\",\"468\",\"469\",\"470\",\"471\",\"472\",\"473\",\"474\",\"475\",\"476\",\"477\",\"478\",\"479\",\"480\",\"481\",\"482\",\"483\",\"484\",\"485\",\"486\",\"487\",\"488\",\"489\",\"490\",\"491\",\"492\",\"493\",\"494\",\"495\",\"496\",\"497\",\"498\",\"499\",\"500\"],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500],[1,1,0,0,0,0,0,1,1,1,0,0,1,0,1,0,0,0,1,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,1,1,0,1,0,0,1,0,1,1,0,0,0,1,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,0,1,1,1,0,1,0,0,0,0,1,0,0,0,1,1,1,0,0,0,1,1,0,1,0,0,1,0,0,1,1,1,1,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,1,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,1,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,1,0,0,0],[0.446302278305615,0.26532971852034,0.453471348118578,0.256035775824876,0.368551990802491,0.174960310670515,0.344177636700269,0.340917797231222,0.443878184117328,0.602908362553207,0.330483218475774,0.102660062281967,0.499170879953014,0.338381260101235,0.648972358390031,0.381344249778949,0.257418384404188,0.410025038864714,0.597456171434552,0.207439740649298,0.567074986559291,0.250781361861338,0.140327648806343,0.667952150861886,0.343849261509886,0.309870100055356,0.259960296874708,0.311612774777685,0.444302578783119,0.325037944105731,0.425146140896271,0.370877870747385,0.321556612464606,0.603059925197208,0.439878827421815,0.335964080184906,0.316138530774485,0.213143385795423,0.295658489437205,0.34433215148595,0.7429120658219,0.389490172667769,0.379552666417903,0.327009786749313,0.36411271119039,0.226089407047167,0.128958735675857,0.421249931357695,0.430141860524027,0.264261193250988,0.386930697562632,0.24745432375409,0.191475733250746,0.278606252580202,0.254464571654015,0.267825480277595,0.465413124068864,0.342441572376279,0.569405875052524,0.229497537154964,0.234054335560298,0.378762060327377,0.449440902594194,0.332828555327291,0.245482113879203,0.471143219057004,0.23893651696877,0.0365202858219479,0.295741282038657,0.669861540609877,0.363956177830699,0.381537892223065,0.274559485921961,0.211595329434071,0.472204897742848,0.213108773452504,0.18379545983868,0.256405225012561,0.268106026106544,0.278778682509981,0.452491893885548,0.314171168170536,0.254289893348721,0.183421743678371,0.168807277789351,0.250386020261741,0.160584148257774,0.31050487273692,0.0567325696100567,0.343420972011269,0.260194499216014,0.251792303851671,0.272638693134209,0.477868807029799,0.402520387434876,0.39086773225582,0.153142258369298,0.409827245236563,0.393148831078003,0.376840091187185,0.267319691472114,0.15646157044219,0.193417391270089,0.431926976168285,0.307013459588669,0.465332224825923,0.0432277341088959,0.501986981942359,0.507449698658859,0.515172540857665,0.399389242195658,0.454394082914651,0.532627173312116,0.103418480556706,0.395777285299114,0.238298951145603,0.371681432568686,0.733543660707965,0.538952570371795,0.418067240154432,0.362568267403518,0.196476096680195,0.303898139610114,0.226763133023028,0.245592745644382,0.515985335977033,0.342543659528683,0.273421232043679,0.455848571151743,0.34933530659047,0.224599688320224,0.280248450457176,0.421055803932273,0.244818045690253,0.195610536795843,0.367415913300007,0.466084789009489,0.539818776193977,0.355399499381155,0.343743667702786,0.46017261914106,0.246540517497603,0.400639922080989,0.493976688424577,0.594235947471043,0.23072667250234,0.381663782904714,0.204185346635774,0.285249266035315,0.236500467964515,0.171774667092168,0.431310384650738,0.0345324863764161,0.252079582967512,0.420651496489975,0.639318451287988,0.165666168563174,0.475534571662939,0.33454272256123,0.455017281112469,0.290995035326045,0.693519471395949,0.151445323956605,0.399396958839335,0.404822087208616,0.3539047405878,0.296685354966804,0.309028620883485,0.273542267226297,0.514693368919637,0.317078656112154,0.220914413935733,0.567507263385432,0.63784500262525,0.257082950245206,0.576359768383357,0.258102309116904,0.385683017890126,0.255811813183763,0.696729139163423,0.651327225679157,0.218869668061909,0.418767478066537,0.0902559834904828,0.322487041285785,0.297831044374611,0.318984968587262,0.494010528137739,0.264039112413813,0.307521767284136,0.350900986941856,0.125966342581881,0.289506985795792,0.16515552488218,0.300426590851466,0.27171077070982,0.140722585880282,0.38499248237469,0.329375660715212,0.372009250586888,0.440035646814678,0.231996701533632,0.227704931357725,0.183340016917696,0.305635812814203,0.661364063419055,0.415171907283412,0.368396080440473,0.209003329657952,0.577508799234945,0.281436540726537,0.290347303133899,0.361183532292029,0.527116692934911,0.433937208177112,0.479664386365964,0.326718495225668,0.362054646310053,0.469813481888827,0.334407887945338,0.606943481188151,0.653695499207214,0.311485606275337,0.378072157128001,0.274372405162676,0.458341888878496,0.531952723430032,0.409739155761811,0.217307368195458,0.192441235174341,0.375492645989865,0.61296082487492,0.230018348338047,0.129147237564438,0.244130556166156,0.632323269138573,0.332886018972625,0.431613115543109,0.30044943084431,0.30381878596447,0.434260047350093,0.442017434308415,0.412317931795663,0.396993467497989,0.458308310353948,0.551700995798912,0.316614473105766,0.380063598550639,0.199116125868299,0.395464466940049,0.244797191314646,0.477535956299482,0.613837851615753,0.256390681432566,0.194279952613833,0.365198533238006,0.245767953717487,0.440214023531497,0.234589553656528,0.441271378409505,0.311138719145661,0.306920438435969,0.389015243729902,0.481435741797897,0.477073454272185,0.753594717763694,0.321130428763587,0.516645999739435,0.161348652509395,0.120558735508716,0.343274973521812,0.210291265048306,0.232993178804892,0.51972447898061,0.349919774768923,0.437559606007195,0.389455917970675,0.354626035339732,0.512139354684035,0.564598713308971,0.245524608517065,0.516731298680924,0.493928866931,0.37101578271031,0.294187773746429,0.144389942529664,0.291890437777473,0.407502390443796,0.323128262697655,0.267819303936525,0.392066795858615,0.311516807858146,0.358673499814596,0.427178842417908,0.163131324134195,0.360189236044023,0.136238431529264,0.462651776395541,0.470742563862288,0.209847817575518,0.361353450309193,0.0598033845992755,0.400808287671838,0.640997063952059,0.197032735458567,0.586865267894815,0.650881988813872,0.665161339166414,0.366999248835544,0.583698525085157,0.247054198495559,0.263115952926237,0.159095818529229,0.309136880174028,0.516765501143079,0.260225342926449,0.218415218752505,0.343503239919828,0.490149823740856,0.41842536858952,0.455520397173059,0.274537019082212,0.33300131819856,0.367515132022786,0.52564922433368,0.627998394644981,0.314002364869967,0.356241446312028,0.266062602026312,0.173908916139269,0.47797759819876,0.290534220779342,0.11109584361829,0.341554318409757,0.609431309515455,0.478115971308295,0.416130316818397,0.440576849205402,0.320896305961508,0.417411129215084,0.644433296902663,0.408327441266701,0.289120920268983,0.469048491892102,0.265185633437307,0.125652581548047,0.265719993588911,0.406723338000307,0.306366149016957,0.546444305179172,0.291268670031673,0.402381271076801,0.289011457351196,0.251646873815886,0.581025248420343,0.357085561940156,0.128823921058137,0.563022826525581,0.407280528043522,0.303059628751184,0.479228233507729,0.203503537626313,0.453336663127712,0.442706318534883,0.432570385647787,0.33485439131703,0.486591900939328,0.161731796770952,0.280066862452292,0.126471236790685,0.418689368813724,0.250951402673791,0.205411419733486,0.356567690816183,0.317418738637429,0.295512586724247,0.359629045828002,0.458230269593163,0.275908723594723,0.267161425493089,0.251031203860327,0.365104134092463,0.270360634587553,0.393251194886005,0.227647141101983,0.215322987755406,0.276595787574279,0.762835915606094,0.491052368485499,0.22187124714143,0.114979258210213,0.440229739536414,0.441838938544244,0.309970823684926,0.257764299930186,0.374721053067698,0.220228055557394,0.627821487995289,0.541593305851592,0.298371576713173,0.0278578434554159,0.457568353015072,0.224925806473412,0.299728698590718,0.34161861825775,0.547224082679556,0.212944190022269,0.688636073479496,0.155176273654014,0.26711942378714,0.224077966158082,0.14193780325137,0.478421757133456,0.33280405820401,0.427857048465954,0.417443968830701,0.56098477742509,0.252495299321636,0.288129654295077,0.20971861155138,0.239783200718385,0.37131367968611,0.441732507017516,0.182031603167191,0.175891949164401,0.426735119256172,0.584879497391085,0.339483150603778,0.261464994013758,0.503120239586573,0.354054942498325,0.331587760316051,0.282862354607594,0.573004037259917,0.600340988981298,0.591508196802577,0.394710121772612,0.261167288315023,0.278305611396878,0.323232268549219,0.404438089962099,0.411831231256972,0.351562771008862,0.295011561221561,0.71410322954427,0.319937822020276,0.527846454391112,0.554835554397704,0.281412074171869,0.264860134075425,0.45842482243606,0.0853954009247449,0.277691991481681,0.31311285500693,0.258929019460988,0.295331424434392,0.276858262529871,0.608136241635347,0.315900793881538,0.371674846220907,0.508236734833996,0.251057896089238,0.247909494663219,0.255605776577885,0.531455842918533,0.281176297446119,0.410725539156234,0.323881353561257,0.27254786502,0.290226225488038,0.234398648008886,0.735781308087297,0.27615122298732,0.281096296435718,0.240041475937722,0.209607532041332,0.379233807156692,0.539208463489858,0.38384508122816,0.379547731705589,0.147590920463807,0.27230207873622,0.31779305625033,0.590441844107338,0.332072061350383,0.317870251897166,0.301841580946317,0.2906053727661,0.1497266111871,0.289950132134281,0.245378510436452,0.351736966346381,0.349436975583904,0.35486353082981,0.0680622927667491,0.363264164156417,0.340095103996133,0.149109265351523,0.181682693258699,0.219382848348743],[1040,783,1145,834,1107,976,1076,987,1100,1582,1053,468,1450,806,1116,1054,993,1546,1998,663,1307,459,590,1655,889,1030,779,844,1238,868,1414,1092,641,1682,967,1205,1394,557,1036,799,1360,1145,961,620,1168,201,290,994,1272,507,1339,844,697,961,1153,1232,954,1135,1340,576,629,821,1188,1012,696,1181,570,57,895,1490,1335,1060,1195,992,1122,670,588,608,904,684,1355,802,686,770,864,1032,720,1111,185,1035,1031,1389,904,1375,1025,821,286,932,1024,992,918,548,369,1032,935,1330,330,1371,1628,1152,979,1401,1332,924,1194,668,957,1769,1282,875,1245,1304,861,574,608,1301,1008,862,894,952,670,851,1229,815,403,1119,1156,1615,1084,828,1260,1027,1097,1535,1768,545,867,611,820,893,433,1197,349,652,1157,1378,748,1040,1020,1277,534,1736,761,1326,1134,1194,1001,965,886,1359,754,561,1366,1732,931,1411,715,940,649,1559,1531,500,1320,164,968,1124,705,1237,747,761,798,461,1049,640,1266,1085,560,826,828,992,910,742,744,550,825,1780,1144,731,623,1460,881,911,1211,1398,1286,946,1075,973,1308,982,1323,1155,883,1079,811,925,1537,1151,533,849,985,1369,992,284,796,2044,788,1028,866,706,1125,1522,744,959,1269,1351,980,819,803,1341,546,1500,1500,909,734,1171,1318,1500,859,659,1005,776,1027,952,1380,1650,1391,1332,910,540,1076,759,942,1146,1140,1146,964,942,1539,1529,673,1458,1344,1195,556,765,915,1190,1012,1089,1081,847,1047,1172,364,1000,589,1311,989,916,925,660,1312,1056,727,1490,2001,1905,1405,1655,858,922,314,992,1315,511,597,810,1354,1006,1255,944,615,461,1164,1834,1386,1330,553,798,1436,1159,887,506,1385,1563,1072,1310,1150,1214,1264,1115,615,1429,680,229,841,1090,1006,1287,1197,857,827,848,1430,1216,640,1898,1322,827,1196,1027,1228,1064,1204,933,1123,609,813,563,1359,715,490,1488,1057,1053,482,1020,703,757,690,1085,491,1135,727,582,835,1537,958,401,340,1360,1133,1099,882,1135,622,1509,1375,1092,323,1176,644,1275,1041,1199,512,1496,239,631,1279,803,1312,1044,892,970,1536,845,835,829,647,1223,1048,827,586,1151,1618,859,775,1133,833,1014,529,1199,1567,1466,854,755,730,870,1163,1053,1063,805,1769,1054,1565,1353,1014,1072,1292,640,1007,740,620,888,770,1531,717,1180,1725,911,483,850,895,724,775,652,968,811,1005,1577,964,928,864,998,1111,1320,1122,1254,289,838,996,1357,780,1018,1166,962,534,627,1236,1014,1054,645,389,1259,781,346,605,353],[1140,883,1195,884,1157,1026,1126,1087,1200,1682,1103,518,1550,856,1216,1104,1043,1596,2098,713,1407,509,640,1755,939,1080,829,944,1288,918,1464,1142,691,1782,1017,1255,1444,607,1086,899,1460,1195,1011,670,1218,251,340,1044,1322,557,1389,894,747,1011,1203,1282,1054,1185,1440,626,679,921,1238,1062,746,1281,620,107,945,1590,1385,1110,1245,1042,1222,720,638,658,954,734,1405,852,736,820,914,1082,770,1161,235,1085,1081,1439,954,1425,1075,871,336,1032,1074,1042,968,598,419,1082,985,1380,380,1421,1678,1252,1029,1501,1382,974,1294,718,1007,1869,1382,925,1295,1354,911,624,658,1401,1058,912,944,1002,720,901,1329,865,453,1169,1256,1715,1134,878,1360,1077,1147,1635,1868,595,967,661,870,943,483,1247,399,702,1207,1478,798,1140,1070,1377,584,1836,811,1376,1184,1244,1051,1015,936,1459,804,611,1416,1832,981,1511,765,990,699,1659,1631,550,1370,214,1018,1224,755,1337,797,811,848,511,1099,690,1316,1135,610,876,878,1042,1010,792,794,600,875,1880,1194,781,673,1560,931,961,1311,1498,1336,1046,1125,1023,1408,1032,1423,1255,933,1129,861,1025,1637,1201,583,899,1035,1469,1042,334,846,2144,838,1078,916,756,1225,1622,794,1009,1319,1451,1030,869,853,1391,596,1550,1600,959,784,1271,1368,1600,909,709,1055,876,1077,1002,1480,1750,1441,1382,960,590,1126,809,992,1246,1190,1196,1014,992,1589,1629,723,1558,1444,1295,606,815,965,1240,1062,1139,1131,897,1097,1272,414,1050,639,1411,1089,966,975,710,1362,1156,777,1590,2101,2005,1455,1755,908,972,364,1042,1415,561,647,860,1454,1106,1355,994,665,511,1264,1934,1436,1430,603,848,1536,1209,937,606,1485,1663,1172,1410,1200,1264,1364,1165,665,1529,730,279,891,1140,1056,1337,1247,907,877,898,1530,1266,690,1998,1372,877,1296,1077,1278,1114,1304,983,1223,659,863,613,1409,765,540,1538,1107,1103,532,1120,753,807,740,1135,541,1185,777,632,885,1637,1058,451,390,1410,1233,1149,932,1185,672,1609,1475,1142,373,1276,694,1325,1091,1299,562,1596,289,681,1329,853,1362,1094,942,1070,1636,895,885,879,697,1273,1148,927,636,1201,1718,909,825,1183,933,1064,579,1299,1667,1566,904,805,780,920,1213,1103,1113,855,1869,1104,1665,1453,1064,1122,1342,690,1057,790,670,938,820,1631,767,1230,1825,961,533,900,945,774,825,702,1018,861,1055,1677,1014,978,914,1048,1161,1420,1172,1354,339,888,1046,1457,830,1068,1266,1012,584,677,1286,1064,1154,695,439,1359,881,396,655,403],[1140,883,1145,834,1107,976,1076,1087,1200,1682,1053,468,1550,806,1216,1054,993,1546,2098,663,1407,459,590,1755,889,1030,779,944,1238,868,1414,1092,641,1782,967,1205,1394,557,1036,899,1460,1145,961,620,1168,201,290,994,1272,507,1339,844,697,961,1153,1232,1054,1135,1440,576,629,921,1188,1012,696,1281,570,57,895,1590,1335,1060,1195,992,1222,670,588,608,904,684,1355,802,686,770,864,1032,720,1111,185,1035,1031,1389,904,1375,1025,821,286,1032,1024,992,918,548,369,1032,935,1330,330,1371,1628,1252,979,1501,1332,924,1294,668,957,1869,1382,875,1245,1304,861,574,608,1401,1008,862,894,952,670,851,1329,815,403,1119,1256,1715,1084,828,1360,1027,1097,1635,1868,545,967,611,820,893,433,1197,349,652,1157,1478,748,1140,1020,1377,534,1836,761,1326,1134,1194,1001,965,886,1459,754,561,1366,1832,931,1511,715,940,649,1659,1631,500,1320,164,968,1224,705,1337,747,761,798,461,1049,640,1266,1085,560,826,828,992,1010,742,744,550,825,1880,1144,731,623,1560,881,911,1311,1498,1286,1046,1075,973,1408,982,1423,1255,883,1079,811,1025,1637,1151,533,849,985,1469,992,284,796,2144,788,1028,866,706,1225,1622,744,959,1269,1451,980,819,803,1341,546,1500,1600,909,734,1271,1318,1600,859,659,1005,876,1027,952,1480,1750,1391,1332,910,540,1076,759,942,1246,1140,1146,964,942,1539,1629,673,1558,1444,1295,556,765,915,1190,1012,1089,1081,847,1047,1272,364,1000,589,1411,1089,916,925,660,1312,1156,727,1590,2101,2005,1405,1755,858,922,314,992,1415,511,597,810,1454,1106,1355,944,615,461,1264,1934,1386,1430,553,798,1536,1159,887,606,1485,1663,1172,1410,1150,1214,1364,1115,615,1529,680,229,841,1090,1006,1287,1197,857,827,848,1530,1216,640,1998,1322,827,1296,1027,1228,1064,1304,933,1223,609,813,563,1359,715,490,1488,1057,1053,482,1120,703,757,690,1085,491,1135,727,582,835,1637,1058,401,340,1360,1233,1099,882,1135,622,1609,1475,1092,323,1276,644,1275,1041,1299,512,1596,239,631,1279,803,1312,1044,892,1070,1636,845,835,829,647,1223,1148,927,586,1151,1718,859,775,1133,933,1014,529,1299,1667,1566,854,755,730,870,1163,1053,1063,805,1869,1054,1665,1453,1014,1072,1292,640,1007,740,620,888,770,1631,717,1180,1825,911,483,850,895,724,775,652,968,811,1005,1677,964,928,864,998,1111,1420,1122,1354,289,838,996,1457,780,1018,1266,962,534,627,1236,1014,1154,645,389,1359,881,346,605,353]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>id<\\/th>\\n      <th>treatment<\\/th>\\n      <th>true_ps<\\/th>\\n      <th>y_0<\\/th>\\n      <th>y_1<\\/th>\\n      <th>y_obs<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nAbout 26.2% of units are treated. We display below the true propensity score distributions by treatment status:\n\n\n\nWe can finally see how the observed revenue is distributed across the two groups:\n\n\n\nAnd we can check whether the ATT and ATC were correctly simulated. The ATT is computed such as:\n\n\n# checking att\nmean(data$y_1[data$treatment==1]) - mean(data$y_0[data$treatment==1])\n\n\n[1] 100\n\nand the ATC:\n\n\n# checking atc\nmean(data$y_1[data$treatment==0]) - mean(data$y_0[data$treatment==0])\n\n\n[1] 50\n\nThe data have been simulated as we wanted.\nOutcome Regression Analysis\nWhat would happen if we analyze our simulated datasets with a simple outcome regression model? Would we recover the true answer?\nWe first create a regression function to run a simple regression model where we simply regress the observed income on the treatment indicator:\n\n\noutcome_regression <- function(data) {\n  data %>%\n    lm(\n      y_obs ~ treatment,\n      data = .\n    ) %>%\n    broom::tidy(., conf.int = TRUE) %>%\n    filter(term == \"treatment\") %>%\n    select(estimate, p.value, conf.low, conf.high)\n}\n\n\n\nWe then simulate 1000 datasets of 500 units and run the regression model:\n\n\n# first simulate simulation id\ndata_simulations <- tibble(sim_id = 1:1000) %>%\n# then simulate data\n  mutate(data = map(sim_id, ~ generate_data_matching(500))) %>%\n# finally run the reg analysis\n  mutate(results = map(data, ~ outcome_regression(.)))\n\n# unnest the results\ndata_simulations <- data_simulations %>%\n  select(-data) %>%\n  unnest(results)\n\n\n\nWe plot the distribution of estimates:\n\n\ndata_simulations %>%\n  ggplot(., aes(x = estimate)) +\n  geom_density(colour = NA) +\n  geom_vline(xintercept = mean(data_simulations$estimate)) +\n  geom_vline(xintercept = 100, colour = \"#EAA95C\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +\n  xlab(\"Revenue (in euros)\") + ylab(\"\") +\n  labs(fill = 'Status:') +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank())\n\n\n\n\nThe average of estimates is equal to 501.\nMatching Procedure\nWe now implement a simple matching where:\nWe implement below a propensity score matching procedure where:\neach treated is matched to its most similar control unit. This is a 1:1 nearest neighbor matching without replacement.\nthe distance metric used for the matching is the propensity score.\nTo see how the bias, statistical power and type M error evolve, we vary the matching distance (the caliper), which is expressed in standard deviation of the propensity score distribution.\nProposensity Score Function\nWe display the below the code for the function ps_function() which runs the matching procedure. It takes to inputs: (i) a dataset and (ii) the value of the caliper.\n\n\n# propensity score analysis function\nps_function <- function(data, caliper_value) {\n  matching_results <- matchit(\n    treatment ~ id,\n    distance = data$true_ps,\n    caliper = caliper_value,\n    data = data\n  )\n  \n  data_matched <- match.data(matching_results)\n  \n  proportion_matched <- sum(data_matched$treatment)/sum(data$treatment)*100\n  \n  true_effect <-\n    mean(data_matched$y_1[data_matched$treatment == 1]) - mean(data_matched$y_0[data_matched$treatment == 1])\n  \n  model_fit <- lm(\n    y_obs ~ treatment,\n    data = data_matched,\n    weights = weights\n  )\n  \n  ps_att <- broom::tidy(coeftest(model_fit, vcov. = vcovCL, cluster = ~ subclass),\n                        conf.int = TRUE) %>%\n    filter(term == \"treatment\") %>%\n    select(term, estimate, p.value, conf.low, conf.high)\n  \n  return(bind_cols(ps_att, proportion_matched = proportion_matched, true_effect = true_effect))\n}\n\n\n\nWe run the function on the data we previously created:\n\n\n# testing the function\nps_function(data, caliper = 0.5)\n\n\n\nThe function returns the estimate for the ATT, the associated \\(p\\)-value and 95% confidence interval, the portion of matched treated unit and the true value of the ATT.\nSimulations\nWe implement Monte-Carlo simulatiuons for a sample size of 300 units and differents values of the caliper:\n\n\ndata_simulations <- tibble(sim_id = 1:300) %>%\n  # then simulate data\n  mutate(data = map(sim_id, ~ generate_data_matching(300))) %>%\n  # generate caliper\n  crossing(caliper = c(seq(from = 1, to = 100, by = 1)/100)) %>%\n  # finally run the matching analysis\n  mutate(results = map2(data, caliper, ~ ps_function(.x, .y)))\n\n# unnest results\nsimulations_matching <- data_simulations %>% \n  select(-data) %>%\n  unnest(results)\n\n# saveRDS(simulations_matching, here(\"Outputs/simulations_matching.RDS\"))\n\n\n\nOnce the simulations have been run, we compute the summary statistics using the summarise_simulations() function:\n\n\nsimulations_matching <- readRDS(here(\"Outputs/simulations_matching.RDS\"))\n\nsummarise_simulations_matching <- function(data) {\n  data %>%\n    mutate(significant = (p.value <= 0.05)) %>% \n    group_by(caliper) %>%\n    summarise(\n      proportion_matched = mean(proportion_matched),\n      power = mean(significant, na.rm = TRUE)*100, \n      bias_sign = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\n\n\nWe apply the function to data_simulations:\n\n\nsummary_simulations_matching <- summarise_simulations_matching(simulations_matching)\n\n\n\nAnd plot the results:\n\n\n\n\n\n\n",
      "last_modified": "2022-01-10T11:20:01+01:00"
    },
    {
      "path": "panoptic-issues.html",
      "title": "Simulations for panoptic issues",
      "description": "In this document, we run a simulation exercise to illustrate panoptic type M issues arising in causal and non-causal identification studies.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModeling choices\nData generation\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the results (proportion treated units)Quick exploration\nComputing bias and type M\nTHE graph\n\nVarying the number of clusters\n\n\nbody {\ntext-align: justify}\n\n\n\nSummary and intuition\nPower and thus type M error can also be affected by aspects that are not proper to one particular identification strategy. In this document we investigate the impact of different factors:\nThe proportion of unit treated (at number of observations constant). We all know that power is maximized when the proportion of units treated is 0.5. The further away we are from this value, the smaller the power and the larger the exaggeration ratio. (I am not certain, I need to check the formula but) The intuition is that a limited number of treated, or control, limits the variation used to estimate the effect.\nLimiting the variation in y (small number of counts in y)\nThe number of clusters. Considering a small number of clusters is to some extent comparable to considering a small number of units and thus partly acts as limiting the number of observations, thus decreasing power and increasing the exaggeration ratio.\nAn illustrative example\nFor readability and to illustrate this loss in power, we consider an example setting. For this illustration we could consider a large variety of distribution parameter for the variables. We narrow this down to an example setting, considering an analysis of the impact of voter turnout on election results, instrumenting voter turnout with rainfall on the day of the election. Our point should stand in more general settings and the choice of values is mostly for illustration.\nModeling choices\nFor simplicity, we consider several assumptions. These assumptions is not representative of the existing literature but the objective is only to calibrate our simulation with somehow realistic parameter values. Again, this illustration is very simplistic. The high level assumptions are:\nFor now, we consider only standard normal distributions\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nWe thus assume that the outcome \\(y\\) for individual \\(i\\) is defined as follows:\n\\[y_{i} = \\alpha + \\beta T_{i} + \\delta u_{i} + \\epsilon_{i}\\]\nWhere \\(\\alpha\\) is a constant, \\(T_{i}\\) a dummy equal to 1 if individual \\(i\\) is treated and if they are not treated, \\(\\beta\\) the treatment effect size, \\(u\\) an unobserved variable, \\(\\delta\\) the intensity of the OVB and \\(\\epsilon\\) an error term.\nMore precisely, we set:\n\\(N\\) the number of observations \n\\(T_i = \\mathbb{1}\\{i\\text{ is treated}\\}\\) the treatment dummy. \\(p_T\\) represents the proportion of treated units.\n\\(u \\sim \\mathcal{N}(0, \\sigma_{u}^{2})\\) the unobserved variable\n\\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})\\) the error term\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\nNote that, for readability, in this document, we only display the chunks of code that may be important to understand the assumptions behind our simulations and the way we built our simulation. We do not display all the arguably “technical” code, in particular the one used to generate tables and graphs. All this code is however openly available on the GitHub of the project.\n\n\ngenerate_data_panoptic <- function(N,\n                                   p_treat,\n                                   sigma_u,\n                                   sigma_e,\n                                   alpha,\n                                   treatment_effect, #beta\n                                   ovb_intensity #delta\n                                   ) {\n  tibble(id = 1:N) %>%\n    mutate(\n      # treated = rbernoulli(N, p_treat),\n      treated = (id %in% sample(1:N, p_treat*N)),\n      u = rnorm(nrow(.), 0, sigma_u),\n      e = rnorm(nrow(.), 0, sigma_e),\n      y = alpha + treatment_effect*treated + ovb_intensity*u + e\n    )\n}\n\n\n\n\nFor now, we consider very simple baseline parameters, mostly standard normal distributions:\n\nN\np_treat\nsigma_u\nsigma_e\nalpha\ntreatment_effect\novb_intensity\n500\n0.5\n1\n1\n1\n0.4\n1\n\nHere is an example of data created with our data generating process:\n\nid\ntreated\nu\ne\ny\n1\nFALSE\n0.8037496\n0.4011674\n2.2049170\n2\nFALSE\n0.0525026\n1.3331472\n2.3856498\n3\nFALSE\n-0.4579376\n-0.5030379\n0.0390245\n4\nFALSE\n-2.2833305\n-0.0668204\n-1.3501509\n5\nTRUE\n-0.3174872\n-0.9441623\n0.1383504\n6\nTRUE\n0.4912762\n-0.1986278\n1.6926484\n7\nTRUE\n-0.1044091\n-0.3623130\n0.9332779\n8\nTRUE\n-1.2257641\n0.8430335\n1.0172693\n9\nFALSE\n-1.4918630\n-0.6269970\n-1.1188600\n10\nTRUE\n-0.4717193\n-0.4784194\n0.4498613\n\nEstimation\nAfter generating the data, we can run an estimation. We want to be able to run the estimation for different numbers of clusters. We create artificial clusters, based on the individual identification numbers id. When we do not wish to cluster the standard errors, we simply set the cluster variable to be equal to the id so that the “clustering” is at the individual level. Note that we use the function lm_robust from the estimatr package to compute this clustering.\n\n\nestimate_panoptic <- function(data, n_clusters = NA) {\n  n_clusters_mod <- ifelse(is.na(n_clusters), nrow(data), n_clusters)\n  \n  data %>%\n    mutate(cluster = cut_number(id, n_clusters_mod, labels = FALSE)) %>%\n    lm(data = ., formula = y ~ treated) %>%\n    coeftest(vcov = vcovCL, cluster = ~cluster) %>% \n    broom::tidy() %>%\n    filter(term == \"treatedTRUE\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se) %>%\n    mutate(n_clusters = n_clusters)\n}\n\n# estimate_panoptic_1 <- function(data, n_clusters = NA) {\n#   n_clusters_mod <- ifelse(is.na(n_clusters), nrow(data), n_clusters)\n#   \n#   data %>%\n#     mutate(cluster = cut_number(id, n_clusters_mod, labels = FALSE)) %>%\n#     estimatr::lm_robust(\n#       data = .,\n#       formula = y ~ treated,\n#       clusters = cluster\n#     ) %>%\n#     broom::tidy() %>%\n#     as_tibble() %>% \n#     filter(term == \"treatedTRUE\") %>%\n#     rename(p_value = p.value, se = std.error) %>%\n#     select(estimate, p_value, se) %>%\n#     mutate(n_clusters = n_clusters)\n# }\n\n# estimate_panoptic_2 <- function(data, n_clusters = NA) {\n# \n#   if (is.na(n_clusters)) {\n#     reg <- lm(\n#       data = data,\n#       formula = y ~ treated\n#     )\n#   } else {\n#     reg <- data %>%\n#       mutate(cluster = cut_number(id, n_clusters, labels = FALSE)) %>%\n#       estimatr::lm_robust(\n#         data = .,\n#         formula = y ~ treated,\n#         clusters = cluster\n#       )\n#   }\n# \n#   reg %>%\n#     broom::tidy() %>%\n#     as_tibble() %>% \n#     filter(term == \"treatedTRUE\") %>%\n#     rename(p_value = p.value, se = std.error) %>%\n#     select(estimate, p_value, se) %>%\n#     mutate(n_clusters = n_clusters)\n# }\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_panoptic and estimate_panoptic. To do so we create the function compute_sim_panoptic. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect. Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_sim_panoptic <- function(N,\n                                 p_treat,\n                                 sigma_u,\n                                 sigma_e,\n                                 alpha,\n                                 treatment_effect,\n                                 ovb_intensity,\n                                 n_clusters = NA) {\n  generate_data_panoptic(\n    N = N,\n    p_treat = p_treat,\n    sigma_u = sigma_u,\n    sigma_e = sigma_e,\n    alpha = alpha,\n    treatment_effect = treatment_effect,\n    ovb_intensity = ovb_intensity\n  ) %>%\n  estimate_panoptic(n_clusters = n_clusters) %>%\n  mutate(\n    true_effect = treatment_effect,\n    p_treat = p_treat\n  ) \n}\n\n\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_p_treat function on each set of parameters. We thus create a table with all the values of the parameters we want to test, param_p_treat. First, we only vary the proportion of unit treated. We will vary the number of clusters in a second analysis.\nNote that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\n\nWe then run the simulations by mapping our compute_sim_p_treat function on param_p_treat.\n\n\n\nAnalysis of the results (proportion treated units)\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_sim_panoptic <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>%\n    group_by(p_treat, n_clusters) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100,\n      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),\n      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      bias_all_median = median(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>%\n    ungroup()\n}\n\nsummary_sim_p_treat <- summarise_sim_panoptic(sim_p_treat)\n\n\n\nTHE graph\nTo analyze our results, we build a unique and simple graph:\n\n\n\nVarying the number of clusters\nWe then reproduce a similar type of analysis and graph but varying the number of clusters.\n\n6.147 sec elapsed\n\n\n\n\n\n",
      "last_modified": "2022-01-10T11:20:15+01:00"
    },
    {
      "path": "RDD.html",
      "title": "Simulations RDD",
      "description": "In this document, we run a simulation exercise to illustrate how using a Regression Discontinuity Design (RDD) to avoid confounders may create type M error.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModeling choices\nData generation\nDefining the bandwidth\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\n\n\n\nbody {\ntext-align: justify}\nSummary and intuition\nIn the case of the RDD, the confounding/type M error trade-off is mediated by the size of the bandwidth considered in the analysis. The underlying idea is that the smaller the bandwidth, the more comparable units are and therefore the smaller the risk of confounding is. Yet, with a smaller bandwidth, sample size and thus power decrease, increasing type M error.\nAn illustrative example\nTo illustrate this trade-off, we consider a standard application of the RD design in economics of education where a grant or additional lessons are assigned based on the score obtained by students on a standardized test. Students with test scores below a given threshold receive the treatment while those above do not. Yet, students far above and far below the threshold may differ along unobserved characteristics such as ability. To limit this bias, the effect of the treatment is estimated by comparing the outcomes of students just below and just above this threshold. This enable to limit disparities in terms of unobserved characteristics.\nThistlewaite and Campbell (1960) introduced the concept of RDD using this type of quasi-experiment. In their paper, they take advantage of a sharp discontinuity in the assignment of an award (a Certificate of Merit) based on qualifying scores at a test. This type of analysis is still used today and many papers leveraging similar methodologies have been published since this seminal work. For instance, Jacob and Lefgren (2004) exploit this type of discontinuity to study the impact of summer school and grade retention programs on test scores. Students who score below a given score are required to attend a summer school and to retake the test. Students who do not pass the second have to repeat the grade.\nModeling choices\nIn the present analysis, we build our simulations to replicate a similar type of quasi-experiment. In our fictional example, all students scoring below a cutoff \\(C\\) in a qualification test are required to take additional lessons. We want to estimate the effect of these additional lessons on scores on a final test taken by all students a year later.\nWe assume that the final score of student \\(i\\), \\(Final_i\\), is correlated with their qualification score \\(Qual_i\\) and their treatment status \\(T_i\\), ie whether student \\(i\\) received additional lessons or not. We further assume that both qualification and final test scores are affected by students’ unobserved ability \\(U_i\\) in a non linear way.\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nFinal test scores are thus defined as follows:\n\\[Final_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} +  \\delta f(U_i) + \\epsilon_{i}\\] Where \\(\\alpha\\) is a constant, \\(f\\) a non linear function and \\(e \\sim \\mathcal{N}(0, \\sigma_{e})\\) noise. The parameter of interest is \\(\\beta\\). Translating this into a potential outcomes framework, we have \\(Final_i(0) = \\alpha + \\gamma Qual_{i} + \\delta f(U_i) + \\epsilon_{i}\\) and \\(Final_i(1) = \\alpha + \\gamma Qual_{i} + \\beta + \\delta f(U_i) + \\epsilon_{i}\\)\n\nTo simplify, we consider the following assumptions:\nFull compliance and a sharp treatment allocation such that \\(T_i = \\mathbb{I}[Qual_{i} < C]\\). All students with a qualification score below the threshold are treated and receive additional lessons. None of the students with a qualification score above the threshold are treated.\nThe unobserved ability affects qualification and final test scores in a cubic way. A large ability has a strong positive impact on test scores. Similarly a particularly low ability strongly impacts test scores negatively. An average ability does not have much impact on test scores. Such a functional form seems realistic. Note that ability creates an OVB only if it has a non linear impact on test scores.\nWe assume constant treatment effects. This assumption is not necessary and our results hold if we consider non-constant treatment effects. We thus may drop this assumption in the future.\nWe assume that the unobserved availability affects the qualification and final score in a similar way and therefore with the same intensity \\(\\delta\\).\nMore precisely, we set:\n\\(N\\) the number of students\n\\(U \\sim \\mathcal{N}(0, \\sigma_u^{2})\\) the unobserved ability.\n\\(Qual_i = H_i + \\delta U_i^{2}\\) where \\(H \\sim \\mathcal{N}(\\mu_h, \\sigma_h^{2})\\). We center the qualification scores such that treated units are below 0 and non treated ones above.\n\\(T_i = \\mathbb{I}[Qual_{i} < q_c]\\) where for now and for simplicity, \\(q_c\\) is a fixed grade threshold given as the quantile in the qualification score distribution.\n\\(e \\sim \\mathcal{N}(0, \\sigma_e^2)\\)\n\\(Final_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} + \\delta U_i^{2} + e_{i}\\)\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\nOnce the fake data is generated, to make things more realistic we consider our data as if it was actual data. We do not take advantage of our knowledge of the data generating process in the estimation procedure. However, we observe both potential outcomes and the unobserved ability. Note that, in a real world setting, one would generally know the value of the threshold (and thus of \\(q_c\\)). Based on that and to simplify the computation of the bandwidth, we store \\(q_c\\).\n\n\ngenerate_data_rdd <- function(N, \n                              sigma_u,\n                              mu_h, \n                              sigma_h, \n                              sigma_e, \n                              alpha, \n                              beta,\n                              gamma,\n                              delta,\n                              q_c) {\n  \n  data <- tibble(id = 1:N) %>% \n    mutate(\n      # qual = rnorm(nrow(.), mu_h, sigma_h),\n      # u = rnorm(nrow(.), 0.5, sigma_u) + qual + 0.3*qual^3,\n      u = rnorm(nrow(.), 0, sigma_u),\n      qual = rnorm(nrow(.), mu_h, sigma_h) + delta*u^2,\n      e = rnorm(nrow(.), 0, sigma_e),\n      # qual_c = qual - quantile(qual, q_c),\n      # treated = qual_c < 0,\n      # threshold = quantile(qual, q_c),\n      treated = qual < quantile(qual, q_c),\n      final0 = alpha + gamma*qual + delta*u^2 + e,\n      final1 = final0 + beta,\n      final = final0 + beta*treated,\n      q_c = q_c\n    )\n  \n  return(data)\n}\n\n\n\nWe set baseline values for the parameters to emulate a somehow realistic observational study in this field. The set of parameters may produce test score outside of the range 0-100 in some iterations but that does not affect the analysis. We add the parameter value for delta separately as we will vary the value later and will reuse the vector baseline_param_RDD.\n\n\nbaseline_param_RDD <- tibble(\n  N = 1000,\n  sigma_u = 0.5,\n  mu_h = 75,\n  sigma_h = 7,\n  sigma_e = 4,\n  alpha = 20,\n  beta = 1,\n  gamma = 0.7,\n  q_c = 0.5\n)\n\n\n\nHere is an example of data created with our data generating process:\n\nid\nu\nqual\ne\ntreated\nfinal0\nfinal1\nfinal\nq_c\n1\n0.4330823\n80.70603\n4.8361755\nFALSE\n81.51796\n82.51796\n81.51796\n0.5\n2\n0.0942671\n75.23900\n1.1462353\nTRUE\n73.82243\n74.82243\n74.82243\n0.5\n3\n-0.0418272\n65.33548\n3.3869641\nTRUE\n69.12355\n70.12355\n70.12355\n0.5\n4\n0.0037051\n78.89305\n3.6874072\nFALSE\n78.91256\n79.91256\n78.91256\n0.5\n5\n-0.3756924\n77.74444\n-2.2495499\nFALSE\n72.31270\n73.31270\n72.31270\n0.5\n6\n-0.1371518\n61.27316\n-1.7107335\nTRUE\n61.19929\n62.19929\n62.19929\n0.5\n7\n0.4657378\n85.28003\n0.4733444\nFALSE\n80.38628\n81.38628\n80.38628\n0.5\n8\n0.0807173\n69.08269\n0.3815391\nTRUE\n68.74594\n69.74594\n69.74594\n0.5\n9\n0.2098066\n78.73868\n5.3042502\nFALSE\n80.46534\n81.46534\n80.46534\n0.5\n10\n-0.6058583\n68.97243\n-2.6077159\nTRUE\n66.04005\n67.04005\n67.04005\n0.5\n\nDefining the bandwidth\nIn a RDD, the model is estimated only for observations close enough to the threshold, ie in a given bandwidth. We therefore create a function to define this bandwidth by adding a variable to the data set treated_bw that is equal to NA if the observations is outside of the bandwidth, TRUE if the observation falls in the bandwidth and the student is treated and FALSE if the observation falls in the bandwidth and the student is not treated. The bandwidth parameter bw represents the proportion of units that are in the bandwidth. If bw = 0.1, 10% of the students are in the bandwidth for instance.\n\n\ndefine_bw <- function (data, bw) {\n  data <- data %>% \n    mutate(\n      treated_bw = ifelse(\n        dplyr::between(\n          qual, \n          quantile(qual, unique(q_c) - bw/2), \n          quantile(qual, unique(q_c) + bw/2)\n        ), \n        treated, \n        NA\n      )\n    )\n} \n\n\n\nThe following graph illustrates this process by plotting final test scores against qualification ones depending on the value of treated_bw.\n\n\n\nEstimation\nAfter generating the data, we can run an estimation.\nNote that to run power calculations, we need to have access to the true effects. Therefore, before running the estimation, we write a short function to compute the average treatment effect on the treated (ATET). We will add this information to the estimation results.\n\n\ncompute_true_effect_rdd <- function(data) {\n  treated_data <- data %>% \n    filter(treated) \n  return(mean(treated_data$final1 - treated_data$final0))\n}  \n\n\n\nWe then run the estimation. To do so, we only consider observations within the bandwidth and regress the final test scores on the treatment, the qualification score and their interaction. Note that we include this interaction term to allow more flexibility and to mimic an realistic estimation. Yet, we know that this interaction term does not appear in the DGP. Including it or not do not change the results. Also note that, of course, we do not include the unobserved ability in this model to create an OVB.\n\n\nestimate_rdd <- function(data, bw) {\n  data_in_bw <- data %>% \n    define_bw(bw = bw) %>% \n    filter(!is.na(treated_bw))\n  \n  reg <- lm(\n    data = data_in_bw, \n    formula = final ~ treated + qual\n  ) %>% \n    broom::tidy() %>%\n    filter(term == \"treatedTRUE\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se) %>%\n    mutate(\n      true_effect = compute_true_effect_rdd(data),\n      bw = bw\n    )\n  \n  return(reg)\n}\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_rdd and estimate_rdd. To do so we create the function compute_sim_RDD. This simple function takes as input the various parameters along with a vector of bandwidth sizes, vect_bw. If we want to run several simulations with different bandwidths, we can reuse the same data, hence why we allow to passing a vector of bandwidths and not only one bandwidth. The function returns a table with the estimate of the treatment, its p-value and standard error, the true effect and the bandwidth and intensity of the OVB considered (delta). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_sim_RDD <- function(N,\n                            sigma_u,\n                            mu_h,\n                            sigma_h,\n                            sigma_e,\n                            alpha,\n                            beta,\n                            gamma,\n                            delta,\n                            q_c,\n                            vect_bw) {\n  \n  data <- generate_data_rdd(\n    N = N,\n    sigma_u = sigma_u,\n    mu_h = mu_h,\n    sigma_h = sigma_h,\n    sigma_e = sigma_e,\n    alpha = alpha,\n    beta = beta,\n    gamma = gamma,\n    delta = delta,\n    q_c = q_c\n  ) \n  \n  map_dfr(vect_bw, estimate_rdd, data = data) %>%\n    mutate(delta = delta)\n} \n\n\n\nHere is an example of an output of this function.\n\nestimate\np_value\nse\ntrue_effect\nbw\ndelta\n2.156786\n0.1683250\n1.553910\n1\n0.1\n1\n2.068786\n0.0787043\n1.170505\n1\n0.2\n1\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_RDD function on each set of parameters. We thus create a table with all the values of the parameters we want to test param_rdd. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\nsimple_param_RDD <- tibble(\n  N = 500,\n  sigma_u = 1,\n  mu_h = 0,\n  sigma_h = 1,\n  sigma_e = 0.5,\n  alpha = 1,\n  beta = 1,\n  gamma = 0.7,\n  q_c = 0.5\n)\n\nfixed_param_RDD <- simple_param_RDD #%>% rbind(...)\n# vect_bw <- seq(0.05, 0.4, 0.05)\nvect_bw <- c(seq(0.05, 0.4, 0.05), seq(0.4, 1, 0.1))\nvect_delta <- c(3)\nn_iter <- 1000\n\nparam_rdd <- fixed_param_RDD %>% \n  crossing(delta = vect_delta) %>% \n  mutate(vect_bw = list(vect_bw)) %>% \n  crossing(rep_id = 1:n_iter) %>% \n  select(-rep_id)\n\n\n\nWe then run the simulations by mapping our compute_sim_RDD function on param_rdd.\n\n\ntic()\nsim_rdd <- pmap_dfr(param_rdd, compute_sim_RDD)\nbeep()\ntoc()\n\n# saveRDS(sim_rdd, here(\"Outputs/sim_rdd.RDS\"))\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results. In the following figure, we can see that for small bandwidth estimates are unbiased but imprecise while for large bandwidths estimates are precise but biased.\n\n\n\nWhen the bandwidth is relatively small, estimates are spread out and the mean of statistically significant estimates is larger than the true effect. Note that the average of all estimates, significant and non-significant, is close to the true effect. Applying a statistical significance filter leads to overestimate the true effect in this case.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}\\left[\\left|\\frac{\\widehat{\\beta_{RDD}}}{\\beta_0}\\right|\\right]\\) and \\(\\mathbb{E}\\left[\\left|\\frac{\\widehat{\\beta_{RDD}}}{\\beta_0}\\right| | signif \\right]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the true effect size. To enable comparison across simulations and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_simulations <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(delta, bw) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),\n      bias_all = mean(abs(estimate/true_effect), na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_sim_rdd <- summarise_simulations(sim_rdd)\n# saveRDS(summary_sim_rdd, here(\"Outputs/summary_sim_rdd.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\nWe notice that, the smaller the bandwidth size, the closer the average of all estimates is to the true effect. Yet, when the bandwidth gets small significant estimates overestimate the true effect. This arises because of a loss of power, as shown in the graph below.\n\n\nsummary_sim_rdd %>% \n  ggplot(aes(x = bw, y = power)) + \n  geom_line(size = 0.8) +\n  labs(\n    x = \"Bandwidth size\",\n    y = \"Power\",\n    title = \"Evolution of power with bandwith size\"\n  )\n\n\n\n\n\n\n\n",
      "last_modified": "2022-01-10T12:06:37+01:00"
    }
  ],
  "collections": []
}
