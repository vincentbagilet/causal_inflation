{
  "articles": [
    {
      "path": "Event_study.html",
      "title": "Simulations Event study/DID",
      "description": "In this document, we run a simulation exercise to illustrate the loss in power and resulting type M error when the number of events in a Difference In Differences design decreases.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "LÃ©o Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModelisation choices\nData generation\nDefining the bandwidth\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\n\n\n\nbody {\ntext-align: justify}\nSummary and intuition\nIn the case of Event studies and DiD, the OVB/type M trade-off is mediated by the number of events.\nAn illustrative example\nFor readability and to illustrate this loss in power, we consider an example setting. For this illustration we could use a large variety of Data Genereting Processes (DGP), both in terms of distribution of the variables and of relations between them. We narrow this down to an example setting, considering an analysis of health impacts of air pollution. Our point should stand in more general settings.\nA threat of confounders often arises when analyzing the health effects of air pollution. To estimate such an effect causally, one can consider exogeneous shocks to air pollution. In the present analysis, we consider the example of plant closures as exogeneous shocks.\nModelisation choices\nIn the present analysis, we build our simulations to replicate an analysis of the causal health effects of air pollution, using plant closures as an exogeneous shifter in air pollution levels. In such analyses, the data is often at the city-daily level. Hospital admissions are usual health outcomes considered in such analyses.\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nThe effect of plant closures, the treatment, on health is often estimated using\n\\[Health_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} +  \\delta f(U_i) + \\epsilon_{i}\\] Where \\(\\alpha\\) is a constant, \\(f\\) a non linear function and \\(e \\sim \\mathcal{N}(0, \\sigma_{e})\\) noise. The parameter of interest is \\(\\beta\\). Translating this into a potential outcomes framework, we have \\(Final_i(0) = \\alpha + \\gamma Qual_{i} + \\delta f(U_i) + \\epsilon_{i}\\) and \\(Final_i(1) = \\alpha + \\gamma Qual_{i} + \\beta + \\delta f(U_i) + \\epsilon_{i}\\)\n\nTo simplify, we consider the following assumptions:\nFull compliance and a sharp treatment allocation such that \\(T_i = \\mathbb{I}[Qual_{i} < C]\\). All students with a qualification score below the threshold are treated and receive additional lessons. None of the students with a qualification score above the threshold are treated.\nThe unobserved ability affects qualification and final test scores in a cubic way. A large ability has a strong positive impact on test scores. Similarly a particularly low ability strongly impacts test scores negatively. An average ability does not have much impact on test scores. Such a functional form seems realistic. Note that ability creates an OVB only if it has a non linear impact on test scores.\nWe assume constant treatment effects. This assumption is not necessary and our results hold if we consider non-constant treatment effects. We thus may drop this assumption in the future.\nWe assume that the unobserved availability affects the qualification and final score in a similar way and therefore with the same intensity \\(\\delta\\).\nMore precisely, we set:\n\\(N\\) the number of students\n\\(U \\sim \\mathcal{N}(0, \\sigma_u^{2})\\) the unobserved ability.\n\\(Qual_i = H_i + \\delta U_i^{3}\\) where \\(H \\sim \\mathcal{N}(\\mu_h, \\sigma_h^{2})\\). We center the qualification scores such that treated units are below 0 and non treated ones above.\n\\(T_i = \\mathbb{I}[Qual_{i} < q_c]\\) where for now and for simplicity, \\(q_c\\) is a fixed grade threshold given as the quantile in the qualification score distribution.\n\\(e \\sim \\mathcal{N}(0, \\sigma_e^2)\\)\n\\(Final_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} + \\delta U_i^{3} + e_{i}\\)\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\nOnce the fake data is generated, to make things more realistic we consider our data as if it was actual data. We do not take advantage of our knowledge of the data generating process in the estimation procedure. However, we observe both potential outcomes and the unobserved ability. Note that, in a real world setting, one would generally know the value of the threshold (and thus of \\(q_c\\)). Based on that and to simplify the computation of the bandwidth, we store \\(q_c\\).\n\n\ngenerate_data_rdd <- function(N, \n                              sigma_u,\n                              mu_h, \n                              sigma_h, \n                              sigma_e, \n                              alpha, \n                              beta,\n                              gamma,\n                              delta,\n                              q_c) {\n  \n  data <- tibble(id = 1:N) %>% \n    mutate(\n      # qual = rnorm(nrow(.), mu_h, sigma_h),\n      # u = rnorm(nrow(.), 0.5, sigma_u) + qual + 0.3*qual^3,\n      u = rnorm(nrow(.), 0, sigma_u),\n      qual = rnorm(nrow(.), mu_h, sigma_h) + delta*u^2,\n      e = rnorm(nrow(.), 0, sigma_e),\n      # qual_c = qual - quantile(qual, q_c),\n      # treated = qual_c < 0,\n      # threshold = quantile(qual, q_c),\n      treated = qual < quantile(qual, q_c),\n      final0 = alpha + gamma*qual + delta*u^2 + e,\n      final1 = final0 + beta,\n      final = final0 + beta*treated,\n      q_c = q_c\n    )\n  \n  return(data)\n}\n\n\n\nWe set baseline values for the parameters to emulate a somehow realistic observational study in this field. The set of parameters may produce test score outside of the range 0-100 in some iterations but that does not affect the analysis. We add the parameter value for delta separately as we will vary the value later and will reuse the vector baseline_parameters_RDD.\n\n\nbaseline_parameters_RDD <- tibble(\n  N = 1000,\n  sigma_u = 0.5,\n  mu_h = 75,\n  sigma_h = 7,\n  sigma_e = 4,\n  alpha = 20,\n  beta = 1,\n  gamma = 0.7,\n  q_c = 0.5\n)\n\n\n\nHere is an example of data created with our data generating process:\n\nid\nu\nqual\ne\ntreated\nfinal0\nfinal1\nfinal\nq_c\n1\n-0.3178828\n78.77449\n-3.2033609\nFALSE\n72.03983\n73.03983\n72.03983\n0.5\n2\n0.4601306\n79.44095\n-5.4797153\nFALSE\n70.34067\n71.34067\n70.34067\n0.5\n3\n-0.3043841\n62.49537\n1.4412064\nTRUE\n65.28062\n66.28062\n66.28062\n0.5\n4\n0.3342143\n70.06890\n1.3323087\nTRUE\n70.49224\n71.49224\n71.49224\n0.5\n5\n0.7013604\n75.37122\n-3.3685519\nFALSE\n69.88321\n70.88321\n69.88321\n0.5\n6\n-0.2892851\n68.19279\n4.6773938\nTRUE\n72.49603\n73.49603\n73.49603\n0.5\n7\n0.4269233\n69.54582\n0.5778817\nTRUE\n69.44222\n70.44222\n70.44222\n0.5\n8\n0.2014268\n79.63616\n5.5933254\nFALSE\n81.37921\n82.37921\n81.37921\n0.5\n9\n0.0885543\n77.96958\n2.4529824\nFALSE\n77.03953\n78.03953\n77.03953\n0.5\n10\n0.3338956\n73.55903\n2.7086810\nTRUE\n74.31149\n75.31149\n75.31149\n0.5\n\nDefining the bandwidth\nIn a RDD, the model is estimated only for observations close enough to the threshold, ie in a given bandwidth. We therefore create a function to define this bandwidth by adding a variable to the data set treated_bw that is equal to NA if the observations is outside of the bandwidth, TRUE if the observation falls in the bandwidth and the student is treated and FALSE if the observation falls in the bandwidth and the student is not treated. The bandwidth parameter bw represents the proportion of units that are in the bandwidth. If bw = 0.1, 10% of the students are in the bandwidth for instance.\n\n\ndefine_bw <- function (data, bw) {\n  data <- data %>% \n    mutate(\n      treated_bw = ifelse(\n        dplyr::between(\n          qual, \n          quantile(qual, unique(q_c) - bw/2), \n          quantile(qual, unique(q_c) + bw/2)\n        ), \n        treated, \n        NA\n      )\n    )\n} \n\n\n\nThe following graph illustrates this process by plotting final test scores against qualification ones depending on the value of treated_bw.\n\n\n\nEstimation\nAfter generating the data, we can run an estimation.\nNote that to run power calculations, we need to have access to the true effects. Therefore, before running the estimation, we write a short function to compute the average treatment effect on the treated (ATET). We will add this information to the estimation results.\n\n\ncompute_true_effect_rdd <- function(data) {\n  treated_data <- data %>% \n    filter(treated) \n  return(mean(treated_data$final1 - treated_data$final0))\n}  \n\n\n\nWe then run the estimation. To do so, we only consider observations within the bandwidth and regress the final test scores on the treatment, the qualification score and their interaction. Note that we include this interaction term to allow more flexibility and to mimic an realistic estimation. Yet, we know that this interaction term does not appear in the DGP. Including it or not do not change the results. Also note that, of course, we do not include the unobserved ability in this model to create an OVB.\n\n\nestimate_rdd <- function(data, bw) {\n  data_in_bw <- data %>% \n    define_bw(bw = bw) %>% \n    filter(!is.na(treated_bw))\n  \n  reg <- lm(\n    data = data_in_bw, \n    formula = final ~ treated + qual\n  ) %>% \n    broom::tidy() %>%\n    filter(term == \"treatedTRUE\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se) %>%\n    mutate(\n      true_effect = compute_true_effect_rdd(data),\n      bw = bw\n    )\n  \n  return(reg)\n}\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_rdd and estimate_rdd. To do so we create the function compute_sim_RDD. This simple function takes as input the various parameters along with a vector of bandwidth sizes, vect_bw. If we want to run several simulations with different bandwidths, we can reuse the same data, hence why we allow to passing a vector of bandwidths and not only one bandwidth. The function returns a table with the estimate of the treatment, its p-value and standard error, the true effect and the bandwidth and intensity of the OVB considered (delta). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_sim_RDD <- function(N,\n                            sigma_u,\n                            mu_h,\n                            sigma_h,\n                            sigma_e,\n                            alpha,\n                            beta,\n                            gamma,\n                            delta,\n                            q_c,\n                            vect_bw) {\n  \n  data <- generate_data_rdd(\n    N = N,\n    sigma_u = sigma_u,\n    mu_h = mu_h,\n    sigma_h = sigma_h,\n    sigma_e = sigma_e,\n    alpha = alpha,\n    beta = beta,\n    gamma = gamma,\n    delta = delta,\n    q_c = q_c\n  ) \n  \n  map_dfr(vect_bw, estimate_rdd, data = data) %>%\n    mutate(delta = delta)\n} \n\n\n\nHere is an example of an output of this function.\n\n# A tibble: 2 x 6\n  estimate p_value    se true_effect    bw delta\n     <dbl>   <dbl> <dbl>       <dbl> <dbl> <dbl>\n1     2.32  0.160   1.64           1   0.1     1\n2     1.94  0.0772  1.09           1   0.2     1\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_RDD function on each set of parameters. We thus create a table with all the values of the parameters we want to test param_rdd. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\nsimple_parameters_RDD <- tibble(\n  N = 500,\n  sigma_u = 1,\n  mu_h = 0,\n  sigma_h = 1,\n  sigma_e = 0.5,\n  alpha = 1,\n  beta = 1,\n  gamma = 0.7,\n  q_c = 0.5\n)\n\nfixed_parameters_RDD <- simple_parameters_RDD #%>% rbind(...)\n# vect_bw <- seq(0.05, 0.4, 0.05)\nvect_bw <- c(seq(0.05, 0.4, 0.05), seq(0.4, 1, 0.1))\nvect_delta <- c(3)\nn_iter <- 1000\n\nparam_rdd <- fixed_parameters_RDD %>% \n  crossing(delta = vect_delta) %>% \n  mutate(vect_bw = list(vect_bw)) %>% \n  crossing(rep_id = 1:n_iter) %>% \n  select(-rep_id)\n\n\n\nWe then run the simulations by mapping our compute_sim_RDD function on param_rdd.\n\n\ntic()\nsimulations_rdd <- pmap_dfr(param_rdd, compute_sim_RDD)\nbeep()\ntoc()\n\n# saveRDS(simulations_rdd, here(\"Outputs/simulations_rdd.RDS\"))\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta_{RDD}}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta_{RDD}}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_simulations <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(delta, bw) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),\n      bias_sign = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_simulations_rdd <- summarise_simulations(simulations_rdd)\n# saveRDS(summary_simulations_rdd, here(\"Outputs/summary_simulations_rdd.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\n\n\n\n",
      "last_modified": "2021-12-06T11:05:54-05:00"
    },
    {
      "path": "index.html",
      "title": "Causal Inflation: Unbiased but Inflated Causal Effects",
      "author": [],
      "contents": "\n\n          \n      \n      Causal inflation\n      \n      \n      Home\n      \n      \n      Simulations\n       \n      â¾\n      \n      \n      RDD\n      IV\n      Matching\n      RCT\n      \n      \n      \n      \n      \n      â°\n      \n      \n      \n        \n          \n            \n              \n            \n              Causal Inflation: Unbiased but Inflated Causal Effects\n            \n            \n              \n                \n                    \n                      \n                         GitHub\n                      \n                    \n                  \n                                  \n            \n          \n        \n        \n        \n          \n            Hi and welcome!\n            This website gathers code and additional material for the paper âCausal Inflation: Unbiased but Inflated Causal Effectsâ by Vincent Bagilet and LÃ©o Zabrocki.\n            The website is under construction and the analysis is still in a preliminary stage.\n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Causal Inflation: Unbiased but Inflated Causal Effects\n            \n            \n              \n                \n                                    \n                    \n                       GitHub\n                    \n                  \n                                  \n              \n            \n            \n              Hi and welcome!\n              This website gathers code and additional material for the paper âCausal Inflation: Unbiased but Inflated Causal Effectsâ by Vincent Bagilet and LÃ©o Zabrocki.\n              The website is under construction and the analysis is still in a preliminary stage.\n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2021-12-01T18:20:20-05:00"
    },
    {
      "path": "IV.html",
      "title": "Simulations IV",
      "description": "In this document, we run a simulation exercise to illustrate the existence of a trade-off between Omitted Variable Bias (OVB) and type M error in the context of an Instrumental Variable (IV) strategy.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "LÃ©o Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModelisation choices\nData generation\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\n\n\n\nbody {\ntext-align: justify}\nSummary and intuition\nIn the case of the IV, the OVB/type M trade-off is mediated by the âstrengthâ of the instrument considered.\nAn illustrative example\nTo illustrate this trade-off between OVB and type M error, we consider a standard application of the IV design in environmental economics.\nModelisation choices\nIn the present analysis, we build our simulations to replicate a similar type of quasi-experiment.\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nTo simplify, we consider the following assumptions:\nMore precisely, we set:\n\\(N\\) the number of observations\n\\(Z \\sim \\mathcal{N}(0, \\sigma_{z}^{2})\\) or \\(Z \\sim \\text{Bernoulli}(p_z)\\) the instrument\n\\(U \\sim \\mathcal{N}(0, \\sigma_{u}^{2})\\) the unobserved variable\n\\(X = \\alpha_x + \\gamma_x Z + \\delta_x U + e_x\\) the dependent variable and where \\(e_x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})\\)\n\\(Y = \\alpha_y + \\beta X + \\delta_y U + e_y\\) the independent variable and where \\(e_y \\sim \\mathcal{N}(0, \\sigma_{y}^{2})\\)\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\nNote that the parameter type_z describes whether z is a random sample from a normal or bernoulli distribution. It can take the values normal or bernoulli.\n\n\ngenerate_data_IV <- function(N,\n                             type_z, #\"normal\" or \"bernoulli\"\n                             param_z,\n                             sigma_u,\n                             sigma_ex,\n                             sigma_ey,\n                             alpha_y,\n                             alpha_x,\n                             treatment_effect,\n                             iv_strength,\n                             ovb_intensity\n                             ) {\n  \n  if (type_z == \"bernoulli\") {\n    z_gen <- rbernoulli(N, param_z)\n  } else if (type_z == \"normal\") {\n    z_gen <- rnorm(N, 0, param_z)\n  } else {\n    stop(\"type_z must be either 'bernoulli' or 'normal'\")\n  } \n  \n  data <- tibble(id = 1:N) %>% \n    mutate(\n      z = z_gen,\n      u = rnorm(nrow(.), 0, sigma_u),\n      e_x = rnorm(nrow(.), 0, sigma_ex),\n      e_y = rnorm(nrow(.), 0, sigma_ey),\n      x = alpha_x + iv_strength*z + ovb_intensity*u + e_x,\n      y = alpha_y + treatment_effect*x + ovb_intensity*u + e_y\n    )\n  \n  return(data)\n}\n\n\n\nWe set baseline values for the parameters to emulate a somehow realistic observational study in this field. The set of parameters may produce test score outside of the range 0-100 in some iterations but that does not affect the analysis. We add the parameter value for delta separately as we will vary the value later and will reuse the vector baseline_parameters_RDD.\n\n\nsimple_parameters_IV <- tibble(\n  N = 500,\n  type_z = \"normal\",\n  param_z = 1,\n  sigma_u = 1,\n  sigma_ex = 1,\n  sigma_ey = 1,\n  alpha_y = 0,\n  alpha_x = 0,\n  treatment_effect = 1\n)\n\n\n\nHere is an example of data created with our data generating process:\n\nid\nz\nu\ne_x\ne_y\nx\ny\n1\n0.2775586\n-0.6221728\n0.8829871\n-0.9290238\n0.6896297\n-0.4882632\n2\n0.8995503\n-0.3021700\n0.4345562\n-0.0825179\n0.4935983\n0.2902124\n3\n0.6340309\n-0.0008905\n0.6234313\n2.0804372\n0.7498813\n2.8299623\n4\n0.9010602\n0.2192239\n0.8891471\n1.4142692\n1.1570487\n2.6590074\n5\n0.5100178\n1.4489707\n-1.3949263\n1.2978400\n-0.7133345\n1.1640937\n6\n-0.0817340\n-0.0042455\n1.8929381\n0.1125944\n1.8748931\n1.9857893\n7\n1.0789195\n-0.9960969\n1.2462396\n-0.3566309\n1.0635848\n0.3085151\n8\n-0.6641403\n-0.5974989\n-1.7225246\n1.2537948\n-2.0943522\n-1.0795569\n9\n-0.5170472\n0.4649146\n-1.4689689\n0.6062510\n-1.3864125\n-0.5941957\n10\n0.8603847\n-0.8976445\n-0.5499971\n-0.5613110\n-0.7369779\n-1.6573467\n\n\n\n\n\n\n\n\n\n\nEstimation\nAfter generating the data, we can run an estimation. We want to compare the IV and the OLS for different IV strength values. Hence, we need to estimate both an IV and an OLS and return both set of outcomes of interest.\n\n\nestimate_IV <- function(data) {\n  reg_IV <- ivreg(\n    data = data, \n    formula = y ~ x | z\n    ) %>% \n    broom::tidy() %>%\n    mutate(model = \"IV\")\n  \n  reg_OLS <- lm(\n    data = data, \n    formula = y ~ x\n    ) %>% \n    broom::tidy() %>%\n    mutate(model = \"OLS\")\n  \n  reg_OLS_unbiased <- lm(\n    data = data, \n    formula = y ~ x + u\n    ) %>% \n    broom::tidy() %>%\n    mutate(model = \"OLS unbiased\")\n  \n  reg <- reg_IV %>% \n    rbind(reg_OLS) %>% \n    rbind(reg_OLS_unbiased) %>% \n    filter(term == \"x\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se, model) %>% \n  \n  return(reg)\n}\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_IV and estimate_IV. To do so we create the function compute_sim_IV. This simple function takes as input the various parameters along with the bandwidth size, bw. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect and the bandwidth and intensity of the OVB considered (ovb_intensity). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_sim_IV <- function(N,\n                                  type_z,\n                                  param_z,\n                                  sigma_u,\n                                  sigma_ex,\n                                  sigma_ey,\n                                  alpha_y,\n                                  alpha_x,\n                                  treatment_effect,\n                                  iv_strength,\n                                  ovb_intensity) {\n  generate_data_IV(\n    N = N,\n    type_z = type_z,\n    sigma_u = sigma_u,\n    param_z = param_z,\n    sigma_ex = sigma_ex,\n    sigma_ey = sigma_ey,\n    alpha_y = alpha_y,\n    alpha_x = alpha_x,\n    treatment_effect = treatment_effect,\n    iv_strength = iv_strength,\n    ovb_intensity = ovb_intensity\n  ) %>%\n    estimate_IV() %>%\n    mutate(\n      iv_strength = iv_strength,\n      ovb_intensity = ovb_intensity\n    )\n} \n\n\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_IV function on each set of parameters. We thus create a table with all the values of the parameters we want to test param_IV. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\nbaseline_parameters <- tibble(\n  N = 500,\n  type_z = \"normal\",\n  param_z = 1,\n  sigma_u = 1,\n  sigma_ex = 1,\n  sigma_ey = 1,\n  alpha_y = 0,\n  alpha_x = 0,\n  treatment_effect = 1\n)\n\nfixed_parameters <- baseline_parameters #%>% rbind(...)\nvect_iv_strength <- c(seq(0.05, 0.4, 0.05), seq(0.4, 0.6, 0.1))\n# vect_iv_strength <- c(0.1)\nvect_ovb_intensity <- c(0.4)\nn_iter <- 1000\n\nparam_IV <- fixed_parameters %>% \n  crossing(vect_iv_strength, vect_ovb_intensity) %>% \n  rename(iv_strength = vect_iv_strength, ovb_intensity = vect_ovb_intensity) %>% \n  crossing(rep_id = 1:n_iter) %>% \n  select(-rep_id)\n\n\n\nWe then run the simulations by mapping our compute_sim_IV function on param_IV.\n\n\ntic()\nsimulations_IV <- pmap_dfr(param_IV, compute_sim_IV)\nbeep()\ntoc()\n\n# saveRDS(simulations_IV, here(\"Outputs/simulations_IV.RDS\"))\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nWe notice that the OLS is always biased and that the IV is never biased. However, for limited IV strengths, the distribution of the estimates flattens. The smaller the IV strength, the most like it is to get an estimate away from the true value, even though the expected value remains equal to the true effect size. \nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta_{i}}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta_{RDD}}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_simulations <- function(data, true_effect = 1) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(ovb_intensity, iv_strength, model) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),\n      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      bias_all_median = median(estimate/true_effect, na.rm = TRUE),\n      # mean_f_stat = mean(f_stat, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_simulations_IV <- summarise_simulations(simulations_IV)\n# saveRDS(summary_simulations_IV, here(\"Outputs/summary_simulations_IV.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\nOf course, if one considers all estimates, as the IV is unbiased, this issue does not arise. For now, we consider the median because for very low IV strength, we get very extreme values. We need to investigate this further.\n\n\n\n\n\n\n",
      "last_modified": "2021-11-16T16:16:57-05:00"
    },
    {
      "path": "Matching.html",
      "title": "Matching Simulations",
      "description": "In this document, we run a simulation exercise to illustrate the existence of a trade-off between Omitted Variable Bias (OVB) and type M error for observational studies relying on matching methods.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "LÃ©o Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nData Generating ProcedureGeneral Approach\nFunction to Generate the Data\nEDA for One Dataset\n\nMatching ProcedureTesting Propensity Score\nProposensity Score Function\n\nSimulationsVincentâs analysis\n\n\n\nbody {\ntext-align: justify}\nSummary and intuition\nIn this document, we show through simulations the Type M error - omitted variable bias trade-off for observational studies relying on matching methods. To make our simulations more concrete and realistic, we create fake-data similar to those used for analyzing labor training program. Should you have any questions or find coding errors, please do not hesitate to reach us at vincent.bagilet@columbia.edu and leo.zabrocki@psemail.eu.\nData Generating Procedure\nGeneral Approach\nTo illustrate the the Type M error - OVB trade-off, we simulate fake-data from a labor training program targeting young individuals:\nWe first create 1,000 unit identifiers (id) and assign 10% of these individuals to the treatment (treatment).\nWe then simulate five covariates:\nage: the age of the individual.\neducation: a dummy equal to one if 1 if the individual obtained post-high education and to 0 otherwise.\nprevious_employment: a dummy equal to 1 if the the individual was employed in the previous year and to 0 otherwise.\nminority: a dummy equal to 1 if the individual belongs to a minority group and to 0 otherwise.\nunobserved_motivation: an unobserved variable measuring the motivation of the individuals to succeed in the job market. It takes discrete values from 0 to 10, where 0 is not motivated and 10 highly motivated.\n\nThe distribution of these covariates is specific each group in order to create some imbalance.\nOnce the covariates are created, we define the potential outcomes of each individual. Here, potential outcomes represent the income (in euros) of the individuals if they undertake the training program or not. The potential outcome without treatment adoption is simulated using the following equation:\nY(0) = 400 + 50 \\(\\times\\) previous_employment + 50 \\(\\times\\) unobserved_motivation + 10 \\(\\times\\) age + 50 \\(\\times\\) education - 25 \\(\\times\\) minority\nWe finally simulate the potential outcomes for the treatment adoption. The average treatment effect on the treated was set to 80. The average treatment effect on the control was set to 50..\nFunction to Generate the Data\nWe display below the code for the function generate_data() which creates the dataset:\n\n\n# create function to generate data\ngenerate_data <- function() {\n  data <- tibble(id = 1:500) %>%\n    # generate treatment assignment\n    mutate(treatment = rbinom(n = 500, size = 1, prob = 0.1)) %>%\n    # generate covariates\n    rowwise() %>%\n    # previous employment\n    mutate(\n      previous_employment = ifelse(\n        treatment == 1,\n        rbinom(n = 1, size = 1, prob = 0.4),\n        rbinom(n = 1, size = 1, prob = 0.55)\n      ),\n      # unobserved motivation on a scale from 1:10\n      unobserved_motivation = ifelse(\n        treatment == 1,\n        rbinom(n = 1, size = 1, prob = 0.7),\n        rbinom(n = 1, size = 1, prob = 0.4)\n      ),\n      # age\n      age = ifelse(\n        treatment == 1,\n        rnorm(n = 1, mean = 25, sd = 1),\n        rnorm(n = 1, mean = 27, sd = 1)\n      ) %>% round(.),\n      # post high school education\n      education = ifelse(\n        treatment == 1,\n        rbinom(n = 1, size = 1, prob = 0.3),\n        rbinom(n = 1, size = 1, prob = 0.5)\n      ),\n      # minority dummy\n      minority = ifelse(\n        treatment == 1,\n        rbinom(n = 1, size = 1, prob = 0.6),\n        rbinom(n = 1, size = 1, prob = 0.4)\n      )\n    )\n  \n  # generate the potential outcomes\n  data <- data %>%\n    rowwise() %>%\n    mutate(\n      y_0 = (400 + 50 * previous_employment + 100 * unobserved_motivation + 10 * age + 50 *\n            education - 25 * minority) + rnorm(1, mean = 0, sd = 300)\n       %>% round(., 0),\n      y_1 = ifelse(\n        treatment == 1,\n        y_0 + 80,\n        y_0 + 50\n      ),\n    y_obs = ifelse(treatment == 1, y_1, y_0) %>% round(., 0)\n    ) %>%\n    ungroup()\n  \n  return(data)\n}\n\n\n\nEDA for One Dataset\nWe run the function to explore the resulting data:\n\n\n# run the function\ndata <- generate_data()\n\n\n\nWe display the distribution of covariates by groups:\n\n\n\nWe can see that all covariates are imbalanced: treatment assignment is biased. We display below summary statistics for each variable by group:\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\"],[\"age\",\"age\",\"education\",\"education\",\"minority\",\"minority\",\"previous_employment\",\"previous_employment\",\"unobserved_motivation\",\"unobserved_motivation\",\"y_obs\",\"y_obs\"],[0,1,0,1,0,1,0,1,0,1,0,1],[27.01,25.13,0.51,0.37,0.36,0.48,0.61,0.42,0.39,0.68,769.24,837.6],[1.03,1.03,0.5,0.49,0.48,0.5,0.49,0.5,0.49,0.47,310.23,293.79],[24,22,0,0,0,0,0,0,0,0,-140,47],[30,27,1,1,1,1,1,1,1,1,1684,1482]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>variable<\\/th>\\n      <th>treatment<\\/th>\\n      <th>mean<\\/th>\\n      <th>sd<\\/th>\\n      <th>min<\\/th>\\n      <th>max<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5,6]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nWe can finally see how the observed revenue is distributed across the two groups:\n\n\n\nAnd we can check wether the ATT and ATC were correctly simulated. The ATT is computed such as:\n\n\n# checking att\nmean(data$y_1[data$treatment==1]) - mean(data$y_0[data$treatment==1])\n\n\n[1] 80\n\nand the ATC:\n\n\n# checking atc\nmean(data$y_1[data$treatment==0]) - mean(data$y_0[data$treatment==0])\n\n\n[1] 50\n\nIf we run the correct DGP in a multivariate regression model, we find on average the correct answer (ATT=+80 euros). We create a regression function to run the true DGP model:\n\n\nreg_true_dgp <- function(data) {\n  data %>%\n    lm(\n      y_obs ~ treatment + previous_employment + age + education + minority + unobserved_motivation,\n      data = .\n    ) %>%\n    broom::tidy(., conf.int = TRUE) %>%\n    filter(term == \"treatment\") %>%\n    select(estimate, p.value, conf.low, conf.high)\n}\n\n\n\nWe simulate 1000 datasets and run the true DGP regression model:\n\n\n# first simulate simulation id\ndata_simulations <- tibble(sim_id = 1:1000) %>%\n# then simulate data\n  mutate(data = map(sim_id, ~ generate_data())) %>%\n# finally run the reg analysis\n  mutate(results = map(data, ~ reg_true_dgp(.)))\n\n# saveRDS(data_simulations, here(\"Outputs/data_simulations.RDS\"))\n\n\n\nWe plot the distribution of estimates:\n\n\ndata_simulations <- readRDS(here(\"Outputs/data_simulations.RDS\"))\n\ndata_simulations %>% \n  select(-data) %>%\n  unnest(results) %>%\n  ggplot(., aes(x = estimate)) +\n  geom_density() +\n  geom_vline(xintercept = 80) +\n  theme_mediocre()\n\n\n\n\nWe compute the statistical power:\n\n\ndata_simulations %>% \n  select(-data) %>%\n  unnest(results) %>%\n  summarise(power = sum(p.value<=0.05)/n()*100)\n\n\n# A tibble: 1 x 1\n  power\n  <dbl>\n1  30.4\n\nResearchers unfortunately do not observe the motivation of individuals and therefore would get biased estimates. We first create the biased regression model without the unobserved motativation:\n\n\nreg_biased <- function(data) {\n  data %>%\n    lm(\n      y_obs ~ treatment + previous_employment + age + education + minority,\n      data = .\n    ) %>%\n    broom::tidy(., conf.int = TRUE) %>%\n    filter(term == \"treatment\") %>%\n    select(estimate, p.value, conf.low, conf.high)\n}\n\n\n\n\n\ndata_simulations <- data_simulations %>%\n  mutate(results_bias = map(data, ~ reg_biased(.)))\n\n\n\n\n\ndata_simulations %>% \n  select(-data) %>%\n  unnest(results_bias) %>%\n  ggplot(., aes(x = estimate)) +\n  geom_density() +\n  geom_vline(xintercept = 80) +\n  theme_mediocre()\n\n\n\n\nWe compute the statistical power:\n\n\ndata_simulations %>% \n  select(-data) %>%\n  unnest(results_bias) %>%\n  summarise(power = sum(p.value<=0.05)/n()*100)\n\n\n# A tibble: 1 x 1\n  power\n  <dbl>\n1  51.9\n\nMatching Procedure\nTesting Propensity Score\nTesting the nearest neighbor propensity score matching:\n\n\n# test nearest-neighbor matching\ntest_matching <-\n  matchit(\n    treatment ~ previous_employment + as.factor(age) + as.factor(education) + minority,\n    data = data\n  )\n\n# display results\ntest_matching\n\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 500 (original), 124 (matched)\n - target estimand: ATT\n - covariates: previous_employment, as.factor(age), as.factor(education), minority\n\nWe retrieve the matched data:\n\n\ndata_matched <- match.data(test_matching)\n\n\n\nWe fit the linear model:\n\n\nfit <- \n  lm(\n    y_obs ~ treatment + previous_employment + age + education + minority,\n    data = data_matched,\n    weights = weights\n  )\n\nbroom::tidy(coeftest(fit, vcov. = vcovCL, cluster = ~subclass), conf.int = TRUE) %>%\n  filter(term == \"treatment\") %>%\n  select(term, estimate, p.value, conf.low, conf.high)\n\n\n# A tibble: 1 x 5\n  term      estimate  p.value conf.low conf.high\n  <chr>        <dbl>    <dbl>    <dbl>     <dbl>\n1 treatment     187. 0.000545     82.7      291.\n\nAnother test with a caliper of 0.1:\n\n\n# test nearest-neighbor matching\ntest_matching <-\n  matchit(\n    treatment ~ previous_employment + as.factor(age) + as.factor(education) + minority,\n    caliper = 0.1,\n    data = data\n  )\n\n# display results\ntest_matching\n\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score [caliper]\n             - estimated with logistic regression\n - caliper: <distance> (0.021)\n - number of obs.: 500 (original), 76 (matched)\n - target estimand: ATT\n - covariates: previous_employment, as.factor(age), as.factor(education), minority\n\nWe retrieve the matched data:\n\n\ndata_matched <- match.data(test_matching)\n\n\n\nWe fit the linear model:\n\n\nfit <- \n  lm(\n    y_obs ~ treatment + previous_employment + age + education + minority,\n    data = data_matched,\n    weights = weights\n  )\n\nbroom::tidy(coeftest(fit, vcov. = vcovCL, cluster = ~subclass), conf.int = TRUE) %>%\n  filter(term == \"treatment\") %>%\n  select(term, estimate, p.value, conf.low, conf.high)\n\n\n# A tibble: 1 x 5\n  term      estimate p.value conf.low conf.high\n  <chr>        <dbl>   <dbl>    <dbl>     <dbl>\n1 treatment     181. 0.00985     44.9      317.\n\nProposensity Score Function\n\n\n# propensity score analysis function\nps_function <- function(data, caliper_value) {\n  matching_results <- matchit(\n    treatment ~ previous_employment + as.factor(age) + as.factor(education) + minority,\n    caliper = caliper_value,\n    data = data\n  )\n  \n  data_matched <- match.data(matching_results)\n  \n  n_matched <- nrow(data_matched)\n  \n  model_fit <- lm(\n    y_obs ~ treatment + previous_employment + age + education + minority,\n    data = data_matched,\n    weights = weights\n  )\n  \n  ps_att <- broom::tidy(coeftest(model_fit, vcov. = vcovCL, cluster = ~ subclass),\n                conf.int = TRUE) %>%\n    filter(term == \"treatment\") %>%\n    select(term, estimate, p.value, conf.low, conf.high)\n  \n  return(bind_cols(ps_att, n_matched = n_matched))\n}\n\n# testing the function\nps_function(data, caliper = 0)\n\n\n# A tibble: 1 x 6\n  term      estimate p.value conf.low conf.high n_matched\n  <chr>        <dbl>   <dbl>    <dbl>     <dbl>     <int>\n1 treatment     168.  0.0181     29.6      307.        74\n\nSimulations\nTrying a simulation pipeline:\n\n\n# first simulate simulation id\ndata_simulations_matching <- tibble(sim_id = 1:250) %>%\n  # then simulate data\n  mutate(data = map(sim_id, ~ generate_data())) %>%\n  # generate caliper\n  crossing(caliper = c(0.1, 1, 0.5, 1.5, 2)) %>%\n  # finally run the matching analysis\n  mutate(results = map2(data, caliper, ~ ps_function(.x, .y)))\n\n# show results\nhead(data_simulations_matching)\n\n# saveRDS(data_simulations_matching, here(\"Outputs/data_simulations_matching.RDS\"))\n\n\n\n\n\ndata_simulations_matching <- readRDS(here(\"Outputs/data_simulations_matching.RDS\"))\n\ndata_results <- data_simulations_matching %>% \n  select(-data) %>%\n  unnest(results)\n\n\n\n\n\ndata_results <- data_results %>%\n  rowwise() %>%\n  mutate(true_captured = ifelse(between(80, conf.low, conf.high) & p.value <= 0.05, 1, 0)) %>%\n  ungroup()\n\n\n\n\n\ndata_results %>%\n  group_by(caliper) %>%\n  summarise(power = sum(true_captured)/n()*100)\n\n\n# A tibble: 5 x 2\n  caliper power\n    <dbl> <dbl>\n1     0.1  20.8\n2     0.5  27.6\n3     1    28.8\n4     1.5  30.8\n5     2    35.2\n\n\n\ndata_results %>%\n  group_by(caliper) %>%\n  summarise(mean_n = mean(n_matched))\n\n\n# A tibble: 5 x 2\n  caliper mean_n\n    <dbl>  <dbl>\n1     0.1   62.1\n2     0.5   65.7\n3     1     72.5\n4     1.5   80.1\n5     2     88.3\n\nDisplaying estimates:\n\n\ndate_means <- data_results %>%\n  group_by(caliper) %>%\n  summarise(mean_estimate = mean(estimate))\n\ndata_results %>%\n  ggplot(., aes(x = estimate)) +\n  geom_density() +\n  geom_vline(xintercept = 80) +\n  geom_vline(data = date_means, aes(xintercept = mean_estimate), colour = \"black\") +\n  facet_wrap(~ caliper) +\n  theme_mediocre()\n\n\n\n\nVincentâs analysis\n\n\nsim_results_matching <- data_results %>% \n  rename(p_value = p.value) %>% \n  select(caliper, estimate, p_value)\n\nsummarise_simulations <- function(data, true_effect = 80) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(caliper) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),\n      bias_sign = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_simulations_matching <- summarise_simulations(sim_results_matching)\n# saveRDS(summary_simulations_matching, here(\"Outputs/summary_simulations_matching.RDS\"))\n\n\n\n\n\nsummary_simulations_matching %>% \n  pivot_longer(cols = c(bias_sign, bias_all), names_to = \"measure\") %>% \n  mutate(\n    measure = ifelse(measure == \"bias_sign\", \"Significant\", \"All\")\n  ) %>% \n  ggplot(aes(x = caliper, y = value, color = measure)) + \n  # geom_point() +\n  geom_line(size = 0.8) +\n  ylim(c(1, NA))\n\n\n\n\n\n\n\n",
      "last_modified": "2021-11-16T16:17:18-05:00"
    },
    {
      "path": "RCT.html",
      "title": "\"Simulations\" RCT",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "LÃ©o Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nPurpose of the document\nReplication of RCTs\n\n\nbody {\ntext-align: justify}\nPurpose of the document\nReplication of RCTs\nWe want to look at replications of RCTs in Development Economics. To do so, we use the list of replication papers put together by Sandip Sukhtankar. We may also end up looking at replications in experimental economics, using the paper by Camerer et al..\nWe gather the list of RCTs that have been replicated in Development Economics.\n\n\nrep_dvpt <- read_dta(here(\"Misc\", \"replication_data_final.dta\"))\n\nrep_dvpt %>% \n  filter((RCT == \"Yes\") & (Replicated == \"Replicated\")) %>% \n  .$ReplicationPaperTitle\n\n\n [1] \"Re-analysis of health and educational impacts of a school-based dewormking programme in western Kenya:Â a statistical replication of a cluster quasi-randomized stepped-wedge trial\"\n [2] \"Broken or Fixed Effects?\"                                                                                                                                                          \n [3] \"A Consistent Variance Estimator for 2SLS When Instruments Identify Different LATEs\"                                                                                                \n [4] \"Experimental Evidence on Returns to Capital and Access to Finance in Mexico\"                                                                                                       \n [5] \"Power to the people?: a replication study of a community-based monitoring programme in Uganda\"                                                                                     \n [6] \"Do Kenyan Teenagers Respond to HIV Risk Information? A Procedural Replication of Dupas (2011)\"                                                                                     \n [7] \" Heterogeneous treatment effects in the low track: Revisiting the Kenyan primary school experiment\"                                                                                \n [8] \"Discrimination in grading: experimental evidence from primary school teachers\"                                                                                                     \n [9] \"Certainty Preference, Random Choice, and Loss Aversion: A Comment on âViolence and Risk Preference: Experimental Evidence from Afghanistanâ\"                                       \n[10] \"Understanding the Impact of Microcredit Expansions: A Bayesian Hierarchical Analysis of 7 Randomised Experiments\"                                                                  \n[11] \"Understanding the Impact of Microcredit Expansions: A Bayesian Hierarchical Analysis of 7 Randomised Experiments\"                                                                  \n[12] \"Understanding the Impact of Microcredit Expansions: A Bayesian Hierarchical Analysis of 7 Randomised Experiments\"                                                                  \n[13] \"Understanding the Impact of Microcredit Expansions: A Bayesian Hierarchical Analysis of 7 Randomised Experiments\"                                                                  \n[14] \"Understanding the Impact of Microcredit Expansions: A Bayesian Hierarchical Analysis of 7 Randomised Experiments\"                                                                  \n[15] \"Extending access to low-cost private schools through vouchers: an alternative interpretation of a two-stage âSchool Choiceâ experiment in India\"                                   \nattr(,\"label\")\n[1] \"ReplicationPaperTitle\"\nattr(,\"format.stata\")\n[1] \"%179s\"\n\nNow look at table S1 in Camerer et al.Â We import it and work on it. Note that some pvalues are strictly smaller than 0.001. As we do not have more information, we set them to 0.001\nWe want to compute the power of the initial analysis if the true effect is in fact equal to the replicationâs.\n\n\nrep_camerer <- read_excel(here(\"Misc\", \"rep_camerer.xlsx\"))\n\nretro_camerer <- rep_camerer %>% \n  mutate(\n    se_original = effect_original/qnorm(1 - pvalue_original), #incorrect\n    se_rep = effect_rep/qnorm(1 - pvalue_rep) #incorrect\n  ) %>% \n  select(A = effect_rep, s = se_original) %>% \n  # select(A, s) %>% \n  pmap_dfr(retrodesign) %>% \n  cbind(rep_camerer) %>% \n  as_tibble()\n\nretro_camerer %>%\n  ggplot() +\n  geom_histogram(aes(x = exaggeration))\n\n\n\n# \n# retro_camerer %>% \n#   ggplot() +\n#   geom_histogram(aes(x = power))\n# \n# retro_camerer %>% \n#   count(exaggeration > 1.5)\n\n1/median(retro_camerer$exaggeration)\n\n\n[1] 0.6197716\n\nmedian(retro_camerer$power)\n\n\n[1] 0.3787278\n\nAnalyzing just one study approximately at random\n\n\n\n\n\n\n",
      "last_modified": "2021-11-16T16:17:27-05:00"
    },
    {
      "path": "RDD.html",
      "title": "Simulations RDD",
      "description": "In this document, we run a simulation exercise to illustrate the existence of a trade-off between Omitted Variable Bias (OVB) and type M error in the context of a Regression Discontinuity Design (RDD).",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "LÃ©o Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModelisation choices\nData generation\nDefining the bandwidth\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\n\n\n\nbody {\ntext-align: justify}\nSummary and intuition\nIn the case of the RDD, the OVB/type M trade-off is mediated by the size of the bandwidth considered in the analysis. The underlying idea is that the smaller the bandwidth, the more comparable units are and therefore the smaller the risk of OVB is. Yet, with a smaller bandwidth, sample size and thus power decrease, increasing type M error.\nAn illustrative example\nTo illustrate this trade-off between OVB and type M error, we consider a standard application of the RD design in economics of education where an grant or additional lessons are assigned based on the score obtained by students on a standardized test. Students with test scores below a given threshold receive the treatment while those above do not. Yet, students far above and far below the threshold may differ along unobserved characteristics such as ability. To limit this bias, the effect of the treatment is estimated by comparing the outcomes of students just below and just above this threshold. This enable to limit disparities in terms of unobserved characteristics.\nThistlewaite and Campbell (1960) introduced the concept of RDD using this type of quasi-experiment. In their paper, they take advantage of a sharp discontinuity in the assignment of an award (a Certificate of Merit) based on qualifying scores at a test. This type of analysis is still used today and many papers leveraging similar methodologies have been published since this seminal work. For instance, Jacob and Lefgren (2004) exploit this type of discontinuity to study the impact of summer school and grade retention programs on test scores. Students who score below a given score are required to attend a summer school and to retake the test. Students who do not pass the second have to repeat the grade.\nModelisation choices\nIn the present analysis, we build our simulations to replicate a similar type of quasi-experiment. In our fictional example, all students scoring below a cutoff \\(C\\) in a qualification test are required to take additional lessons. We want to estimate the effect of these additional lessons on scores on a final test taken by all students a year later.\nWe assume that the final score of student \\(i\\), \\(Final_i\\), is correlated with their qualification score \\(Qual_i\\) and their treatment status \\(T_i\\), ie whether student \\(i\\) received additional lessons or not. We further assume that both qualification and final test scores are affected by studentsâ unobserved ability \\(U_i\\) in a non linear way.\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nFinal test scores are thus defined as follows:\n\\[Final_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} +  \\delta f(U_i) + \\epsilon_{i}\\] Where \\(\\alpha\\) is a constant, \\(f\\) a non linear function and \\(e \\sim \\mathcal{N}(0, \\sigma_{e})\\) noise. The parameter of interest is \\(\\beta\\). Translating this into a potential outcomes framework, we have \\(Final_i(0) = \\alpha + \\gamma Qual_{i} + \\delta f(U_i) + \\epsilon_{i}\\) and \\(Final_i(1) = \\alpha + \\gamma Qual_{i} + \\beta + \\delta f(U_i) + \\epsilon_{i}\\)\n\nTo simplify, we consider the following assumptions:\nFull compliance and a sharp treatment allocation such that \\(T_i = \\mathbb{I}[Qual_{i} < C]\\). All students with a qualification score below the threshold are treated and receive additional lessons. None of the students with a qualification score above the threshold are treated.\nThe unobserved ability affects qualification and final test scores in a cubic way. A large ability has a strong positive impact on test scores. Similarly a particularly low ability strongly impacts test scores negatively. An average ability does not have much impact on test scores. Such a functional form seems realistic. Note that ability creates an OVB only if it has a non linear impact on test scores.\nWe assume constant treatment effects. This assumption is not necessary and our results hold if we consider non-constant treatment effects. We thus may drop this assumption in the future.\nWe assume that the unobserved availability affects the qualification and final score in a similar way and therefore with the same intensity \\(\\delta\\).\nMore precisely, we set:\n\\(N\\) the number of students\n\\(U \\sim \\mathcal{N}(0, \\sigma_u^{2})\\) the unobserved ability.\n\\(Qual_i = H_i + \\delta U_i^{3}\\) where \\(H \\sim \\mathcal{N}(\\mu_h, \\sigma_h^{2})\\). We center the qualification scores such that treated units are below 0 and non treated ones above.\n\\(T_i = \\mathbb{I}[Qual_{i} < q_c]\\) where for now and for simplicity, \\(q_c\\) is a fixed grade threshold given as the quantile in the qualification score distribution.\n\\(e \\sim \\mathcal{N}(0, \\sigma_e^2)\\)\n\\(Final_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} + \\delta U_i^{3} + e_{i}\\)\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\nOnce the fake data is generated, to make things more realistic we consider our data as if it was actual data. We do not take advantage of our knowledge of the data generating process in the estimation procedure. However, we observe both potential outcomes and the unobserved ability. Note that, in a real world setting, one would generally know the value of the threshold (and thus of \\(q_c\\)). Based on that and to simplify the computation of the bandwidth, we store \\(q_c\\).\n\n\ngenerate_data_rdd <- function(N, \n                              sigma_u,\n                              mu_h, \n                              sigma_h, \n                              sigma_e, \n                              alpha, \n                              beta,\n                              gamma,\n                              delta,\n                              q_c) {\n  \n  data <- tibble(id = 1:N) %>% \n    mutate(\n      # qual = rnorm(nrow(.), mu_h, sigma_h),\n      # u = rnorm(nrow(.), 0.5, sigma_u) + qual + 0.3*qual^3,\n      u = rnorm(nrow(.), 0, sigma_u),\n      qual = rnorm(nrow(.), mu_h, sigma_h) + delta*u^2,\n      e = rnorm(nrow(.), 0, sigma_e),\n      # qual_c = qual - quantile(qual, q_c),\n      # treated = qual_c < 0,\n      # threshold = quantile(qual, q_c),\n      treated = qual < quantile(qual, q_c),\n      final0 = alpha + gamma*qual + delta*u^2 + e,\n      final1 = final0 + beta,\n      final = final0 + beta*treated,\n      q_c = q_c\n    )\n  \n  return(data)\n}\n\n\n\nWe set baseline values for the parameters to emulate a somehow realistic observational study in this field. The set of parameters may produce test score outside of the range 0-100 in some iterations but that does not affect the analysis. We add the parameter value for delta separately as we will vary the value later and will reuse the vector baseline_parameters_RDD.\n\n\nbaseline_parameters_RDD <- tibble(\n  N = 1000,\n  sigma_u = 0.5,\n  mu_h = 75,\n  sigma_h = 7,\n  sigma_e = 4,\n  alpha = 20,\n  beta = 1,\n  gamma = 0.7,\n  q_c = 0.5\n)\n\n\n\nHere is an example of data created with our data generating process:\n\nid\nu\nqual\ne\ntreated\nfinal0\nfinal1\nfinal\nq_c\n1\n0.2402616\n68.28862\n4.1062809\nTRUE\n71.96604\n72.96604\n72.96604\n0.5\n2\n-0.1342016\n70.36762\n-3.1817578\nTRUE\n66.09359\n67.09359\n67.09359\n0.5\n3\n-0.0537259\n74.24822\n0.1498218\nFALSE\n72.12647\n73.12647\n72.12647\n0.5\n4\n0.7607821\n86.73584\n-1.5391385\nFALSE\n79.75474\n80.75474\n79.75474\n0.5\n5\n-0.6162482\n68.21111\n-0.3848027\nTRUE\n67.74273\n68.74273\n68.74273\n0.5\n6\n-0.4951851\n60.72755\n-5.2145093\nTRUE\n57.53998\n58.53998\n58.53998\n0.5\n7\n-0.7577642\n80.49600\n-6.1606173\nFALSE\n70.76079\n71.76079\n70.76079\n0.5\n8\n0.0391409\n64.45129\n-1.4890664\nTRUE\n63.62837\n64.62837\n64.62837\n0.5\n9\n0.1660450\n78.73036\n6.4619514\nFALSE\n81.60078\n82.60078\n81.60078\n0.5\n10\n0.2854092\n73.76977\n4.9036697\nFALSE\n76.62396\n77.62396\n76.62396\n0.5\n\nDefining the bandwidth\nIn a RDD, the model is estimated only for observations close enough to the threshold, ie in a given bandwidth. We therefore create a function to define this bandwidth by adding a variable to the data set treated_bw that is equal to NA if the observations is outside of the bandwidth, TRUE if the observation falls in the bandwidth and the student is treated and FALSE if the observation falls in the bandwidth and the student is not treated. The bandwidth parameter bw represents the proportion of units that are in the bandwidth. If bw = 0.1, 10% of the students are in the bandwidth for instance.\n\n\ndefine_bw <- function (data, bw) {\n  data <- data %>% \n    mutate(\n      treated_bw = ifelse(\n        dplyr::between(\n          qual, \n          quantile(qual, unique(q_c) - bw/2), \n          quantile(qual, unique(q_c) + bw/2)\n        ), \n        treated, \n        NA\n      )\n    )\n} \n\n\n\nThe following graph illustrates this process by plotting final test scores against qualification ones depending on the value of treated_bw.\n\n\n\nEstimation\nAfter generating the data, we can run an estimation.\nNote that to run power calculations, we need to have access to the true effects. Therefore, before running the estimation, we write a short function to compute the average treatment effect on the treated (ATET). We will add this information to the estimation results.\n\n\ncompute_true_effect_rdd <- function(data) {\n  treated_data <- data %>% \n    filter(treated) \n  return(mean(treated_data$final1 - treated_data$final0))\n}  \n\n\n\nWe then run the estimation. To do so, we only consider observations within the bandwidth and regress the final test scores on the treatment, the qualification score and their interaction. Note that we include this interaction term to allow more flexibility and to mimic an realistic estimation. Yet, we know that this interaction term does not appear in the DGP. Including it or not do not change the results. Also note that, of course, we do not include the unobserved ability in this model to create an OVB.\n\n\nestimate_rdd <- function(data, bw) {\n  data_in_bw <- data %>% \n    define_bw(bw = bw) %>% \n    filter(!is.na(treated_bw))\n  \n  reg <- lm(\n    data = data_in_bw, \n    formula = final ~ treated + qual\n  ) %>% \n    broom::tidy() %>%\n    filter(term == \"treatedTRUE\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se) %>%\n    mutate(\n      true_effect = compute_true_effect_rdd(data),\n      bw = bw\n    )\n  \n  return(reg)\n}\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_rdd and estimate_rdd. To do so we create the function compute_sim_RDD. This simple function takes as input the various parameters along with a vector of bandwidth sizes, vect_bw. If we want to run several simulations with different bandwidths, we can reuse the same data, hence why we allow to passing a vector of bandwidths and not only one bandwidth. The function returns a table with the estimate of the treatment, its p-value and standard error, the true effect and the bandwidth and intensity of the OVB considered (delta). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_sim_RDD <- function(N,\n                            sigma_u,\n                            mu_h,\n                            sigma_h,\n                            sigma_e,\n                            alpha,\n                            beta,\n                            gamma,\n                            delta,\n                            q_c,\n                            vect_bw) {\n  \n  data <- generate_data_rdd(\n    N = N,\n    sigma_u = sigma_u,\n    mu_h = mu_h,\n    sigma_h = sigma_h,\n    sigma_e = sigma_e,\n    alpha = alpha,\n    beta = beta,\n    gamma = gamma,\n    delta = delta,\n    q_c = q_c\n  ) \n  \n  map_dfr(vect_bw, estimate_rdd, data = data) %>%\n    mutate(delta = delta)\n} \n\n\n\nHere is an example of an output of this function.\n\n# A tibble: 2 x 6\n  estimate p_value    se true_effect    bw delta\n     <dbl>   <dbl> <dbl>       <dbl> <dbl> <dbl>\n1     3.63  0.0264  1.61           1   0.1     1\n2     2.75  0.0172  1.15           1   0.2     1\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_RDD function on each set of parameters. We thus create a table with all the values of the parameters we want to test param_rdd. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\nsimple_parameters_RDD <- tibble(\n  N = 500,\n  sigma_u = 1,\n  mu_h = 0,\n  sigma_h = 1,\n  sigma_e = 0.5,\n  alpha = 1,\n  beta = 1,\n  gamma = 0.7,\n  q_c = 0.5\n)\n\nfixed_parameters_RDD <- simple_parameters_RDD #%>% rbind(...)\n# vect_bw <- seq(0.05, 0.4, 0.05)\nvect_bw <- c(seq(0.05, 0.4, 0.05), seq(0.4, 1, 0.1))\nvect_delta <- c(3)\nn_iter <- 1000\n\nparam_rdd <- fixed_parameters_RDD %>% \n  crossing(delta = vect_delta) %>% \n  mutate(vect_bw = list(vect_bw)) %>% \n  crossing(rep_id = 1:n_iter) %>% \n  select(-rep_id)\n\n\n\nWe then run the simulations by mapping our compute_sim_RDD function on param_rdd.\n\n\ntic()\nsimulations_rdd <- pmap_dfr(param_rdd, compute_sim_RDD)\nbeep()\ntoc()\n\n# saveRDS(simulations_rdd, here(\"Outputs/simulations_rdd.RDS\"))\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta_{RDD}}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta_{RDD}}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_simulations <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(delta, bw) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),\n      bias_sign = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_simulations_rdd <- summarise_simulations(simulations_rdd)\n# saveRDS(summary_simulations_rdd, here(\"Outputs/summary_simulations_rdd.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\n\n\n\n",
      "last_modified": "2021-12-01T18:44:53-05:00"
    }
  ],
  "collections": []
}
