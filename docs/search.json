{
  "articles": [
    {
      "path": "DID.html",
      "title": "Simulations DID / Event study",
      "description": "In this document, we run a simulation exercise to illustrate the loss in power and resulting type M error when the number of events in a Difference In Differences design decreases.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModeling choices\nData generation\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\n\nVarying the proportion of treated\n\n\nbody {\ntext-align: justify}\nSummary and intuition\nIn the case of Event studies and DiD, the OVB/type M trade-off is mediated by the number of events.\nAn illustrative example\n\n\nModeling choices\nTo simplify, I consider the assumptions described below. Of course these assumptions are purely arbitrary and I invite you to play with them. Note that, fixed effects and the covariate are not necessary to the analysis. I only add them to make the analysis more realistic if necessary but I set their baseline values to 0.\nEach individual has fixed characteristics drawn from a normal distribution,\nEach time period presents fixed characteristics also drawn from a normal distribution,\nA unique covariate \\(x\\) drawn from a normal distribution,\nA proportion \\(p_{treat}\\) of individuals are ever treated over the period. Hence, a proportion of \\(1-p_{treat}\\) individuals are never treated over the period. I draw these individual at random. Note that the value of the individual identifiers do not matter here. So I could assume that the non-treated individuals are those with the larger individual ids for instance,\nThe implementation of the treatment can be staggered or not. If it is not staggered, the treatment date is set to be in the middle of the period\nThe treatment can vary along two dimensions, time and individual. Details are given below.\nMore precisely, I set:\n\\(N_i\\) the number of individual\n\\(N_t\\) the number of periods\n\\(\\lambda_i \\sim \\mathcal{N}(\\mu_{IFE}, \\sigma_{IFE}^{2})\\) the fixed effect for individual \\(i\\)\n\\(\\eta_t \\sim \\mathcal{N}(\\mu_{TFE}, \\sigma_{TFE}^{2})\\) the fixed effect for time period \\(t\\)\n\\(x_{it} \\sim \\mathcal{N}(\\mu_{x}, \\sigma_{x}^{2})\\)\n\\(e_{it} \\sim \\mathcal{N}(0, \\sigma_{e}^{2})\\) some noise\n\\(T_{it}\\) represent the treatment allocation, it is equal to one if individual \\(i\\) is treated at time \\(t\\) and 0 otherwise,\n\\(y_{it} = \\alpha + \\beta_{it} T_{it} + \\gamma x_{it} + \\lambda_i + \\eta_t + e_{it}\\) where \\(\\alpha\\) and \\(\\gamma\\) are some constants.\n\\(\\beta_{it}\\) is represents the magnitude of the treatment effect and is linked to the input parameter beta.\nAcross individuals, the treatment can either be:\nhomogeneous: het_indiv == homogeneous, for each individual, the treatment is equal to beta,\nrandom: het_indiv == random, for each individual, the treatment is drawn from \\(\\mathcal{U}(0.5\\beta, 1.5\\beta)\\),\nlarger for those that are treated first: het_indiv == large_first, for each individual, the treatment is equal to \\(N_t - \\beta\\).\n\nAcross time, the effect of the treatment can either be\nconstant: het_time == constant,\nincreasing linearly in time: het_time == linear.\n\n\nI also create a bunch of variables that can be useful:\n\\(InTreatment_i\\) equal to 1 if individual \\(i\\) ever gets treated,\n\\(t^{event}_i\\) equal to the date at which individual \\(i\\) gets treated,\n\\(t^{centered}_i\\) representing the distance in terms of period to the beginning of the treatment for individual \\(i\\),\n\\(Post_{it}\\) equal to 1 if the period \\(t\\) is after the treatment has begun for individual \\(i\\). This variable is only useful for non-staggered treatment allocation,\nData generation\nI write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\n\n\ngenerate_data_DID <- function(N_i,\n                              N_t,\n                              sigma_e,\n                              p_treat,\n                              staggered,\n                              het_indiv,\n                              het_time,\n                              alpha,\n                              beta,\n                              mu_indiv_fe = 0, \n                              sigma_indiv_fe = 0,\n                              mu_time_fe = 0, \n                              sigma_time_fe = 0,\n                              mu_x = 0, \n                              sigma_x = 0,\n                              gamma = 0\n                             ) {\n\n  if (!is.logical(staggered)) {stop(\"staggered must be logical\")} \n  if (!(het_indiv %in% c(\"large_first\", \"random\", \"homogeneous\"))) {\n    stop('het_indiv must be either \"large_first\", \"random\" or \"homogeneous\"')\n  } \n  if (!(het_time %in% c(\"constant\", \"linear\"))) {\n    stop('het_time must be either \"constant\" or \"linear\"')\n  } \n  \n  data <- tibble(indiv = 1:N_i) %>%\n    mutate(in_treatment = (indiv %in% sample(1:N_i, floor(N_i*p_treat)))) %>% \n    crossing(t = 1:N_t) %>%\n    group_by(indiv) %>%\n    mutate(\n      indiv_fe = rnorm(1, mu_indiv_fe, sigma_indiv_fe),\n      t_event = ifelse(staggered, sample(2:(N_t - 1), 1), floor(N_t/2)), \n        #I use 2:(N_t-1) to have a pre and post period\n      t_event = ifelse(in_treatment, t_event, NA),\n      beta_i = case_when(\n        het_indiv == \"large_first\" ~ N_t-t_event,\n        het_indiv == \"random\" ~ runif(1, beta*0.5, beta*1.5), \n        het_indiv == \"homogeneous\" ~ beta\n      ),\n      beta_i = ifelse(is.na(t_event), 0, beta_i)\n    ) %>%\n    ungroup() %>%\n    group_by(t) %>%\n    mutate(time_fe = rnorm(1, mu_time_fe, sigma_time_fe)) %>%\n    ungroup() %>%\n    mutate(\n      post = (t > t_event),\n      treated = in_treatment & post, \n      beta_i = ifelse(\n        het_time == \"linear\" & post & !is.na(t_event),\n        beta_i*(t - t_event), \n        beta_i\n      ),\n      t_centered = t - t_event,\n      x = rnorm(nrow(.), mu_x, sigma_x),\n      e = rnorm(nrow(.), 0, sigma_e),\n      y0 = alpha + gamma * x + indiv_fe + time_fe + e,\n      y1 = y0 + beta_i,\n      y = treated*y1 + (1 - treated)*y0\n    )\n  \n  return(data)\n}\n\n\n\nI set baseline values for the parameters as very standard. These values are arbitrary.\n\n\nbaseline_param_DID <- tibble(\n  N_i = 20,\n  N_t = 50,\n  sigma_e = 1,\n  p_treat = 0.5,\n  staggered = FALSE,\n  het_indiv = \"homogeneous\",\n  het_time = \"constant\",\n  alpha = 1,\n  beta = 1\n)\n\n\n\nHere is an example of data created with the data generating process and baseline parameter values, for 2 individuals and 8 time periods:\n\nindiv\nin_treatment\nt\nindiv_fe\nt_event\nbeta_i\ntime_fe\npost\ntreated\nt_centered\nx\ne\ny0\ny1\ny\n1\nFALSE\n1\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-0.5803581\n0.4196419\n0.4196419\n0.4196419\n1\nFALSE\n2\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-1.2421404\n-0.2421404\n-0.2421404\n-0.2421404\n1\nFALSE\n3\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-0.7072837\n0.2927163\n0.2927163\n0.2927163\n1\nFALSE\n4\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n0.0089650\n1.0089650\n1.0089650\n1.0089650\n1\nFALSE\n5\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-0.6711233\n0.3288767\n0.3288767\n0.3288767\n1\nFALSE\n6\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-0.2100376\n0.7899624\n0.7899624\n0.7899624\n1\nFALSE\n7\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n-0.7383283\n0.2616717\n0.2616717\n0.2616717\n1\nFALSE\n8\n0\nNA\n0\n0\nNA\nFALSE\nNA\n0\n1.2384807\n2.2384807\n2.2384807\n2.2384807\n2\nTRUE\n1\n0\n4\n1\n0\nFALSE\nFALSE\n-3\n0\n-0.2937199\n0.7062801\n1.7062801\n0.7062801\n2\nTRUE\n2\n0\n4\n1\n0\nFALSE\nFALSE\n-2\n0\n1.1832996\n2.1832996\n3.1832996\n2.1832996\n2\nTRUE\n3\n0\n4\n1\n0\nFALSE\nFALSE\n-1\n0\n0.1183179\n1.1183179\n2.1183179\n1.1183179\n2\nTRUE\n4\n0\n4\n1\n0\nFALSE\nFALSE\n0\n0\n-1.2373115\n-0.2373115\n0.7626885\n-0.2373115\n2\nTRUE\n5\n0\n4\n1\n0\nTRUE\nTRUE\n1\n0\n-1.2770120\n-0.2770120\n0.7229880\n0.7229880\n2\nTRUE\n6\n0\n4\n1\n0\nTRUE\nTRUE\n2\n0\n-1.2135986\n-0.2135986\n0.7864014\n0.7864014\n2\nTRUE\n7\n0\n4\n1\n0\nTRUE\nTRUE\n3\n0\n-0.9774353\n0.0225647\n1.0225647\n1.0225647\n2\nTRUE\n8\n0\n4\n1\n0\nTRUE\nTRUE\n4\n0\n0.1826774\n1.1826774\n2.1826774\n2.1826774\n\nLet’s now have a look at different types of treatment and treatment allocations. First, let’s look at treatment allocation mechanisms. The allocation can either be staggered or not, the treatment homogeneous across individual or not and cconstant or not in time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimation\nAfter generating the data, we can run an estimation.\n\n\nestimate_DID <- function(data) {\n  reg <- data %>% \n    mutate(\n      indiv = as.factor(indiv),\n      t = as.factor(t),\n      treated = as.numeric(treated),\n      in_treatment = as.numeric(in_treatment),\n      t_centered = as.factor(t_centered)\n    ) %>% \n    feols(\n      data = ., \n      fml = y ~ treated | indiv + t\n    ) %>% \n    broom::tidy() %>% \n    rename(p_value = p.value, se = std.error) %>% \n    select(-statistic) \n    # suppressMessages() #Warning saying that NA values dropped and \n    # #that one or two factors are removed due to colinearity\n  \n  return(reg)\n}\n\n\n\n\n\nbaseline_param_DID %>% \n  pmap_dfr(generate_data_DID) %>%\n  estimate_DID() %>% \n  slice(1:15) %>% \n  kable()\n\n\nterm\nestimate\nse\np_value\ntreated\n0.9019159\n0.1188596\n4e-07\n\nOne simulation\nWe can now run a simulation, combining generate_data_DID and estimate_DID. To do so we create the function compute_sim_DID. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect, the IV strength and the intensity of the OVB considered (ovb_intensity). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_true_effect_DID <- function(data) {\n  data %>% \n    filter(treated) %>% \n    summarise(true_effect = mean(y1 - y0)) %>% \n    .$true_effect\n}  \n\n\n\n\n\ncompute_sim_DID <- function(N_i,\n                                    N_t,\n                                    sigma_e,\n                                    p_treat,\n                                    staggered,\n                                    het_indiv,\n                                    het_time,\n                                    alpha,\n                                    beta,\n                                    mu_indiv_fe = 0,\n                                    sigma_indiv_fe = 0,\n                                    mu_time_fe = 0,\n                                    sigma_time_fe = 0,\n                                    mu_x = 0,\n                                    sigma_x = 0,\n                                    gamma = 0) {\n  data <- generate_data_DID(\n    N_i = N_i,\n    N_t = N_t,\n    sigma_e = sigma_e,\n    p_treat = p_treat,\n    staggered = staggered,\n    het_indiv = het_indiv,\n    het_time = het_time,\n    alpha = alpha,\n    beta = beta,\n    mu_indiv_fe = mu_indiv_fe,\n    sigma_indiv_fe = sigma_indiv_fe,\n    mu_time_fe = mu_time_fe,\n    sigma_time_fe = sigma_time_fe,\n    mu_x = mu_x,\n    sigma_x = sigma_x,\n    gamma = gamma\n  ) \n  \n  data %>%\n    estimate_DID() %>% \n    mutate(\n      N_i = N_i,\n      N_t = N_t,\n      p_treat = p_treat,\n      true_effect = compute_true_effect_DID(data)\n    )\n} \n\n\n\nHere is an example of an output of this function.\n\nterm\nestimate\nse\np_value\nN_i\nN_t\np_treat\ntrue_effect\ntreated\n1.192916\n0.1332938\n0\n20\n50\n0.5\n1\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_DID function on each set of parameters. We thus create a table with all the values of the parameters we want to test, param_DID. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\n\nWe then run the simulations by mapping our compute_sim_IV function on param_IV.\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta_{i}}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta_{IV}}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_sim_DID <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(p_treat, N_i, N_t) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),\n      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      bias_all_median = median(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_sim_DID <- summarise_sim_DID(sim_DID)\n# saveRDS(summary_sim_DID, here(\"Outputs/summary_sim_DID.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\nVarying the proportion of treated\n\n74.822 sec elapsed\n\n\n\n\n\n",
      "last_modified": "2022-01-12T10:23:08+01:00"
    },
    {
      "path": "experiments.html",
      "title": "Intuition, experimental studies and replication crisis",
      "description": "In this document, we analyze replications of experimental studies to illustrate the consequences of low statistical power in economics.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nLaboratory experimentsAnalyzing all studies\nFocus on one particular study\n\n\n\nbody {\ntext-align: justify}\nLaboratory experiments\nTo illustrate the consequences of low power, we analyze the results from Camerer et al (2016) through the prism of statistical power. The authors of this paper replicated laboratory experiments in economics.\nThey report their replication results, alongside the original results, on the project website, we only need to run their Stata script create_studydetails.do to recover all the infomation.\nWe then compute the power of the initial analysis if the true effect is in fact equal to the replication’s.\n\n\n\nAnalyzing all studies\nWe quickly plot and analyze the results obtained, ie the distribution of the exaggeration ratio and power.\n\n\n\nThe median power would be 0.93%. The median replicated estimates is equal to 0.96 times the original estimate.\nWe then compute the number and proportion of original studies that were statistically significant.\n\nOriginal estimate statistically significant\nNumber\nProportion\nNo\n2\n0.11\nYes\n16\n0.89\n\nWe then do the same thing for the replication studies.\n\nReplication estimate statistically significant\nNumber\nProportion\nNo\n7\n0.39\nYes\n11\n0.61\n\nWe then compute the proportion of original studies that would have adequate power as defined by the customary and arbitrary 80% threshold, still assuming that the true effect is equal to the replication one.\n\nAdequate power\nNumber\nProportion\nNo\n8\n0.44\nYes\n10\n0.56\n\nFocus on one particular study\nHere, we focus on one particular study in order to illustrate in more details the problem at play. We want to simulate what could have yielded replication of the initial study if the true effect was equal to the replication estimate.\nWe select one of the studies, Abeler et al. (2011), that we initially selected at random and draw the graph of interest. The way we calculated the standard error is not perfectly accurate so we use the information available in the replication report.\n\n\n\nIn red is the estimate from the original study and its 95% confidence interval\nThe estimate is significant and has been published. Yet, it is pretty noisy.\nIn blue is the estimate from the replicated study and its 95% confidence interval\nWe notice that this second estimate is both more precise and smaller than the initial one. It still remains noisy\nLet’s assume that the true effect is actually equal to this second estimate (note that this is unlikely)\nWould the design of the initial study be good enough to detect this true effect? ie if we replicated the initial study, could we reject the null of no effect (knowing that the true effect is equal to the replicated estimate)\nIn gray is the estimate form the replicated study but with a standard error equal to the initial study’s (approximately the standard errors that would have been obtained with the design of the initial study)\nThis estimate is non significant. In this instance, we would not have been able to reject the null of no effect\nNow, if we replicate this study 500 times, running 500 lab experiments, in some cases we would get statisitcally significant estimates (the beige dots) and in some others non statistically significant ones (the green dots)\nIf we would have been a bit more lucky, we could have gotten a sample of individuals that would have yielded one of the beige estimates\nNow, we notice that, on average, statistically significant estimates overestimate the true effect by a factor 2.4783975 (average of 0.1959678 while the true effect is 0.0790704). Gelman and Carlin call this inflation factor type M error.\nIn this case, the power is basically the proportion of statistically significant estimates\nIf the study had more power, the sd would be smaller and most estimates would be statistically significative (because there is indeed a non null effect)\nBut since the power is low, if by chance the sample of individuals we get yields a statistically significant estimate, this estimate will overestimate the true effect\n\n\n\n",
      "last_modified": "2022-01-24T14:15:42+01:00"
    },
    {
      "path": "index.html",
      "title": "Unbiased but Inflated Causal Effects",
      "author": [],
      "contents": "\n\n          \n      \n      Unbiased but Inflated Causal Effects\n      \n      \n      Home\n      Intuition\n      \n      \n      Simulations\n       \n      ▾\n      \n      \n      RDD\n      IV\n      Matching\n      DiD / Event study\n      Panoptic issues\n      \n      \n      \n      \n      \n      ☰\n      \n      \n      \n        \n          \n            \n              \n            \n              Unbiased but Inflated Causal Effects\n            \n            \n              \n                \n                    \n                      \n                         GitHub\n                      \n                    \n                  \n                                  \n            \n          \n        \n        \n        \n          \n            Hi and welcome!\n            This website gathers code and additional material for the paper “Unbiased but Inflated Causal Effects” by Vincent Bagilet and Léo Zabrocki.\n            The website is under construction and the analysis is still at a preliminary stage.\n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Unbiased but Inflated Causal Effects\n            \n            \n              \n                \n                                    \n                    \n                       GitHub\n                    \n                  \n                                  \n              \n            \n            \n              Hi and welcome!\n              This website gathers code and additional material for the paper “Unbiased but Inflated Causal Effects” by Vincent Bagilet and Léo Zabrocki.\n              The website is under construction and the analysis is still at a preliminary stage.\n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2022-01-12T10:23:18+01:00"
    },
    {
      "path": "IV.html",
      "title": "Simulations IV",
      "description": "In this document, we run a simulation exercise to illustrate how using an Instrumental Variable (IV) strategy to avoid confounders may create type M error.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModeling choices\nData generation\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\nF-statistic analysis\n\nA basic example\n\n\nbody {\ntext-align: justify}\n\n\n\nSummary and intuition\nIn the case of the IV, the confounding/type M trade-off is mediated by the ‘strength’ of the instrument considered. When the instrument does not explain a lot of the variation in the explanatory variable, the IV can still be successful in avoiding confounders but the power can low, potentially leading to high rates of type M error.\nAn illustrative example\nFor readability and to illustrate this loss in power, we consider an example setting. For this illustration we could consider a large variety of distribution parameter for the variables. We narrow this down to an example setting, considering an analysis of the impact of voter turnout on election results, instrumenting voter turnout with rainfall on the day of the election. Our point should stand in more general settings and the choice of values is mostly for illustration.\nA threat of confounders often arises when analyzing the link between voter turnout and election results. To estimate such an effect causally, one can consider exogeneous shocks to voter turnout such as rainfall. Some potential exclusion restriction problems have been highlighted in this instance but we abstract from them and simulate no exclusion restriction violations here.\nModeling choices\nFor simplicity, we consider several assumptions. These assumptions is not representative of the existing literature but the objective is only to calibrate our simulation with somehow realistic parameter values. Again, this illustration is very simplistic. The high level assumptions are:\nWe abstract from the panel dimension in this analysis and consider only one time period. This is could be considered as looking at the outcomes of a unique election.\nWe only consider the impact of rain on the day of the election.\nWe assume no correlation in rainfall between locations. This could be equivalent to considering only a set of remote locations.\nWe assume simplify the data generating process and thus do not add any exclusion restriction violations.\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nThe DGP for the vote share of let’s say the republican party in location \\(i\\), \\(Share_i\\), is defined as follows:\n\\[Share_{i} = \\alpha + \\beta Turnout_{i} + \\delta u_{i} + e^{(S)}_{i}\\]\nWhere \\(\\alpha\\) is a constant, \\(u\\) represents an unobserved variable and \\(e^{(S)} \\sim \\mathcal{N}(0, \\sigma_{e_S})\\) noise. \\(\\beta\\) is the parameter of interest. We call it ‘treatment effect’.\nThe DGP for the turnout data is as follows:\n\\[Turnout_{i} = \\gamma + \\lambda Rain_{i} + \\eta u_{i} + e^{(T)}_{i}\\]\nWhere \\(\\mu\\) is a constant, \\(Rain\\) is either a continuous variable (amount of rain in location \\(i\\) on the day of the election) or a dummy variable (whether it rained or not) and \\(e^{(T)} \\sim \\mathcal{N}(0, \\sigma_{e_T})\\) noise. We refer to \\(\\lambda\\) as “IV strength”.\nThe impact on voter turnout on election outcome (share of the republican party) is estimated using 2 Stages Least Squares.\nMore precisely, we set:\n\\(N\\) the number of observations\n\\(Rain \\sim \\text{Gamma}(k, \\theta)\\), \\(Rain \\sim \\mathcal{N}(0, \\sigma_{R}^{2})\\) or \\(Rain \\sim \\text{Bernoulli}(p_R)\\) the instrument\n\\(u \\sim \\mathcal{N}(0, \\sigma_{u}^{2})\\) the unobserved variable\n\\(e^{(S)} \\sim \\mathcal{N}(0, \\sigma_{e_S}^{2})\\)\n\\(e^{(T)} \\sim \\mathcal{N}(0, \\sigma_{e_T}^{2})\\)\nWe assume that \\(\\delta = -\\eta\\) for simplicity. There is no actual basis for that and we may change that in the future. The opposite sign is just to get an upward bias, which makes the comparison between OLS and IV easier since the bias and type M go in the same direction.\nIf one abstract from the name of the variable, they can notice that this setting is actually very general.\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\nNote that the parameter type_rain describes whether \\(Rain\\) is a random sample from a normal or Bernoulli distribution. The distributions of rainfall heights can be approximated with a gamma distribution. The Bernoulli distribution is used if one only consider the impact of rain or no rain on voter turnout. A normal distribution does not represent actual rainfall distributions but is added to run these simulations in other contexts than linking rainfall, voter turnout and election outcomes.\ntype_rain can take the values gamma, bernoulli or normal. param_rain represents either \\(\\sigma_R\\) if \\(Rain\\) is normal, \\(p_R\\) if it is Bernoulli or a vector of shape and scale parameters for the gamma distribution.\nNote that, for readability, in this document, we only display the chunks of code that may be important to understand the assumptions behind our simulations and the way we built our simulation. We do not display all the arguably “technical” code, in particular the one used to generate tables and graphs. All this code is however openly available on the GitHub of the project.\n\n\ngenerate_data_IV <- function(N,\n                             type_rain, #\"gamma\", \"normal\" or \"bernoulli\"\n                             param_rain,\n                             sigma_u,\n                             sigma_es,\n                             sigma_et,\n                             alpha,\n                             gamma,\n                             treatment_effect,\n                             iv_strength,\n                             ovb_intensity\n                             ) {\n  \n  if (type_rain == \"bernoulli\") {\n    rain_gen <- rbernoulli(N, param_rain[1])\n  } else if (type_rain == \"normal\") {\n    rain_gen <- rnorm(N, 0, param_rain[1])\n  } else if (type_rain == \"gamma\") {\n    rain_gen <- rgamma(N, shape = param_rain[1], scale = param_rain[2])\n  } else {\n    stop(\"type_rain must be either 'bernoulli', 'gamma' or 'normal'\")\n  }\n  \n  data <- tibble(id = 1:N) %>%\n    mutate(\n      rain = rain_gen,\n      u = rnorm(nrow(.), 0, sigma_u),\n      e_s = rnorm(nrow(.), 0, sigma_es),\n      e_t = rnorm(nrow(.), 0, sigma_et),\n      turnout = gamma + iv_strength*rain - ovb_intensity*u + e_t,\n      share = alpha + treatment_effect*turnout + ovb_intensity*u + e_s\n    )\n\n  return(data)\n}\n\n\n\nWe set baseline values for the parameters to emulate a somehow realistic observational study. We add the parameter value for delta separately as we will vary the value later and will reuse the vector baseline_param_IV.\nWe get “inspiration” for the values of parameters from Fujiwara et al. and Cooperman who replicates a work by Gomez et al..\nWe consider that:\nturnout and vote share are expressed in percent\nFujiwara et al. find that “The trends specifications suggest that 1 millimeter of rainfall decreases turnout by 0.05–0.07 percentage points” and Gomez et al. (and thus Cooperman) find “a county that receives one inch of rainfall on election day is likely to have approximately 1 percentage point lower voter turnout” which is equivalent to a 1mm increase in rainfall is associated with about a 0.04 percentage points decrease in voter turnout.\nFor simplicity in interpretation, when rainfall is not a dummy, it is expressed in centimeters. So, we will consider iv_strength in the range -0.1 and -1\nWe set the standard deviation of the omitted variable bias to be of the order of magnitude of the treatment effect\nWe calibrate the distribution parameters to fit a mix if information from table 1 from both Fujiwara et al. and Cooperman (converting the rainfall into centimeters):\nA gamma distribution represents well the distribution of rainfall. Gamma distribution can have two parameters a shape and a scale. The mean is \\(shape \\times scale\\) and the variance \\(shape \\times scale^{2}\\). The parameters of the distribution of rainfall are comparable in both papers (mean 2.4 and standard deviation 6.6). We solve the system of mean and variance for shape and scale and get 0.13 and 18.\nWe set the intercepts and standard deviations of the errors to produce turnouts and vote shares consistent with the papers. Voter turnout parameters are roughly similar in both papers (mean 58 sd 14) and mean and standard deviation of Republican vote share are given in Fujiwara et al. (mean 55.3 and sd 14.2). We may actually take values from a recent election (eg the last presidential election)\n\nWe thus consider the following parameters:\n\nN\ntype_rain\nparam_rain\nsigma_u\nsigma_es\nsigma_et\nalpha\ngamma\ntreatment_effect\n500\ngamma\n0.13, 18.00\n2\n9\n10\n112\n58\n-1\n\nHere is an example of data created with our data generating process:\n\nid\nrain\nu\ne_s\ne_t\nturnout\nshare\n1\n0.0000356\n0.1778264\n2.468880\n9.484628\n67.66244\n46.62862\n2\n0.0020568\n5.4261525\n2.633470\n-1.502734\n61.92239\n47.28493\n3\n0.0111119\n-4.5789390\n3.041930\n-3.183787\n50.23172\n69.38915\n4\n11.3806220\n2.1706406\n3.338365\n3.755300\n58.23563\n54.93210\n5\n0.0033441\n0.2817406\n-2.092464\n9.164472\n67.44454\n42.18125\n6\n0.0000001\n1.0723378\n-9.724307\n7.522362\n66.59470\n34.60866\n7\n5.6924024\n-0.1687575\n1.717059\n15.021074\n70.00611\n43.87970\n8\n0.6968785\n2.3940394\n1.094538\n15.637214\n75.68281\n35.01768\n9\n0.0023225\n-0.6255069\n6.719902\n-1.578709\n55.79462\n63.55079\n10\n0.0072424\n-1.1992162\n-2.275293\n-2.560434\n54.23673\n56.68719\n\nExploring the distribution of the data\nWe just quickly explore the distribution of the data for a baseline set of parameters. For this, we consider a mid range value for IV strength (-0.5).\n\n\n\nWe also check the standard deviation and means of the parameters:\n\nStatistic\nshare\nturnout\nrain\nMean\n55.08780\n56.71861\n2.342058\nStandard Deviation\n14.35317\n10.69820\n6.483874\n\nEstimation\nAfter generating the data, we can run an estimation. We want to compare the IV and the OLS for different IV strength values. Hence, we need to estimate both an IV and an OLS and return both set of outcomes of interest.\n\n\nestimate_IV <- function(data) {\n  reg_IV <- AER::ivreg(\n    data = data, \n    formula = share ~ turnout | rain\n    ) \n  \n  fstat_IV <- summary(\n    reg_IV, \n    diagnostics = TRUE\n  )$diagnostics[\"Weak instruments\", \"statistic\"]\n  \n  reg_IV <- reg_IV %>% \n    broom::tidy() %>%\n    mutate(\n      model = \"IV\",\n      fstat = fstat_IV\n    )\n  \n  reg_OLS <- lm(\n    data = data, \n    formula = share ~ turnout\n    ) %>% \n    broom::tidy() %>%\n    mutate(\n      model = \"OLS\",\n      fstat = NA\n    )\n  \n  reg_OLS_unbiased <- lm(\n    data = data, \n    formula = share ~ turnout + u\n    ) %>% \n    broom::tidy() %>%\n    mutate(\n      model = \"OLS unbiased\",\n      fstat = NA\n    )\n  \n  reg <- reg_IV %>% \n    rbind(reg_OLS) %>% \n    rbind(reg_OLS_unbiased) %>% \n    filter(term == \"turnout\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se, fstat, model) %>% \n  \n  return(reg)\n}\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_IV and estimate_IV. To do so we create the function compute_sim_IV. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the F-statistic for the IV, the true effect, the IV strength and the intensity of the OVB considered (ovb_intensity). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_sim_IV <- function(N,\n                           type_rain,\n                           param_rain,\n                           sigma_u,\n                           sigma_es,\n                           sigma_et,\n                           alpha,\n                           gamma,\n                           treatment_effect,\n                           iv_strength,\n                           ovb_intensity) {\n  generate_data_IV(\n    N = N,\n    type_rain = type_rain,\n    sigma_u = sigma_u,\n    param_rain = param_rain,\n    sigma_es = sigma_es,\n    sigma_et = sigma_et,\n    alpha = alpha,\n    gamma = gamma,\n    treatment_effect = treatment_effect,\n    iv_strength = iv_strength,\n    ovb_intensity = ovb_intensity\n  ) %>%\n    estimate_IV() %>%\n    mutate(\n      iv_strength = iv_strength,\n      ovb_intensity = ovb_intensity,\n      true_effect = treatment_effect\n    )\n} \n\n\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_IV function on each set of parameters. We thus create a table with all the values of the parameters we want to test, param_IV. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\n\nWe then run the simulations by mapping our compute_sim_IV function on param_IV.\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nWe notice that the OLS is always biased and that the IV is never biased. However, for limited IV strengths, the distribution of the estimates flattens. The smaller the IV strength, the most like it is to get an estimate away from the true value, even though the expected value remains equal to the true effect size. \nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta_{i}}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta_{IV}}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_simulations <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(ovb_intensity, iv_strength, model) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate - true_effect), NA), na.rm = TRUE),\n      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      bias_all_median = median(estimate/true_effect, na.rm = TRUE),\n      median_fstat = mean(fstat, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_sim_IV <- summarise_simulations(sim_IV)\n# saveRDS(summary_sim_IV, here(\"Outputs/summary_sim_IV.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\nOf course, if one considers all estimates, as the IV is unbiased, this issue does not arise. For now, we consider the median because for very low IV strength, we get very extreme values. We need to investigate this further.\n\n\n\nF-statistic analysis\nWe then run some exploratory analysis to see the link between type M and F-stat (under construction).\n\n\n\nWe can notice that there is a correlation between what we call IV strength and the F statistic. When looking in more details at the distribution of F-stats, we can notice that, even for some IV strength, there are a a good chunk of significant estimates with F-stats greater thant the standard but arbitrary threshold of 10.\n\n\n\nWe cannot compute directly the bias of interest against the F-statistic because the F-statistic is not a parameter of the simulations and we do not control them, only the IV strength. To overcome this, we compute the median power by binned F-statistic. However, this is not correct as we end up comparing and pulling together simulations with different parameter values. We still display the graph:\n\n\n\n\n\n\nA basic example\nFor simplicity and for communication purposes, we considered an applied example. However, our results can also hold in general settings. We illustrate this, considering that most of the variables are distributed according to standard normal distributions. We keep the same DAG as it is the classic IV DAG. We also keep the same variable names but one should abstract from their meaning as they do not represent the same measure any longer.\nWe define the following parameters:\n\nN\ntype_rain\nparam_rain\nsigma_u\nsigma_es\nsigma_et\nalpha\ngamma\ntreatment_effect\n250\nnormal\n1\n1\n1\n1\n0\n0\n-0.1\n\nWe then run the whole analysis as before, with an OVB intensity of 0.4 so that it is not extremely large.\n\n421.41 sec elapsed\n\n\nThe results and the overall illustration are comparable with the political economy example.\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2022-01-12T10:30:43+01:00"
    },
    {
      "path": "Matching.html",
      "title": "Matching Simulations",
      "description": "In this document, we run a simulation exercise to illustrate how using a matching procedure to avoid confounding may create type M error.\"\n",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://lzabrocki.github.io/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nLoading Packages\nData Generating ProcedureGeneral Approach\nFunction to Generate the Data\nEDA for One Dataset\n\nOutcome Regression Analysis\nMatching ProcedurePropensity Score Function\nSimulations\n\n\n\nbody {\ntext-align: justify}\nIn this document, we show through simulations how matching procedures targeting the common support of the data could be under-powered and lead to inflated statistically significant estimates. We illustrate this issue with fake-data similar to those from non-randomized labor training programs. In this case, treated individuals self-select themselves to get the treatment and may therefore have different characteristics from individuals who do not enroll in the program. To estimate the causal effect of the treatment on treated, researchers can rely on matching, which is a pre-processing technique to approximate an hypothetical randomized experiment. The method has three main advantages:\nContrary to outcome regression approach, it does not directly model the relationship between the treatment of interest and the outcome\nIt adjusts non-parametrically for observed confounders\nBy pruning control units not similar to treated units, matching help reveal the common support of the data upon which researchers can more confidently draw their inference. Statistical models used to analyze the resulting data will suffer less from extrapolation bias.\nThe method however relies on the strong assumption that no unmeasured confounders exist and, depending on its stringency, may discard many units from the analysis. This second limit could result in a lack of statistical power to estimate the average treatment on the treated. If journal editors and researchers favor studies based on the their statistical significance, under-powered studies based on matching procedures could lead to estimates that are too large.\nLoading Packages\nWe first load the required packages to set-up the simulations:\n\n\n# load required packages\nlibrary(knitr) # for creating the R Markdown document\nlibrary(tidyverse) # for data manipulation and visualization\nlibrary(MatchIt) # for matching analysis\nlibrary(lmtest) # for modifying regression standard errors\nlibrary(sandwich) # for robust and cluster robust standard errors\nlibrary(DT) # for displaying the data as tables\nlibrary(mediocrethemes) # vincent's custom ggplot2 theme\nlibrary(tictoc) # for measuring running time\nlibrary(beepr) # for making a sound when the code is done\nlibrary(here) # for paths management\n\n# set ggplot theme\nset_mediocre_all(pal = \"coty\")\n\n\n\nData Generating Procedure\nGeneral Approach\nTo illustrate how matching procedures could be particularly sensitive to statistically significant estimates that inflated, we simulate fake-data from a non-randomized labor training program targeting young individuals. Below are the main steps of the simulation:\nWe first create the units identifiers (id). Each unit is an individual.\nMany simulations found in the applied statistics literature test the performance on matching algorithms by first simulating covariates and then simulating the true but unknown propensity to be treated of units. Our goal here is different as we do not want to test the performance of various matching algorithms but rather illustrate how a lack of common overlap in propensity scores can result in a loss statistical power. We therefore first assign a fraction of individual (p_treat) to the treatment and then simulate the true propensity score variable true_ps for treated and control units. For treated units, we draw the propensity scores from a normal distribution \\(N(\\mu_{T}, \\sigma_{T})\\) and for control units, from a normal distribution \\(N(\\mu_{C}, \\sigma_{C})\\).\nOnce the the true propensity scores are created, we define the potential outcomes of each individual. Here, potential outcomes represent the income (in euros) of the individuals if they undertake the training program or not. The potential outcome without treatment adoption, Y(0), is simulated using the following equation: \\(Y_{0} = Wage \\times True Propensity Score + N(\\mu_{noise}, \\sigma_{noise})\\). This equation makes the potential outcomes Y(0) partly different for treated and control units.\nWe finally simulate the potential outcomes when individuals benefit from the training program. The average treatment effect on the treated (ATT) was set to a constant effect of +100 euros. The average treatment effect on the control (ATC) was set to a constant effect of +50. The constant treatment effect assumption is made to simply the illustration of the issue we are interested in. In our simulations, when we make the propensity score matching more stringent, not all treated units will be matched to similar control units. The causal estimand will no longer be the ATT and we should compute it true effect for each iteration if the causal effect was not constant.\nFunction to Generate the Data\nWe display below the code for the function generate_data_matching() which creates the required dataset. Its arguments are the desired sample size (sample_size), the proportion of treated units (p_treat), the mean and standard deviation of the propensity score distributions of treated and control units (mu_t, sigma_t, mu_c, sigma_c), the baseline wage wage, the noise of the equation for simulating the Y(0) (mu_noise, sigma_noise), the ATC and ATT (atc, att).\n\n\ngenerate_data_matching <- function(sample_size,\n                                    p_treat,\n                                    mu_t,\n                                    sigma_t,\n                                    mu_c,\n                                    sigma_c,\n                                    wage,\n                                    mu_noise,\n                                    sigma_noise,\n                                    atc,\n                                    att) {\n  data <- tibble(id = 1:sample_size) %>%\n    mutate(\n      # assign treatment status\n      treatment = rbinom(n = sample_size, size = 1, prob = p_treat),\n      # create the propensity score distributions\n      true_ps = ifelse(\n        treatment == 0,\n        rnorm(n(), mean = mu_c, sd = sigma_c),\n        rnorm(n(), mean = mu_t, sd = sigma_t)\n      ),\n      # make sure that the propensity score is between 0 and 1\n      true_ps = case_when(true_ps > 1 ~ 1,\n                          true_ps < 0 ~ 0,\n                          true_ps >= 0 & true_ps <= 1 ~ true_ps),\n      # generate the potential outcomes\n      y_0 = wage * true_ps + rnorm(n(), mean = 300, sd = 200),\n      y_0 = y_0 %>% round(., 0),\n      y_1 = ifelse(treatment == 1,\n                   y_0 + att,\n                   y_0 + atc),\n      # generate observed outcomes\n      y_obs = ifelse(treatment == 1, y_1, y_0) %>% round(., 0)\n    )\n  return(data)\n}\n\n\n\nIn our simulations, we use the following parameters to create a data of 300 units, with about 25% being treated, and with a lack of common overlap in the propensity scores for treated and control units:\n\nsample_size\np_treat\nmu_t\nsigma_t\nmu_c\nsigma_c\nwage\nmu_noise\nsigma_noise\natc\natt\n300\n0.25\n0.5\n0.1\n0.3\n0.1\n2000\n300\n200\n50\n100\n\nEDA for One Dataset\nWe run one iteration of the function generate_data_matching() to explore the resulting data with 500 units:\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\",\"273\",\"274\",\"275\",\"276\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"286\",\"287\",\"288\",\"289\",\"290\",\"291\",\"292\",\"293\",\"294\",\"295\",\"296\",\"297\",\"298\",\"299\",\"300\"],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300],[0,1,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,1,1,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,1,0,1,1,1,0,0,0,0,0,0,1,1,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,1,1,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,1,1,0,1,0,0,1,0,0,0,1,0,0,0,0,0],[0.34,0.58,0.44,0.39,0.45,0.44,0.17,0.24,0.49,0.48,0.41,0.11,0.42,0.29,0.3,0.38,0.38,0.22,0.5,0.34,0.28,0.56,0.29,0.34,0.27,0.37,0.19,0.33,0.06,0.1,0.33,0.29,0.47,0.44,0.38,0.55,0.34,0.69,0.21,0.68,0.46,0.37,0.34,0.42,0.16,0.32,0.35,0.31,0.33,0.33,0.28,0.31,0.17,0.37,0.76,0.2,0.16,0.42,0.4,0.64,0.22,0.47,0.58,0.44,0.32,0.5,0.3,0.39,0.2,0.49,0.34,0.45,0.25,0.3,0.21,0.16,0.22,0.34,0.17,0.24,0.23,0.24,0.59,0.15,0.2,0.27,0.29,0.26,0.46,0.37,0.24,0.41,0.42,0.39,0.32,0.34,0.49,0.59,0.39,0.45,0.21,0.31,0.31,0.26,0.32,0.3,0.24,0.21,0.21,0.2,0.62,0.36,0.44,0.41,0.43,0.27,0.37,0.36,0.36,0.26,0.39,0.38,0.45,0.27,0.5,0.22,0.16,0.36,0.55,0.36,0.36,0.34,0.43,0.34,0.45,0.17,0.28,0.36,0.3,0.15,0.39,0.29,0.49,0.32,0.19,0.43,0.28,0.35,0.62,0.54,0.54,0.46,0.31,0.31,0.22,0.25,0.51,0.59,0.65,0.25,0.35,0.26,0.16,0.46,0.36,0.51,0.39,0.32,0.5,0.27,0.26,0.25,0.29,0.4,0.18,0.38,0.29,0.33,0.41,0.36,0.2,0.56,0.46,0.4,0.17,0.23,0.33,0.25,0.36,0.45,0.57,0.31,0.22,0.38,0.21,0.28,0.32,0.34,0.39,0.38,0.48,0.19,0.49,0.61,0.4,0.3,0.39,0.3,0.3,0.26,0.42,0.3,0.4,0.37,0.44,0.39,0.12,0.44,0.2,0.33,0.35,0.23,0.4,0.51,0.3,0.5,0.27,0.29,0.24,0.14,0.16,0.32,0.32,0.33,0.21,0.18,0.35,0.52,0.45,0.23,0.46,0.56,0.41,0.31,0.4,0.16,0.32,0.19,0.49,0.31,0.54,0.25,0.27,0.16,0.27,0.42,0.3,0.4,0.24,0.34,0.6,0.47,0.44,0.55,0.24,0.35,0.39,0.3,0.23,0.39,0.2,0.24,0.22,0.09,0.27,0.41,0.26,0.58,0.19,0.53,0.28,0.25,0.31,0.52,0.47,0.31,0.55,0.56,0.04,0.36,0.37,0.43,0.21,0.25,0.47,0.4,0.3,0.24,0.29,0.42],[1127,1595,1043,1254,1085,1137,480,729,1450,1380,1149,655,1214,1010,998,1151,1210,497,1410,995,393,1287,699,1314,724,964,564,690,370,194,1011,764,1289,1058,908,1370,1345,1524,468,1805,1242,1091,1094,929,148,1020,414,1325,970,536,860,1168,264,1527,1864,771,752,1209,1282,1711,778,1178,1396,1206,1042,1310,494,1002,769,1055,924,1390,782,736,633,1026,744,907,777,515,813,896,1487,645,661,1223,690,1038,1107,1251,408,1201,1297,1445,745,971,1138,1782,911,1098,482,1028,886,1011,1170,1200,880,669,934,664,1517,673,1085,863,1304,829,1234,1370,1201,1136,660,1231,1167,921,1644,1169,482,525,1326,1201,1288,917,904,1037,1153,536,528,1149,961,401,711,968,1457,579,556,1262,555,704,1581,1651,1186,1529,806,1506,767,787,1141,1529,1611,512,1083,755,381,1543,1167,1566,870,1221,1736,734,661,658,824,765,581,1055,1033,831,1487,1047,772,1391,1071,1174,388,579,961,954,927,1457,1801,880,866,994,872,1022,1261,1005,1186,945,1418,658,1217,1673,1056,1146,1011,795,1170,794,946,618,1323,1135,949,857,535,1234,920,1159,748,536,1113,1342,716,1621,503,529,756,1090,1068,958,1063,800,756,1009,1141,1588,1012,704,1402,1557,850,856,1002,546,785,649,806,902,1519,406,978,648,974,1405,781,1045,1313,782,1518,1431,1256,1182,775,1265,1289,1226,850,1392,688,1042,723,452,796,986,1034,1439,525,1329,1019,1095,1155,1748,1158,696,1450,1542,413,1183,1177,1345,627,1121,1031,1292,831,688,1183,1286],[1177,1695,1143,1354,1135,1187,530,779,1550,1430,1249,705,1264,1060,1048,1201,1260,547,1460,1045,443,1387,749,1364,824,1064,614,740,420,244,1061,814,1339,1108,1008,1470,1395,1624,518,1905,1292,1141,1144,979,198,1070,464,1375,1020,586,910,1218,314,1577,1964,821,802,1259,1332,1811,828,1278,1496,1306,1092,1410,544,1052,819,1155,1024,1440,832,786,683,1076,794,957,827,565,863,946,1587,695,711,1273,740,1088,1157,1301,458,1251,1397,1495,795,1021,1238,1882,961,1198,532,1128,936,1061,1220,1250,930,719,984,714,1617,723,1185,913,1354,879,1284,1420,1251,1186,710,1281,1267,971,1744,1219,532,575,1426,1301,1338,967,1004,1087,1203,586,578,1199,1011,451,811,1018,1557,629,606,1362,605,804,1681,1751,1236,1579,856,1556,817,837,1241,1629,1711,562,1133,805,431,1643,1217,1616,920,1271,1836,784,711,708,874,865,631,1105,1083,881,1587,1097,822,1441,1121,1224,438,629,1011,1004,977,1507,1901,930,916,1044,922,1072,1311,1055,1286,995,1518,708,1317,1773,1106,1196,1111,845,1220,844,996,668,1423,1185,999,957,585,1284,970,1209,798,586,1163,1442,816,1671,553,579,806,1140,1118,1008,1113,850,806,1059,1191,1688,1062,754,1452,1657,900,906,1052,596,835,699,856,952,1619,456,1028,698,1024,1455,831,1095,1363,882,1618,1481,1356,1282,825,1315,1339,1276,950,1442,738,1092,773,502,846,1036,1084,1539,575,1429,1069,1145,1205,1848,1258,796,1500,1642,463,1233,1277,1395,677,1171,1131,1342,881,738,1233,1336],[1127,1695,1143,1354,1085,1137,480,729,1550,1380,1249,655,1214,1010,998,1151,1210,497,1410,995,393,1387,699,1314,824,1064,564,690,370,194,1011,764,1289,1058,1008,1470,1345,1624,468,1905,1242,1091,1094,929,148,1020,414,1325,970,536,860,1168,264,1527,1964,771,752,1209,1282,1811,778,1278,1496,1306,1042,1410,494,1002,769,1155,1024,1390,782,736,633,1026,744,907,777,515,813,896,1587,645,661,1223,690,1038,1107,1251,408,1201,1397,1445,745,971,1238,1882,911,1198,482,1128,886,1011,1170,1200,880,669,934,664,1617,673,1185,863,1304,829,1234,1370,1201,1136,660,1231,1267,921,1744,1169,482,525,1426,1301,1288,917,1004,1037,1153,536,528,1149,961,401,811,968,1557,579,556,1362,555,804,1681,1751,1186,1529,806,1506,767,787,1241,1629,1711,512,1083,755,381,1643,1167,1566,870,1221,1836,734,661,658,824,865,581,1055,1033,831,1587,1047,772,1391,1071,1174,388,579,961,954,927,1457,1901,880,866,994,872,1022,1261,1005,1286,945,1518,658,1317,1773,1056,1146,1111,795,1170,794,946,618,1423,1135,949,957,535,1234,920,1159,748,536,1113,1442,816,1621,503,529,756,1090,1068,958,1063,800,756,1009,1141,1688,1012,704,1402,1657,850,856,1002,546,785,649,806,902,1619,406,978,648,974,1405,781,1045,1313,882,1618,1431,1356,1282,775,1265,1289,1226,950,1392,688,1042,723,452,796,986,1034,1539,525,1429,1019,1095,1155,1848,1258,796,1450,1642,413,1183,1277,1345,627,1121,1131,1292,831,688,1183,1286]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>id<\\/th>\\n      <th>treatment<\\/th>\\n      <th>true_ps<\\/th>\\n      <th>y_0<\\/th>\\n      <th>y_1<\\/th>\\n      <th>y_obs<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nAbout 24% of units are treated. We display below the true propensity score distributions by treatment status:\n\n\n\nThe distribution of potential outcomes Y(0) should be different across the two groups:\n\n\n\nWe can also see how the observed revenue is distributed across the two groups:\n\n\n\nWe can check whether the ATT and ATC were correctly simulated. The ATT is computed such as:\n\n\n# checking att\nmean(data$y_1[data$treatment==1]) - mean(data$y_0[data$treatment==1])\n\n\n[1] 100\n\nand the ATC:\n\n\n# checking atc\nmean(data$y_1[data$treatment==0]) - mean(data$y_0[data$treatment==0])\n\n\n[1] 50\n\nThe data have been correctly simulated.\nOutcome Regression Analysis\nBefore moving to the matching procedure, readers might be interested to see what would happen if we analyze our simulated datasets with a simple outcome regression model? Would we recover the true answer?\nWe first create a regression function to run a simple regression model where we simply regress the observed income on the treatment indicator:\n\n\noutcome_regression <- function(data) {\n  data %>%\n    lm(\n      y_obs ~ treatment,\n      data = .\n    ) %>%\n    broom::tidy(., conf.int = TRUE) %>%\n    filter(term == \"treatment\") %>%\n    select(estimate, p.value, conf.low, conf.high)\n}\n\n\n\nWe then simulate 1000 datasets of 300 units and run the regression model:\n\n\n# first simulate simulation id\ndata_simulations <- tibble(sim_id = 1:1000) %>%\n  # then simulate data\n  mutate(data = map(\n    sim_id,\n    ~ pmap_dfr(baseline_param_match, generate_data_matching)\n    )\n  ) %>%\n  # finally run the reg analysis\n  mutate(results = map(data, ~ outcome_regression(.)))\n\n# unnest the results\ndata_simulations <- data_simulations %>%\n  select(-data) %>%\n  unnest(results)\n\n\n\nWe plot the distribution of estimates:\n\n\ndata_simulations %>%\n  ggplot(., aes(x = estimate)) +\n  geom_density(colour = NA) +\n  geom_vline(xintercept = mean(data_simulations$estimate)) +\n  annotate(\"text\", x = 540, y = 0.009, label = \"Mean of Estimates\") + \n  annotate(\"text\", x = 125, y = 0.009, label = \"True ATT\") + \n  geom_vline(xintercept = 100, colour = \"#EAA95C\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +\n  ggtitle(\"Distribution of Outcome Regression Estimates\") + \n  xlab(\"Estimates of Treatment Effect (in euros)\") + ylab(\"Density\") +\n  labs(fill = 'Status:')\n\n\n\n\nWith an outcome regression analysis, the average of estimates is equal to 500! The outcome regression analysis suffers from extrapolation bias.\nMatching Procedure\nWe now implement a simple matching where:\nWe implement below a propensity score matching procedure where:\nEach treated is matched to its most similar control unit. This is a 1:1 nearest neighbor matching without replacement.\nThe distance metric used for the matching is the propensity score.\nWe vary the matching distance (the caliper), which is expressed in standard deviation of the propensity score distribution. Lower value of the caliper implies a stricter matching procedure.\nPropensity Score Function\nWe display below the code for the function ps_function() which runs the matching procedure. It takes two inputs: (i) a dataset and (ii) the value of the caliper.\n\n\n# propensity score analysis function\nps_function <- function(data, caliper_value) {\n  # implements the propensisty score matching\n  matching_results <- matchit(\n    treatment ~ id,\n    distance = data$true_ps,\n    caliper = caliper_value,\n    data = data\n  )\n  # retrieves the matched dataset\n  data_matched <- match.data(matching_results)\n  \n  # computes the proportion of matched treated units\n  proportion_matched <-\n    sum(data_matched$treatment) / sum(data$treatment) * 100\n  \n  # compute the true causal effect for matched units\n  true_effect <-\n    mean(data_matched$y_1[data_matched$treatment == 1]) - mean(data_matched$y_0[data_matched$treatment == 1])\n  \n  # estimate the causal effect with a simple regression model\n  model_fit <- lm(y_obs ~ treatment,\n                  data = data_matched,\n                  weights = weights)\n  \n  ps_att <-\n    broom::tidy(coeftest(model_fit, vcov. = vcovCL, cluster = ~ subclass),\n                conf.int = TRUE) %>%\n    filter(term == \"treatment\") %>%\n    select(term, estimate, p.value, conf.low, conf.high)\n  \n  # return relevant statistics\n  return(\n    bind_cols(\n      ps_att,\n      proportion_matched = proportion_matched,\n      true_effect = true_effect\n    )\n  )\n}\n\n\n\nWe run the function on the dataset we previously created:\n\n\n# testing the function\nps_function(data, caliper = 0.5) %>% \n  mutate_at(vars(-term), ~ round(., 1)) %>% \n  kable(., align = c(\"l\", rep('c', 6)))\n\n\nterm\nestimate\np.value\nconf.low\nconf.high\nproportion_matched\ntrue_effect\ntreatment\n152.5\n0\n81.7\n223.3\n77.8\n100\n\nThe function returns the estimate for the ATT, the associated \\(p\\)-value and 95% confidence interval, the proportion of matched treated unit and the true value of the ATT.\nSimulations\nWe implement Monte-Carlo 300 simulations for different values of the caliper (it currently takes 24 minutes to run on a laptop computer):\n\n\ndata_simulations <- tibble(sim_id = 1:300) %>%\n  # then simulate data\n  mutate(data = map(\n    sim_id,\n    ~ pmap_dfr(baseline_param_match, generate_data_matching)\n  )) %>%\n  # generate caliper\n  crossing(caliper = c(seq(\n    from = 1, to = 100, by = 1\n  ) / 100)) %>%\n  # finally run the matching analysis\n  mutate(results = map2(data, caliper, ~ ps_function(.x, .y)))\n\ndata_simulations <- data_simulations %>%\n  select(-data) %>%\n  unnest(results)\n\n# saveRDS(data_simulations, here(\"Outputs/sim_matching.RDS\"))\n\n\n\nOnce the simulations have been run, we compute the summary statistics using the summarise_simulations() function. We denote \\(\\tau\\) the true value of the causal estimand and \\(\\widehat{\\tau}\\) its estimate. To illastrue the consequences of a loss of statistical power with lower values of the caliper, we compare \\(\\mathbb{E}\\left[\\left|\\frac{\\widehat{tau}}{\\tau}\\right|\\right]\\) and \\(\\mathbb{E}\\left[\\left|\\frac{\\widehat{\\tau}}{\\tau}\\right| | signif \\right]\\). The first term represents the bias and the second term represents the type M error.\n\n\n# load simulation results\nsim_matching <- readRDS(here(\"Outputs/sim_matching.RDS\"))\n\n# function to compute power, type m error and bias\nsummarise_sim_matching <- function(data) {\n  data %>%\n    mutate(significant = (p.value <= 0.05)) %>% \n    group_by(caliper) %>%\n    summarise(\n      proportion_matched = mean(proportion_matched),\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),\n      bias_all = mean(abs(estimate/true_effect), na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\n\n\nWe apply the function to data_simulations and plot the results:\n\n\n\n\n\n\nThe blue line indicates the inflation of all estimates, regardless of their statistical significance. As the value of the caliper increases, estimates are more biased: this is due to the fact that we are comparing units that are less similar. The yellow line represents the inflation of statistically significant estimates at the 5% level. We clearly see with this line the danger of editorial policies biased toward small \\(p\\)-values: with low values of the caliper, statistically significant estimates are inflated!\n\n\n\nWhy statistically significant estimates are inflated with low values of the caliper? The figure below gives the answer: as the value of the caliper decreases, the sample size of the matched sample is reduced and thereby the statistical power shrinks. Only large estimates can be statistically significant but these estimates are misleading.\n\n\n\n\n\n\n",
      "last_modified": "2022-01-12T10:31:13+01:00"
    },
    {
      "path": "panoptic-issues.html",
      "title": "Simulations for panoptic issues",
      "description": "In this document, we run a simulation exercise to illustrate panoptic type M issues arising in causal and non-causal identification studies.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModeling choices\nData generation\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the results (proportion treated units)Quick exploration\nComputing bias and type M\nTHE graph\n\nVarying the number of clusters\n\n\nbody {\ntext-align: justify}\n\n\n\nSummary and intuition\nPower and thus type M error can also be affected by aspects that are not proper to one particular identification strategy. In this document we investigate the impact of different factors:\nThe proportion of unit treated (at number of observations constant). We all know that power is maximized when the proportion of units treated is 0.5. The further away we are from this value, the smaller the power and the larger the exaggeration ratio. (I am not certain, I need to check the formula but) The intuition is that a limited number of treated, or control, limits the variation used to estimate the effect.\nLimiting the variation in y (small number of counts in y)\nThe number of clusters. Considering a small number of clusters is to some extent comparable to considering a small number of units and thus partly acts as limiting the number of observations, thus decreasing power and increasing the exaggeration ratio.\nAn illustrative example\nFor readability and to illustrate this loss in power, we consider an example setting. For this illustration we could consider a large variety of distribution parameter for the variables. We narrow this down to an example setting, considering an analysis of the impact of voter turnout on election results, instrumenting voter turnout with rainfall on the day of the election. Our point should stand in more general settings and the choice of values is mostly for illustration.\nModeling choices\nFor simplicity, we consider several assumptions. These assumptions is not representative of the existing literature but the objective is only to calibrate our simulation with somehow realistic parameter values. Again, this illustration is very simplistic. The high level assumptions are:\nFor now, we consider only standard normal distributions\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nWe thus assume that the outcome \\(y\\) for individual \\(i\\) is defined as follows:\n\\[y_{i} = \\alpha + \\beta T_{i} + \\delta u_{i} + \\epsilon_{i}\\]\nWhere \\(\\alpha\\) is a constant, \\(T_{i}\\) a dummy equal to 1 if individual \\(i\\) is treated and if they are not treated, \\(\\beta\\) the treatment effect size, \\(u\\) an unobserved variable, \\(\\delta\\) the intensity of the OVB and \\(\\epsilon\\) an error term.\nMore precisely, we set:\n\\(N\\) the number of observations \n\\(T_i = \\mathbb{1}\\{i\\text{ is treated}\\}\\) the treatment dummy. \\(p_T\\) represents the proportion of treated units.\n\\(u \\sim \\mathcal{N}(0, \\sigma_{u}^{2})\\) the unobserved variable\n\\(\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})\\) the error term\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\nNote that, for readability, in this document, we only display the chunks of code that may be important to understand the assumptions behind our simulations and the way we built our simulation. We do not display all the arguably “technical” code, in particular the one used to generate tables and graphs. All this code is however openly available on the GitHub of the project.\n\n\ngenerate_data_panoptic <- function(N,\n                                   p_treat,\n                                   sigma_u,\n                                   sigma_e,\n                                   alpha,\n                                   treatment_effect, #beta\n                                   ovb_intensity #delta\n                                   ) {\n  tibble(id = 1:N) %>%\n    mutate(\n      # treated = rbernoulli(N, p_treat),\n      treated = (id %in% sample(1:N, p_treat*N)),\n      u = rnorm(nrow(.), 0, sigma_u),\n      e = rnorm(nrow(.), 0, sigma_e),\n      y = alpha + treatment_effect*treated + ovb_intensity*u + e\n    )\n}\n\n\n\n\nFor now, we consider very simple baseline parameters, mostly standard normal distributions:\n\nN\np_treat\nsigma_u\nsigma_e\nalpha\ntreatment_effect\novb_intensity\n500\n0.5\n1\n1\n1\n0.4\n1\n\nHere is an example of data created with our data generating process:\n\nid\ntreated\nu\ne\ny\n1\nFALSE\n-1.5037738\n-0.2660837\n-0.7698575\n2\nTRUE\n-0.5704614\n-1.1301156\n-0.3005771\n3\nFALSE\n1.9150797\n-3.0629174\n-0.1478377\n4\nTRUE\n-0.6688884\n1.2720665\n2.0031781\n5\nFALSE\n0.7568319\n-1.1508101\n0.6060218\n6\nFALSE\n0.7463943\n-0.3617895\n1.3846048\n7\nFALSE\n2.1819793\n-1.5699271\n1.6120522\n8\nTRUE\n0.6580293\n-1.0616906\n0.9963387\n9\nTRUE\n-1.1840848\n0.0174934\n0.2334087\n10\nTRUE\n0.3395678\n0.0833465\n1.8229143\n\nEstimation\nAfter generating the data, we can run an estimation. We want to be able to run the estimation for different numbers of clusters. We create artificial clusters, based on the individual identification numbers id. When we do not wish to cluster the standard errors, we simply set the cluster variable to be equal to the id so that the “clustering” is at the individual level. Note that we use the function lm_robust from the estimatr package to compute this clustering.\n\n\nestimate_panoptic <- function(data, n_clusters = NA) {\n  n_clusters_mod <- ifelse(is.na(n_clusters), nrow(data), n_clusters)\n  \n  data %>%\n    mutate(cluster = cut_number(id, n_clusters_mod, labels = FALSE)) %>%\n    lm(data = ., formula = y ~ treated) %>%\n    coeftest(vcov = vcovCL, cluster = ~cluster) %>% \n    broom::tidy() %>%\n    filter(term == \"treatedTRUE\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se) %>%\n    mutate(n_clusters = n_clusters)\n}\n\n# estimate_panoptic_1 <- function(data, n_clusters = NA) {\n#   n_clusters_mod <- ifelse(is.na(n_clusters), nrow(data), n_clusters)\n#   \n#   data %>%\n#     mutate(cluster = cut_number(id, n_clusters_mod, labels = FALSE)) %>%\n#     estimatr::lm_robust(\n#       data = .,\n#       formula = y ~ treated,\n#       clusters = cluster\n#     ) %>%\n#     broom::tidy() %>%\n#     as_tibble() %>% \n#     filter(term == \"treatedTRUE\") %>%\n#     rename(p_value = p.value, se = std.error) %>%\n#     select(estimate, p_value, se) %>%\n#     mutate(n_clusters = n_clusters)\n# }\n\n# estimate_panoptic_2 <- function(data, n_clusters = NA) {\n# \n#   if (is.na(n_clusters)) {\n#     reg <- lm(\n#       data = data,\n#       formula = y ~ treated\n#     )\n#   } else {\n#     reg <- data %>%\n#       mutate(cluster = cut_number(id, n_clusters, labels = FALSE)) %>%\n#       estimatr::lm_robust(\n#         data = .,\n#         formula = y ~ treated,\n#         clusters = cluster\n#       )\n#   }\n# \n#   reg %>%\n#     broom::tidy() %>%\n#     as_tibble() %>% \n#     filter(term == \"treatedTRUE\") %>%\n#     rename(p_value = p.value, se = std.error) %>%\n#     select(estimate, p_value, se) %>%\n#     mutate(n_clusters = n_clusters)\n# }\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_panoptic and estimate_panoptic. To do so we create the function compute_sim_panoptic. This simple function takes as input the various parameters. It returns a table with the estimate of the treatment, its p-value and standard error, the true effect. Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_sim_panoptic <- function(N,\n                                 p_treat,\n                                 sigma_u,\n                                 sigma_e,\n                                 alpha,\n                                 treatment_effect,\n                                 ovb_intensity,\n                                 n_clusters = NA) {\n  generate_data_panoptic(\n    N = N,\n    p_treat = p_treat,\n    sigma_u = sigma_u,\n    sigma_e = sigma_e,\n    alpha = alpha,\n    treatment_effect = treatment_effect,\n    ovb_intensity = ovb_intensity\n  ) %>%\n  estimate_panoptic(n_clusters = n_clusters) %>%\n  mutate(\n    true_effect = treatment_effect,\n    p_treat = p_treat\n  ) \n}\n\n\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_p_treat function on each set of parameters. We thus create a table with all the values of the parameters we want to test, param_p_treat. First, we only vary the proportion of unit treated. We will vary the number of clusters in a second analysis.\nNote that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\n\nWe then run the simulations by mapping our compute_sim_p_treat function on param_p_treat.\n\n\n\nAnalysis of the results (proportion treated units)\nQuick exploration\nFirst, we quickly explore the results.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}[\\beta_0 - \\widehat{\\beta}]\\) and \\(\\mathbb{E}[|\\beta_0 - \\widehat{\\beta}||signif]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the effect size. To enable comparison across simulation and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_sim_panoptic <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>%\n    group_by(p_treat, n_clusters) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100,\n      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),\n      bias_signif = mean(ifelse(significant, estimate/true_effect, NA), na.rm = TRUE),\n      bias_all = mean(estimate/true_effect, na.rm = TRUE),\n      bias_all_median = median(estimate/true_effect, na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>%\n    ungroup()\n}\n\nsummary_sim_p_treat <- summarise_sim_panoptic(sim_p_treat)\n\n\n\nTHE graph\nTo analyze our results, we build a unique and simple graph:\n\n\n\nVarying the number of clusters\nWe then reproduce a similar type of analysis and graph but varying the number of clusters.\n\n2.59 sec elapsed\n\n\n\n\n\n",
      "last_modified": "2022-01-12T10:31:21+01:00"
    },
    {
      "path": "RDD.html",
      "title": "Simulations RDD",
      "description": "In this document, we run a simulation exercise to illustrate how using a Regression Discontinuity Design (RDD) to avoid confounders may create type M error.",
      "author": [
        {
          "name": "Vincent Bagilet",
          "url": "https://vincentbagilet.github.io/"
        },
        {
          "name": "Léo Zabrocki",
          "url": "https://www.parisschoolofeconomics.eu/en/"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\nSummary and intuition\nAn illustrative exampleModeling choices\nData generation\nDefining the bandwidth\nEstimation\nOne simulation\nAll simulations\n\nAnalysis of the resultsQuick exploration\nComputing bias and type M\nGraph\n\n\n\nbody {\ntext-align: justify}\nSummary and intuition\nIn the case of the RDD, the confounding/type M error trade-off is mediated by the size of the bandwidth considered in the analysis. The underlying idea is that the smaller the bandwidth, the more comparable units are and therefore the smaller the risk of confounding is. Yet, with a smaller bandwidth, sample size and thus power decrease, increasing type M error.\nAn illustrative example\nTo illustrate this trade-off, we consider a standard application of the RD design in economics of education where a grant or additional lessons are assigned based on the score obtained by students on a standardized test. Students with test scores below a given threshold receive the treatment while those above do not. Yet, students far above and far below the threshold may differ along unobserved characteristics such as ability. To limit this bias, the effect of the treatment is estimated by comparing the outcomes of students just below and just above this threshold. This enable to limit disparities in terms of unobserved characteristics.\nThistlewaite and Campbell (1960) introduced the concept of RDD using this type of quasi-experiment. In their paper, they take advantage of a sharp discontinuity in the assignment of an award (a Certificate of Merit) based on qualifying scores at a test. This type of analysis is still used today and many papers leveraging similar methodologies have been published since this seminal work. For instance, Jacob and Lefgren (2004) exploit this type of discontinuity to study the impact of summer school and grade retention programs on test scores. Students who score below a given score are required to attend a summer school and to retake the test. Students who do not pass the second have to repeat the grade.\nModeling choices\nIn the present analysis, we build our simulations to replicate a similar type of quasi-experiment. In our fictional example, all students scoring below a cutoff \\(C\\) in a qualification test are required to take additional lessons. We want to estimate the effect of these additional lessons on scores on a final test taken by all students a year later.\nWe assume that the final score of student \\(i\\), \\(Final_i\\), is correlated with their qualification score \\(Qual_i\\) and their treatment status \\(T_i\\), ie whether student \\(i\\) received additional lessons or not. We further assume that both qualification and final test scores are affected by students’ unobserved ability \\(U_i\\) in a non linear way.\nThe DGP can be represented using the following Directed Acyclic Graph (DAG):\n\n\n\nFinal test scores are thus defined as follows:\n\\[Final_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} +  \\delta f(U_i) + \\epsilon_{i}\\] Where \\(\\alpha\\) is a constant, \\(f\\) a non linear function and \\(e \\sim \\mathcal{N}(0, \\sigma_{e})\\) noise. The parameter of interest is \\(\\beta\\). Translating this into a potential outcomes framework, we have \\(Final_i(0) = \\alpha + \\gamma Qual_{i} + \\delta f(U_i) + \\epsilon_{i}\\) and \\(Final_i(1) = \\alpha + \\gamma Qual_{i} + \\beta + \\delta f(U_i) + \\epsilon_{i}\\)\n\nTo simplify, we consider the following assumptions:\nFull compliance and a sharp treatment allocation such that \\(T_i = \\mathbb{I}[Qual_{i} < C]\\). All students with a qualification score below the threshold are treated and receive additional lessons. None of the students with a qualification score above the threshold are treated.\nThe unobserved ability affects qualification and final test scores in a cubic way. A large ability has a strong positive impact on test scores. Similarly a particularly low ability strongly impacts test scores negatively. An average ability does not have much impact on test scores. Such a functional form seems realistic. Note that ability creates an OVB only if it has a non linear impact on test scores.\nWe assume constant treatment effects. This assumption is not necessary and our results hold if we consider non-constant treatment effects. We thus may drop this assumption in the future.\nWe assume that the unobserved availability affects the qualification and final score in a similar way and therefore with the same intensity \\(\\delta\\).\nMore precisely, we set:\n\\(N\\) the number of students\n\\(U \\sim \\mathcal{N}(0, \\sigma_u^{2})\\) the unobserved ability.\n\\(Qual_i = H_i + \\delta U_i^{2}\\) where \\(H \\sim \\mathcal{N}(\\mu_h, \\sigma_h^{2})\\). We center the qualification scores such that treated units are below 0 and non treated ones above.\n\\(T_i = \\mathbb{I}[Qual_{i} < q_c]\\) where for now and for simplicity, \\(q_c\\) is a fixed grade threshold given as the quantile in the qualification score distribution.\n\\(e \\sim \\mathcal{N}(0, \\sigma_e^2)\\)\n\\(Final_{i} = \\alpha + \\beta T_i + \\gamma Qual_{i} + \\delta U_i^{2} + e_{i}\\)\nData generation\nWe write a simple function that generates the data. It takes as input the values of the different parameters and returns a data frame containing all the variables for this analysis.\nOnce the fake data is generated, to make things more realistic we consider our data as if it was actual data. We do not take advantage of our knowledge of the data generating process in the estimation procedure. However, we observe both potential outcomes and the unobserved ability. Note that, in a real world setting, one would generally know the value of the threshold (and thus of \\(q_c\\)). Based on that and to simplify the computation of the bandwidth, we store \\(q_c\\).\n\n\ngenerate_data_rdd <- function(N, \n                              sigma_u,\n                              mu_h, \n                              sigma_h, \n                              sigma_e, \n                              alpha, \n                              beta,\n                              gamma,\n                              delta,\n                              q_c) {\n  \n  data <- tibble(id = 1:N) %>% \n    mutate(\n      # qual = rnorm(nrow(.), mu_h, sigma_h),\n      # u = rnorm(nrow(.), 0.5, sigma_u) + qual + 0.3*qual^3,\n      u = rnorm(nrow(.), 0, sigma_u),\n      qual = rnorm(nrow(.), mu_h, sigma_h) + delta*u^2,\n      e = rnorm(nrow(.), 0, sigma_e),\n      # qual_c = qual - quantile(qual, q_c),\n      # treated = qual_c < 0,\n      # threshold = quantile(qual, q_c),\n      treated = qual < quantile(qual, q_c),\n      final0 = alpha + gamma*qual + delta*u^2 + e,\n      final1 = final0 + beta,\n      final = final0 + beta*treated,\n      q_c = q_c\n    )\n  \n  return(data)\n}\n\n\n\nWe set baseline values for the parameters to emulate a somehow realistic observational study in this field. The set of parameters may produce test score outside of the range 0-100 in some iterations but that does not affect the analysis. We add the parameter value for delta separately as we will vary the value later and will reuse the vector baseline_param_RDD.\n\nN\nsigma_u\nmu_h\nsigma_h\nsigma_e\nalpha\nbeta\ngamma\nq_c\n1000\n0.5\n75\n7\n4\n20\n1\n0.7\n0.5\n\nHere is an example of data created with our data generating process:\n\nid\nu\nqual\ne\ntreated\nfinal0\nfinal1\nfinal\nq_c\n1\n0.6184824\n81.22040\n-4.3558763\nFALSE\n72.88093\n73.88093\n72.88093\n0.5\n2\n0.1769192\n65.83023\n2.8614875\nTRUE\n68.97395\n69.97395\n69.97395\n0.5\n3\n0.7391287\n76.25360\n3.4724584\nFALSE\n77.39629\n78.39629\n77.39629\n0.5\n4\n0.2756673\n70.57394\n5.5570385\nTRUE\n75.03479\n76.03479\n76.03479\n0.5\n5\n-0.6398870\n76.15083\n-0.7083878\nFALSE\n73.00665\n74.00665\n73.00665\n0.5\n6\n-0.6178104\n76.70284\n-6.4382163\nFALSE\n67.63546\n68.63546\n67.63546\n0.5\n7\n0.0479340\n77.05372\n-5.4455459\nFALSE\n68.49436\n69.49436\n68.49436\n0.5\n8\n0.6020962\n68.27389\n-4.9295771\nTRUE\n63.22467\n64.22467\n64.22467\n0.5\n9\n-0.3604265\n58.65782\n5.6788380\nTRUE\n66.86922\n67.86922\n67.86922\n0.5\n10\n-0.0249590\n74.55914\n-1.5227105\nTRUE\n70.66931\n71.66931\n71.66931\n0.5\n\nDefining the bandwidth\nIn a RDD, the model is estimated only for observations close enough to the threshold, ie in a given bandwidth. We therefore create a function to define this bandwidth by adding a variable to the data set treated_bw that is equal to NA if the observations is outside of the bandwidth, TRUE if the observation falls in the bandwidth and the student is treated and FALSE if the observation falls in the bandwidth and the student is not treated. The bandwidth parameter bw represents the proportion of units that are in the bandwidth. If bw = 0.1, 10% of the students are in the bandwidth for instance.\n\n\ndefine_bw <- function (data, bw) {\n  data <- data %>% \n    mutate(\n      treated_bw = ifelse(\n        dplyr::between(\n          qual, \n          quantile(qual, unique(q_c) - bw/2), \n          quantile(qual, unique(q_c) + bw/2)\n        ), \n        treated, \n        NA\n      )\n    )\n} \n\n\n\nThe following graph illustrates this process by plotting final test scores against qualification ones depending on the value of treated_bw.\n\n\n\nEstimation\nAfter generating the data, we can run an estimation.\nNote that to run power calculations, we need to have access to the true effects. Therefore, before running the estimation, we write a short function to compute the average treatment effect on the treated (ATET). We will add this information to the estimation results.\n\n\ncompute_true_effect_rdd <- function(data) {\n  treated_data <- data %>% \n    filter(treated) \n  return(mean(treated_data$final1 - treated_data$final0))\n}  \n\n\n\nWe then run the estimation. To do so, we only consider observations within the bandwidth and regress the final test scores on the treatment, the qualification score and their interaction. Note that we include this interaction term to allow more flexibility and to mimic an realistic estimation. Yet, we know that this interaction term does not appear in the DGP. Including it or not do not change the results. Also note that, of course, we do not include the unobserved ability in this model to create an OVB.\n\n\nestimate_rdd <- function(data, bw) {\n  data_in_bw <- data %>% \n    define_bw(bw = bw) %>% \n    filter(!is.na(treated_bw))\n  \n  reg <- lm(\n    data = data_in_bw, \n    formula = final ~ treated + qual\n  ) %>% \n    broom::tidy() %>%\n    filter(term == \"treatedTRUE\") %>%\n    rename(p_value = p.value, se = std.error) %>%\n    select(estimate, p_value, se) %>%\n    mutate(\n      true_effect = compute_true_effect_rdd(data),\n      bw = bw\n    )\n  \n  return(reg)\n}\n\n\n\nOne simulation\nWe can now run a simulation, combining generate_data_rdd and estimate_rdd. To do so we create the function compute_sim_RDD. This simple function takes as input the various parameters along with a vector of bandwidth sizes, vect_bw. If we want to run several simulations with different bandwidths, we can reuse the same data, hence why we allow to passing a vector of bandwidths and not only one bandwidth. The function returns a table with the estimate of the treatment, its p-value and standard error, the true effect and the bandwidth and intensity of the OVB considered (delta). Note for now, that we do not store the values of the other parameters for simplicity because we consider them fixed over the study.\n\n\ncompute_sim_RDD <- function(N,\n                            sigma_u,\n                            mu_h,\n                            sigma_h,\n                            sigma_e,\n                            alpha,\n                            beta,\n                            gamma,\n                            delta,\n                            q_c,\n                            vect_bw) {\n  \n  data <- generate_data_rdd(\n    N = N,\n    sigma_u = sigma_u,\n    mu_h = mu_h,\n    sigma_h = sigma_h,\n    sigma_e = sigma_e,\n    alpha = alpha,\n    beta = beta,\n    gamma = gamma,\n    delta = delta,\n    q_c = q_c\n  ) \n  \n  map_dfr(vect_bw, estimate_rdd, data = data) %>%\n    mutate(delta = delta)\n} \n\n\n\nHere is an example of an output of this function.\n\nestimate\np_value\nse\ntrue_effect\nbw\ndelta\n2.471264\n0.1119850\n1.540774\n1\n0.1\n1\n1.990881\n0.0788492\n1.126977\n1\n0.2\n1\n\nAll simulations\nWe will run the simulations for different sets of parameters by mapping our compute_sim_RDD function on each set of parameters. We thus create a table with all the values of the parameters we want to test param_rdd. Note that in this table each set of parameters appears n_iter times as we want to run the analysis \\(n_{iter}\\) times for each set of parameters.\n\n\nsimple_param_RDD <- tibble(\n  N = 500,\n  sigma_u = 1,\n  mu_h = 0,\n  sigma_h = 1,\n  sigma_e = 0.5,\n  alpha = 1,\n  beta = 1,\n  gamma = 0.7,\n  q_c = 0.5\n)\n\nfixed_param_RDD <- simple_param_RDD #%>% rbind(...)\n# vect_bw <- seq(0.05, 0.4, 0.05)\nvect_bw <- c(seq(0.05, 0.4, 0.05), seq(0.4, 1, 0.1))\nvect_delta <- c(3)\nn_iter <- 1000\n\nparam_rdd <- fixed_param_RDD %>% \n  crossing(delta = vect_delta) %>% \n  mutate(vect_bw = list(vect_bw)) %>% \n  crossing(rep_id = 1:n_iter) %>% \n  select(-rep_id)\n\n\n\nWe then run the simulations by mapping our compute_sim_RDD function on param_rdd.\n\n\ntic()\nsim_rdd <- pmap_dfr(param_rdd, compute_sim_RDD)\nbeep()\ntoc()\n\n# saveRDS(sim_rdd, here(\"Outputs/sim_rdd.RDS\"))\n\n\n\nAnalysis of the results\nQuick exploration\nFirst, we quickly explore the results. In the following figure, we can see that for small bandwidth estimates are unbiased but imprecise while for large bandwidths estimates are precise but biased.\n\n\n\nWhen the bandwidth is relatively small, estimates are spread out and the mean of statistically significant estimates is larger than the true effect. Note that the average of all estimates, significant and non-significant, is close to the true effect. Applying a statistical significance filter leads to overestimate the true effect in this case.\n\n\n\nComputing bias and type M\nWe want to compare \\(\\mathbb{E}\\left[\\left|\\frac{\\widehat{\\beta_{RDD}}}{\\beta_0}\\right|\\right]\\) and \\(\\mathbb{E}\\left[\\left|\\frac{\\widehat{\\beta_{RDD}}}{\\beta_0}\\right| | signif \\right]\\). The first term represents the bias and the second term represents the type M error. This terms depend on the true effect size. To enable comparison across simulations and getting terms independent of effect sizes, we also compute the average of the ratios between the estimate and the true effect, conditional on significance.\n\n\nsummarise_simulations <- function(data) {\n  data %>%\n    mutate(significant = (p_value <= 0.05)) %>% \n    group_by(delta, bw) %>%\n    summarise(\n      power = mean(significant, na.rm = TRUE)*100, \n      type_m = mean(ifelse(significant, abs(estimate/true_effect), NA), na.rm = TRUE),\n      bias_all = mean(abs(estimate/true_effect), na.rm = TRUE),\n      .groups  = \"drop\"\n    ) %>% \n    ungroup()\n} \n\nsummary_sim_rdd <- summarise_simulations(sim_rdd)\n# saveRDS(summary_sim_rdd, here(\"Outputs/summary_sim_rdd.RDS\"))\n\n\n\nGraph\nTo analyze our results, we build a unique and simple graph:\n\n\n\nWe notice that, the smaller the bandwidth size, the closer the average of all estimates is to the true effect. Yet, when the bandwidth gets small significant estimates overestimate the true effect. This arises because of a loss of power, as shown in the graph below.\n\n\nsummary_sim_rdd %>% \n  ggplot(aes(x = bw, y = power)) + \n  geom_line(size = 0.8) +\n  labs(\n    x = \"Bandwidth size\",\n    y = \"Power\",\n    title = \"Evolution of power with bandwith size\"\n  )\n\n\n\n\n\n\n\n",
      "last_modified": "2022-01-12T10:31:30+01:00"
    },
    {
      "path": "summary.html",
      "title": "Plain language summary",
      "description": "In this paper, we show that, combined with current academic publication practices, front line empirical methods, efficient to identify how a factor causes another, may be more likely to exaggerate effect sizes.\n",
      "author": [],
      "contents": "\nEmpirical studies often aim to get a sense of how a factor causes another. For instance, one may want to evaluate the impact of a professional training program on wages. Such effects are often challenging to estimate. A simple difference between wages of people who participated in the program or not may not reflect the actual wage increase brought by the program. For instance, it might be the case that people who participated in the program would have earned higher wages even if they did not take the training. To measure the actual magnitude of the effect of the program, researchers use a particular set of methods, causal inference methods. In this paper, we show how academic publication practices can lead these otherwise effective methods to yield incorrect effect sizes.\nThese methods yield, on average, the true effect. However they can be imprecise, meaning that the range of possible values for the true effect they produce can be wide. Since, studies are often only carried out once, it might be the case that, by chance, a given study would produce an effect far away from the true effect. On the other hand, we know that publication practices favor effects that are clearly different from zero, ie larger. Combining these publication practices and the relative imprecision of causal inference methods thus creates another type of problem leading results to possibly differ from the true effect. Hence, these methods enable to avoid a first problem but are also more prone to another problem. In this paper, we show that there is a trade-off between these two problems.\nTo do so we build fake data simulations. Contrarily to real life cases, in simulations the true effect is known and we can evaluate how far the result of an analysis is from these true effects. We find that\nRecommendations\n\n\n\n",
      "last_modified": "2022-01-21T10:37:59+01:00"
    }
  ],
  "collections": []
}
